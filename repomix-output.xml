This file is a merged representation of a subset of the codebase, containing specifically included files, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of a subset of the repository's contents that is considered the most important context.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Multiple file entries, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Only files matching these patterns are included: sidebars.ts, docusaurus.config.ts, src/**/*, documentation/**/*
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

</file_summary>

<directory_structure>
documentation/
  adrs/
    decisions/
      2022-09-19-use-markdown-architectural-decision-records.md
      2022-10-05-using-tapir-library-as-a-dsl-for-openapi-specification.md
      2022-10-06-store-private-keys-of-issuers-inside-prism-agent.md
      2023-01-18-quill-library-for-sql-statement-generation.md
      2023-04-05-did-linked-resources.md
      2023-05-09-message-routing-for-multi-tenant.md
      2023-05-15-mediator-message-storage.md
      2023-05-16-hierarchical-deterministic-key-generation-algorithm.md
      2023-05-18-data-isolation-for-multitenancy.md
      2023-05-27-use-keycloak-and-jwt-tokens-for-authentication-and-authorisation-to-facilitate-multitenancy-in-cloud-agent.md
      2023-06-28-apollo-as-centralised-and-secure-cryptography-management-module.md
      2023-07-14-performance-framework-for-atala-prism.md
      2023-09-26-use-keycloak-authorisation-service-for-managing-wallet-permissions.md
      2023-09-28-revocation-status-list-expansion-strategy.md
      2024-01-03-use-jwt-claims-for-agent-admin-auth.md
      2024-01-15-Error-handling-report-problem-agent.md
      2024-01-16-use-zio-failures-and-defects-effectively.md
      2024-03-07-handle-errors-in-bg-jobs-by-storing-on-state-records-and-sending-via-webhooks.md
      2024-05-20-use-did-urls-to-reference-resources.md
  developers/
    cloud-agent/
      _category_.json
      authentication.md
      building-blocks.md
      did-management.md
      environment-variables.md
      secrets-storage.md
      troubleshooting&considerations.md
      vdr.md
    quick-start.md
    README.md
    sidebar.ts
  learn/
    components/
      cloud-agent/
        _category_.json
        multi-tenancy.md
        README.md
      did-prism/
        _category_.json
        did-prism-resolver-sdk.md
        README.md
      prism-node/
        _category_.json
        README.md
        running-node.md
      _category_.json
      mediator.md
    glossary.md
    identity.md
    README.md
    sidebar.ts
  reference/
    adrs/
      _category_.json
      README.md
    _category_.json
    README.md
    sidebar.ts
    specifications.md
src/
  components/
    atala-graphic/
      index.js
      index.module.css
    blob/
      index.js
      index.module.css
    button/
      index.js
      styles.module.css
    features/
      index.js
      styles.module.css
    resources/
      index.js
      index.module.css
  config/
    headerMenu.ts
    multiDocPreset.ts
    presets.ts
  css/
    custom.css
  pages/
    index.js
    index.module.css
  plugins/
    remarkLinkFixer.js
  theme/
    Footer/
      index.js
      index.module.css
  utils/
    index.ts
docusaurus.config.ts
sidebars.ts
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path="documentation/adrs/decisions/2022-09-19-use-markdown-architectural-decision-records.md">
# Use Markdown Architectural Decision Records

- Status: accepted
- Date: 2022-09-19
- Tags: doc

## Context and Problem Statement

We want to record architectural decisions made in this project.
Which format and structure should these records follow?

## Decision Drivers 

- We want to improve the information and technical documentation of our software engineering projects
- We want to create an immutable log of important architectural decisions we have made during the software development
- We recognise the need for a complement to RFCs that typically documents the process before a decision has been reached (and not after)
- We want this decision log to offer a a standardised, lightweight, and extensible manner to increase consistency across systems
- We want this decision log to live as close as possible to the relevant code-base
- We want this decision log to be easily readable, discoverable and meaningfully searchable

## Considered Options

- [MADR](https://github.com/adr/madr/compare/3.0.0-beta...3.0.0-beta.2) 3.0.0-beta.2 
- [MADR](https://adr.github.io/madr/) 2.1.2 with Log4brains patch
- [MADR](https://adr.github.io/madr/) 2.1.2 – The original Markdown Architectural Decision Records
- [Michael Nygard's template](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions) – The first incarnation of the term "ADR"

## Decision Outcome

Chosen option: "MADR 2.1.2 with Log4brains patch", because

- The MADR format is lean and fits our development style.
- The MADR structure is comprehensible and facilitates usage & maintenance.
- The Log4brains patch adds more features, like tags.
- This format is compatible with Log4brains and allows us to run a portal with a timeline of ADRs

The "Log4brains patch" performs the following modifications to the original template:

- Change the ADR filenames format (`NNN-adr-name` becomes `YYYYMMDD-adr-name`), to avoid conflicts during Git merges.
- Add a `Tags` field.

### Additional Information

We will implement Architectural Decision Records (ADRs) with immediate effect;

- ADRs are to be authored and published with (at minimum) 1 TA as decider;
- ADRs will be formatted using MADR 2.12 with log4Brains Patches format;
- ADRs are to be used to log system-wide decisions;
- Should the system consist of multiple code-repositories, ADRs should live in the main system repository;
- ADRs are to be stored in a subfolder docs/decisions/ of the repository for the software affected;
- ADRs will follow a flat filename convention with relevant components in their filename

## Links

- Relates to [RFC-0016](https://input-output.atlassian.net/wiki/spaces/ATB/pages/3580559403/RFC+0016+-+Use+Architectural+Design+Records)
</file>

<file path="documentation/adrs/decisions/2022-10-05-using-tapir-library-as-a-dsl-for-openapi-specification.md">
# Using tapir library as a DSL for OpenAPI specification

- Status: accepted
- Deciders: Yurii Shynbuiev, David Poltorak, Benjamin Voiturier, Ilya Peresadin, Bart Suichies
- Date: [2022-10-05]
- Tags: OpenAPI, DSL, Tapir, code-generation, RESTAPI

Related ADR/AIP: [Introduce REST HTTP for existing Node services](https://input-output.atlassian.net/wiki/spaces/AV2/pages/3454500948/AIP+-+001)

## Context and Problem Statement
Identus Platform will contain the REST API. The decision was made by team consensus during the first AOH meeting to follow "OpenAPI specification first" approach and generate stubs, server side and client side code based on OAS.
Following this strategy we currently have 4-5 OAS files (Castor, Pollux, Mercury, Configuration).

The following tool was selected for code generation: [OpenAPI Tools](https://github.com/OpenAPITools/openapi-generator)

Instead of using the yaml file as OpenAPI specification and openapi-generator for server and client stub generation - this ADR proposes to use [Tapir](https://tapir.softwaremill.com/en/latest/index.html) Scala library as DSL for OpenAPI specification, `interpret` the endpoint defitions as Scala server and client stub, generate the yaml file, and use openapi-generator for client stubs.

Technology stack that is going to be used in the Identus platform backend: Scala 3 + ZIO ecosystem

Akka framework after version 2.6.x cannot be used because [Lightbend changed the license type to BSL 1.1](https://www.lightbend.com/blog/why-we-are-changing-the-license-for-akka).

Looks like Akka 2.6.x still can be used according to [License FQA](https://www.lightbend.com/akka/license-faq)

Currently, we have a code generation for Akka that is wrapped up into ZIO. Code generation mustache templates for ZIO-http are not available in OpenAPI tools.

Mustache templates and code generation doesn't work out of the box, so the original templates where copied to the project and fixed by @Shota and @Pat.
Current templates and generator contains constraints that were reported by [@Pat](https://docs.google.com/document/d/1WhUtflM_o-5uSx9LW76lycz2kbk071cVZiv6EtVwhAQ/edit#heading=h.ywcvgffenpz) and [@Shota](https://input-output-rnd.slack.com/archives/G018JE9NHAM/p1664563129397819), this requires engineering time to adopt the OAS for a code generation. @Ben says that we can live with these constraints

Generally, OAS files are written by the engineers with different experience and different view on formatting, schemas, normalization, datatype. For instance, in current templates don't have
- a consistent way for paginating the entities
- standard Responses for 4xx and 5xx errors
- normalized data types (we use ```anyOf```, ```allOf```)
- query parameters convention for filtering the entities
- some data types are duplicated in both Castor and Pollux OAS

As OAS specification evolves it's getting harder to manage it because of the size of the file.
To mitigate this issue @Pat proposed to use well-known tools:
"Knowing that there are tools like [dhall](https://dhall-lang.org/#) or [CUE](https://cuelang.org/docs/integrations/openapi/) that allow us to write large configuration in yaml (or json) in a typesafe / reuseable way, I'm not hesitant to go contract-first."(c)

Quality and formatting of autogenerated code depend on the template (not all templates are good enough). Making the good code from existing templates require additional time of engineers.

### OpenAPI code generator constraints for Akka server
#### @Pat
- oneOf is not supported. It combines everything from the list if it’s an object, discard if it’s a primitive
- allOf is not supported as stated in the documentation, but testing locally it worked
- Have to handwrite the serialization layer
#### @Shota
- Undefined type ```AnyType```. You can have additionalProperties (```components/schemas/<schema name>/properties/additionalProperties```) in the schema, when you add it, it will generate a type for \<schema name\> that has another type called `AnyType` inside, this type is not defined, it just does not exist in generated code so the compilation will fail, if you get a compilation error in your sources with some `AnyType` that is not defined, look for additionalProperties in your schema
- Values of type object without properties don’t serialize with spray json. You can have ```componets/schemas/<schema name>/properties/<property name>``` and every property has a type, like string, int, etc.., you can have type as object, but if you do so, you must provide object properties as well like in example below, if you don’t add it, it will generate this object type with Any in scala, and then the Akka marshaller will fail, because we use SprayJson there, and it does not support Reader and Writer for type Any (basically it can’t serialize type Any into json), you could probably define Writer and Reader for type Any to be an empty object, but I personally don’t see a reason to have value of type object and not define what properties it is going to have anyway.
- ```requestBody``` in every path must be explicitly ```required:true```. It is ```false``` by default, if not marked as ```true``` it will generate a service functions that accepts ```Option[Type]``` instead of ```Type``` but endpoints are always expecting ```Type``` even if required is ```false```, not ```Option[Type]```, then when you try to generate sources you will get compilation error ```expecting Type got Option[Type]```

## Decision Drivers <!-- optional -->

- enforce type-safety to endpoint definitions using Scala compiler and Tapir DSL, add CI for endpoints definitions
- make endpoint definitions convenient for engineers by reusing common abstractions and definitions
- introduce a standard types, schemas and approaches for all endpoint definitions: query, pagination, responses, etc
- reuse endpoint definitions for creating server and client stubs in Scala
- align the server side of REST API with the current technology stack (ZIO + ecosystem)
- have a control over the codebase and data types
- reduce time-of-maintenance of the code (either OAS should be adapted for generator or mustache templates should be fixed)
- functional way of implementation of non-functional requirement (metrics, tracing, logging)
- straight forward generation of Swagger UI, Redoc documentation and Async API documentation based on endpoint definitions

## Considered Options

- use OpenAPI tools (edit OAS manually, generate server stub for Akka and client stubs for any other languages)
- use OpenAPI tools, but generate code for other server-side library (Play, Finch, Lagom)
- use Tapir library (edit endpoint definitions as Scala code, reuse endpoint definitions for server stubs, generate OAS based on endpoint definitions, generate client stubs for any other language)

## Decision Outcome

Chosen option:"use Tapir library" till the end of the year, evaluate this solution in 2023

All endpoint definition are written in Tapir DSL.

OpenAPI specification generated based on endpoint definition and is published as an artefact. (must be a part of CI)

The server side is interpreted using a ZIO-HTTP interpreter to be aligned with the given technology stack.

Client side stubs are generated using OpenAPI tools and OpenAPI specification file. (must be a part of CI)

For server-side code the flow is following:

<pre class="mermaid">
graph TD
    ED(Endpoint Definition) --> |Generate| OAS(OpenAPI Specification)
    ED --> |Generate| AAUI(AsyncAPI Specification)
    ED --> |Interpret| SSS(Scala Server Stub)
    ED --> |Interpret| SCS(Scala Client Stub)
    ED --> |Produce| SUI(Swagger UI)
    ED --> |Produce| RUI(Redoc UI)
    OAS --> |Input| OAT(OpenAPI Tools)
    OAT --> |Generate| SS(Server Stub)
    OAT --> |Generate| CS(Client Stub)
</pre>

### Positive Consequences <!-- optional -->

- Type-safety and OAS configuration as a code will speed up development
- Generated OpenAPI specification is unified according to the single standard (Tapir generator)
- Errors in the types and endpoint definitions will be found in compile-time
- Code generations will be replaced with interpretation with higher guarantees of stability
- Engineers will save time for feature implementation instead of investigating the issues with AOS files or templates
- Better management of OAS spec and control over the documentation (Swagger UI, Redoc, Async API for WebSockets)

### Negative Consequences <!-- optional -->
- Not all engineers will be able to edit the endpoint definitions in Tapir DLS, so either only engineer with Scala knowledge will do this, or knowledge sharing and workshops "How to use Tapir" are required.
- OAS is going to be generated from the model defined by DLS, so the granular/manual control over the spec will be replaced by Tapir generator
- There is a risk that Tapir might have some hidden surprises and constraints

### Option 1 & 2: Feature Implementation Workflow
<pre class="mermaid">
graph TD
    U[Start Feature] --> |Edit OAS| A
    A[OAS File] --> |Input| E
    U --> |Edit Template| E
    E[Generator & Templates]-->|Generate Server Code| B(Server Code)
    E -->|Generate Client Code| C(Client Code)
    C -->|Compile| OC(Other Compiler)
    OC -->|Compilation Error| I
    OC -->|Success| T
    E -->|Host file as Swagger UI| D(Swagger)
    B --> |Compile| S(Scala Compiler)
    S --> |Compilation Error| I(Investigate)
    I --> |Try again| U
    S --> |Success| T(Complete Feature)
</pre>

### Option 3: Feature Implementation Workflow
<pre class="mermaid">
graph TD
    U[Start Feature] --> |Edit Endpoint Specification| ED(Endpoint Definition)
    U --> |Edit Input/Output Types| DM(Domain Model)
    ED --> |Input| TE(Tapir Library)
    DM --> |Input| TE
    TE --> |Generate| A
    TE --> |Interpret| SC(Server Code)
    TE --> |Interpret| CC(Client Code)
    TE --> |Produce| SW(Swagger UI)
    TE --> |Produce| RD(Redoc UI)
    TE --> |Compilation Error| U
    A[OAS File] --> |Input| E
    U --> |Edit Template| E
    E[Generator & Templates]-->|Generate Server Code| B(Server Code)
    E -->|Generate Client Code| C(Client Code)
    C -->|Compile| OC(Other Compiler)
    OC -->|Compilation Error| I
    OC -->|Success| T
    E -->|Host file as Swagger UI| D(Swagger)
    B --> |Compile| S(Scala Compiler)
    S --> |Compilation Error| I(Investigate)
    I --> |Try again| U
    S --> |Success| T(Complete Feature)
</pre>

## Pros and Cons of the Options <!-- optional -->

### Option 1: use OpenAPI tools and mustache templates for Akka server

- Good, because @Pat and @Shota already stabilized the templates, and we have a working solution
- Good, because any engineer from CoreDID and Product Foundry team is able to contribute to the documentation
- Good, because the same source of truth (OAS file) is used to generate Server and Client stub (less integration problems for client stubs)
- Bad, because there are known constraints in the mustache templates that can slow down engineering
- Bad, because Akka changed the licence and version 2.6.x will not be supported in 1 year.
- Bad, because it's hard to keep the same standard for OAS that are written by different engineers
- Bad, because all OAS files are merged together at infrastructure level which is slightly complex solution for this task.
- Bad, because Akka Framework is not in ZIO ecosystem (it's not a good practice to use both frameworks)

### Option 2: use OpenAPI tools and mustache templates for alternative Scala server libraries (Finch, Lagom, Play )

[example | description | pointer to more information | …] <!-- optional -->
- All ```good``` and ```bad``` are the same as in Option 1
- Bad, because we don't know if the mustache templates are good enough for Scala 3
- Bad, because we need to evaluate if engineering team have the experience in Finch, Lagom or Play

### Optoin 3: use Tapir as DSL for OpenAPI specification

[example | description | pointer to more information | …] <!-- optional -->

- Good, because type-safety and DLS will save the engineering time by providing a quick feedback loop in compile time
- Good, because generated OAS will be aligned with the common standards
- Good, because engineers can define and reuse the abstractions in FP way
- Good, because entities (inputs/outputs) will be reused by Scala backend library
- Good, because the endpoint definition will be reused in Scala Server or Client stub
- Good, because there is no need to generate the code, stubs are interpreted by the library
- Good, because ZIO-HTTP will be used, which is aligned with the current stack
- Good, because Open API, Swagger, Redoc, Async API document/site generations are supported
- Bad, because only Scala engineers will be able to edit the documentation
- Bad, because the granular control over OAS YAML file will be lost (OAS file is generated automatically)
- Bad, because we need to spend 3-5 day to transform OAS files into Tapir DSL

## How to migrate from the current state to Tapir?
### Current state: OpenAPI Tools + mustache templates for Akka server
### Desired state: Endpoint Definitions in Tapir + ZIO-HTTP
Estimated migration time is 4-6 days which we don't really want to waste.

So, engineering team can proceed with keeping the existing endpoints in the current state and even work on the new endpoints using generated server stubs for Akka.

At the same time OAS file can be translated to Tapir step-by-step and the endpoint definitions can be [interpreted by Tapir library as Akka routes](https://tapir.softwaremill.com/en/latest/server/akkahttp.html), and attached to the server endpoint in the same way as generated endpoints.

This transitions period might take 2-3 weeks till engineering team get enough knowledge of using Tapir.

Then all the endpoints are translated to Tapir, it will be possible to switch the interpreter from Akka to [ZIO-HTTP library](https://tapir.softwaremill.com/en/latest/server/ziohttp.html).

## Links <!-- optional -->

- [OpenAPI Tools](https://github.com/OpenAPITools/openapi-generator)
- [Goals of Tapir library](https://tapir.softwaremill.com/en/latest/goals.html)
- [Tapir](https://tapir.softwaremill.com/en/latest/index.html)

<!--
<script src="https://cdnjs.cloudflare.com/ajax/libs/mermaid/9.2.1/mermaid.min.js"/>
<script>
  mermaid.initialize({ startOnLoad: true });
</script>
-->
</file>

<file path="documentation/adrs/decisions/2022-10-06-store-private-keys-of-issuers-inside-prism-agent.md">
# Store private keys of Issuers inside the Cloud Agent

- Status: accepted
- Deciders: Benjamin Voiturier, Pat Losoponkul, Miloš Džepina, Shailesh Patil, Shota Jolbordi, Bart Suichies, Ezequiel Postan, Yurii Shynbuiev, David Poltorak
- Date: 2022-10-05

## Context and Problem Statement

While each holder has a wallet application on the phone (edge agent) to store private keys, contacts, and credentials, Identus Cloud Agent will provide a custodial solution to Issuers and Verifiers. Thus they won't have their wallets or store/manage keys. There needs to be storage for the private keys of Issuers and Verifiers on the Cloud Agent side.


## Considered Options

- Having issuers store and manage their own keys on the edge wallet (Prism 1.4 approach)
- Storing keys in a dedicated wallet application that is connected to the Cloud Agent
- Having the Cloud Agent store and manage keys directly


## Decision Outcome

Chosen option: Option 3, because it is the simplest approach that satisfies the needs of providing the Issuer and Verifier with key storage while also not requiring them to manage their own keys. Option 3 was chosen instead of Option 2 because it achieves the same goal but does not require work on integrating another wallet application, so in short, it is simpler and faster to implement.

### Negative Consequences <!-- optional -->

While Option 3 is simpler to implement then Option 2 and provides basic functionality required to solve the problem emphasized in [Context and Problem Statement](#context-and-problem-statement), it does not provide full functionality and security of widely used and well tested wallet application. Therefore this decision is considered to be temporary and made only in the interest of solving the problem as fast as possible.


## Links

- [Recording of the meeting where decision was made](https://drive.google.com/file/d/120YyW2IEpl-F-6kF0V0Fau4bM7BbQ6mT/view?usp=sharing)
</file>

<file path="documentation/adrs/decisions/2023-04-05-did-linked-resources.md">
# DID-linked-resources

- Status: draft
- Deciders: Yurii Shynbuiev, Benjamin Voiturier, Lohan Spies, Ezequiel Postan, Shota Jolbordi
- Date: 2023-04-05
- Tags: did, linked-data, ledger

## Target

[Research Spike - Schema and Verifiable Presentation Registry](https://input-output.atlassian.net/browse/ATL-3186)

- Provide a clear and concise analysis of the various schema registry implementation and the associated benefits and downfalls of each approach.
- Provide a concrete proposal for what we would like to implement for the Identus platform.
- Provide a generic way of storing and linking the resources for the DID in the Identus platform.

## Context and Problem Statement

Identus platform must be able to store and distribute the various resources such as credential schemas, logos, revocation status lists, and documents (aka any text, JSON, images, etc). But in the scope of the current ADR the following resource types are discussed:

- credential schema (JSON and AnonCreds)
- credential definition (AnonCreds)
- revocation list

**NOTE**: Resources containing the PII must never be stored on-chain.

Requirements for storing and distributing resources:

- Decentralization - resources must be stored in decentralized storage
- Discoverability - it should be possible to discover the resource
- Longevity - the ability of a storage solution to maintain the availability and integrity of stored resources
- Interoperability - it should be possible for other SSI systems to fetch the resource
- Trust - resources must be stored in reliable tamper-proof storage and be trusted by other SSI systems

Other requirements, such as `versioning`, `security` and `immutability` are out of the scope of this ADR:

- Versioning - is a specific requirement for the particular resource and belongs to the resource metadata
- Security - is an important aspect that must be taken into account by the underlying storage system and data access layer
- Immutability - is one of the strategies to guarantee `trust` and `decentralisation``, but it shouldn't be a requirement by itself.

The technical solution contains a lot of variations and particular small decisions but overall it can be split into two main questions:

- where the resource is stored?
- how the resource is discovered and fetched?

## Constraints

### Storage limitations

All decentralized storage (DLT or IPFS) has storage limitations, and the amount of data that can be stored is limited by the available storage capacity and the way how the resources are stored.

The following aspect must be taken into account for storing the resources in DLT:

- transaction size limit (can be mitigated by data fragmentation, so the single resource is stored in multiple transactions) - 16KB, 32KB, 64KB, up to 1MB - depending on the type of the blockchain
- throughput - bytes we can insert to storage per unit of time
- latency - multi-second time per insert
- cost - each insertion costs fees

Based on the nature of the resource the size limitations must be considered.

For the following resource types and the common use cases 16KB should be enough, so it's possible to store these on DLT:
- credential schema
- credential definition
- logo in SVG format
- Merkle Tree
- documentation in the markdown format

For larger resource types IPFS or another option should be considered. Large resource examples:
- media files
- large documents
- large revocation status lists

IPFS doesn't have a size limitation (it's limited by the underlying storage or the particular SDK) and requires additional infrastructure and `incentives` (the way to pay for the storage) from the community.

IPFS can be used for storing the resources, but it should be covered in the scope of separated ADR.

### Scalability

While DLT and IPFS are designed for scalability, they can still face issues with scalability when it comes to storing SSI resources. As more users store their SSI resources on these platforms, it can become more difficult to scale the infrastructure to handle the increased demand.

To mitigate the scalability challenge the `hybrid` solution can be considered. In this case, the resource is stored in the centralized database or IPFS system, and the `hash`, `signature` or other metadata that guarantees the `trust` is stored on-chain.

Storing credential schemas, and logos, not large documents don't require the hybrid solution, so the use cases for it is out of scope in the current ADR.

Scalability issues also must be considered in the decision for linking the resources to the DID. For instance, the Cheqd solution keeps all the resources linked to the DID inside of the metadata of the DIDDoc which leads to growing the DIDDoc size after some period and an update of the DID Document when the new resource is published on-chain and linked to the DID.

### Access control

SSI resources stored in DLT and IPFS can be accessed by anyone who has access to the network.

This can be a security concern for organizations that need to control access to their SSI resources.

Access control also can be an issue for interoperability with other SSI systems.

The types of resources such as credential schemas, logos, and revocation lists should be available without additional access control.

### Data privacy

While DLT and IPFS are designed to be secure, there is still a risk that SSI resources stored on these platforms could be accessed or stolen by unauthorized parties.

This is especially concerning when it comes to sensitive personal information.

Personal data or any other sensitive information should not be stored on-chain and be available for unauthorized parties.

Credential schemas, documents, and logos usually do not contain personal data, so can be stored on-chain.

Revocation lists that are designed using privacy-preserving capabilities can be stored on-chain as well.

## Decision Drivers

- Interoperability
- Trust
- Longevity
- Scalability
- Discoverability
- Vendor Lock

## Storage

Choosing the right storage for resources is an architectural decision.

Companies that build SSI platforms usually use the underlying blockchain for storing the resources in a generic way and an API layer with an underlying centralized database and SDK for indexing, and access to the resources.

Usually, resources are stored efficiently (any binary format, protobuf, CBOR) on-chain to reduce the size and the cost of the operation.

The application layer that communicates with the underlying blockchain is used for publishing and retrieval of the resource.
Based on concrete implementation, the resources can be decoded and indexed in the database and available via internal API, SDI, or Universal Resolver.

Storing resources off-chain also makes sense, but in this case, the `longevity` of the storage and an API layer is limited by the lifetime of the organization that published the resource. For this solution, `trust` can be achieved by signing the resource using the key of the DID stored on-chain. This solution is not fully centralized as the organizations have their infrastructure with the database.

## Linking the resource to the DID

Linking a resource to a DID means associating a specific resource with a DID and resolving the resource via the Universal Resolver or application API or SDK or finding it on-chain.

In all the considered solutions this is achieved using a DID document and the algorithm for discovery and resource dereferencing.

## Considered Options

### DID document linkedResources field

The particular resource must be available via URL and the metadata of the resource are described in the `linkedResources` array.

Example:

```
{
  "@context": "https://w3id.org/did/v1",
  "id": "did:example:123456789abcdefghi",
  "publicKey": [{
    "id": "did:example:123456789abcdefghi#keys-1",
    "type": "Ed25519VerificationKey2018",
    "controller": "did:example:123456789abcdefghi",
    "publicKeyBase58": "7dNN1A8H4DwPU1h4btvohGadnbx8sHF2U6XJU6vLBBfA"
  }],
  "linkedResources": [{
    "url": "https://example.com/credentialschema/123",
    "type": "CredentialSchema",
    "name": "DrivingLicense"
  }]
}
```

#### Positive Consequences

- solution describes the simple way of linking the resources to the DID Document. This approach looks outdated and is not part of the did-core specification.

#### Negative Consequences

The drawbacks of the solution:

- interoperability: it is not a part of the DID-core specification anymore even if it is possible to find information about it
- interoperability: the resource should be fetched by the application or SDK
- trust: must be guaranteed by the underlying DLT, the DID document should have an anchor: hash, signature or Tx id that references to the DLT
- discoverability: information about the resource's metadata (author, version, content type) is absent
- scalability: DID document must be updated when a new resource is added, so the solution sacrifices `scalability` as the content of the DID document will grow

#### Out of the Scope

- longevity: should be guaranteed by the underlying DLT

### DID document didDocumentMetadata -> linkedResourceMetadata (Cheqd ADR)

Each resource entry is a part of the collection and is described in the `linkedResourceMetadata` field.

The solution is described in the Cheqd ARD in the [Links](#links) section of the current ADR

Example:

```
{
 "didDocumentMetadata": {
  "linkedResourceMetadata": [
    {
      "resourceURI": "did:cheqd:mainnet:1f8e08a2-eeb6-40c3-9e01-33e4a0d1479d/resources/f3d39687-69f5-4046-a960-3aae86a0d3ca",
      "resourceCollectionId": "1f8e08a2-eeb6-40c3-9e01-33e4a0d1479d",
      "resourceId": "f3d39687-69f5-4046-a960-3aae86a0d3ca",
      "resourceName": "PassportSchema", // First version of a Resource called PassportSchema
      "resourceType": "CL-Schema",
      "mediaType": "application/json",
      "created": "2022-07-19T08:40:00Z",
      "checksum": "7b2022636f6e74656e74223a202274657374206461746122207d0ae3b0c44298",
      "previousVersionId": null, // null if no previous version, otherwise, resourceId of previous version
      "nextVersionId": null, // null if no new version, otherwise, resourceId of new version
    }
  ]
 }
}
```

The solution is not fully interoperable with the SSI ecosystem, but it's probably the first successful specification that formalizes the DID-linked resources and the DID URL.

Cheqd's approach for linking the resources to the DID is not a part of the current version of DID specification. Even if it's possible to find some information about `linkedResources` and `linkedResourceMetadata` optional field of the DIDDoc in the cache of the search system or ChatGPT.

Looks like the ToIP specification is inspired by Cheqd's ADR.

#### Positive Consequences

- versioning of the resource, as metadata contains the references to the previous and next version
- collection definition is formalized and published on-chain and in the DID document, so all the resources are categorized
- discoverability: URI is replaced with DID URL that allows discovering the resource using Internal and/or Universal resolver
- trust: the `checksum` is provided, so it is possible to verify that the resource was not modified by 3rd party


#### Negative Consequences

- scalability: the DID document should be updated when the new resource is created
- interoperability: using the Universal Resolver is optional, so either SDK or internal application API must be used to fetch the resource
- standard: the `linkedResourceMetadata` field is not a standard part of the DID specification, so the application should be aware of how to deal with it


### DID URL dereferencing (W3C specification)

The current solution is based on the dereferencing algorithm described in the [DID-Resolution#dereferencing](https://w3c-ccg.github.io/did-resolution/#dereferencing) specification and describes how the DID resolver can dereference the resource linked to the DID. It does not describe where the resource is stored.

The main idea is an algorithm that allows using the DID URL and the information about the services in the DID Document that allows DID Resolver to compose the final resource URL and return the requested resource.

Dereference is performed by defining the service `id` and `relativeRef` params or `path` in the DID URL

**NOTE:**
The `service.type` property is not taken into account in this flow.
According to the did-core specification, the service type and its associated properties SHOULD be registered in the [DID Specification Registries](
https://www.w3.org/TR/did-spec-registries/#service-types).
So, defining and registering the `schemaService` or `resourceService` should be the next step to facilitate the interoperability of SSI systems.

Example 1: using `service` and `relativeRef`

The credential schema resource can be defined as a DID URL

```
did:prism:abcdefg?service=credentialschema&relativeRef=%2Fcredentialschemas%2F123e4567-e89b-12d3-a456-426614174000
```

and the DID Document must have the service defined

```
{  
  "service": [
    {
      "id": "did:prism:abcdefg#credentialschema",
      "type": "CredentialSchema",
      "serviceEndpoint": "https://agent.example.com/schema-registry"
    }
  ]
}
```

so, the Universal Resolver using the concrete DID resolver must dereference the resource as

```
https://agent.example.com/schema-registry/credentialschemas/F123e4567-e89b-12d3-a456-426614174000
```

and should return the instance of the credential schema

Example 2: is another variation but using `service` and `path` in the DID URL

```
did:prism:abcdefg/credentialschemas/123e4567-e89b-12d3-a456-426614174000?service=credentialschema
```

In this case, the DID Method may describe how the path should be resolved and the resource must be fetched.

#### Positive Consequences

- interoperability: the resource is resolved by the conformant DID resolver according to the specification
- discoverability: the resource defined in DID URL is resolved and fetched dynamically
- scalability: the DID document is not updated for each new resource

#### Negative Consequences

- specification: for the particular cases when the `path` is used in the DID URL, the resolution behavior must be described in the DID Method
- scalability: the algorithm contains 2 or 3 steps and the DID Document is always must be resolved in the first step

#### Out of the Scope
- trust, longevity, and technology stack are not specified in this solution

### DID URL Dereferencing (Trust over IP specification - outdated)

[ToIP specification](https://wiki.trustoverip.org/display/HOME/DID+URL+Resource+Parameter+Specification) is an analog of the W3C dereferencing specification and describes the convention for dereferencing the resources from the DID URL

The main idea is the same: use the DID URL and a combination of convention and the DID method to resolve the digital resource associated with the DID.

But instead of relying on the `service` and the `relativeRef` parameter, the ToIP spec is focused on the `resource` parameter - so, if the DID URL contains the `resource` parameter - it must return the resource.

Example:

```
did:example:21tDAKCERh95uGgKbJNHYp/some/path?resource=true

did:example:21tDAKCERh95uGgKbJNHYp/some/longer/path?resource=json

did:example:21tDAKCERh95uGgKbJNHYp/uuid:33ad7beb-1abc-4a26-b892-466df4379a51/?resource=ld+json

did:example:21tDAKCERh95uGgKbJNHYp/resources/uuid:33ad7beb-1abc-4a26-b892-466df4379a51/?resource=cbor
```

The main disadvantage of this approach is that the logic for resolving and fetching the resource associated with the given DID URL completely relies on DID method specification (in the W3C variation it's just a convention and the algorithm for the resource resolution)

ToIP specification doesn't describe the details about the storage of the underlying resource. It might be DLT (blockchain or IPFS) or classic cloud or on-premise storage.

### DID URL Dereferencing (Trust over IP specification - latest)

The new specification for DID URL dereferencing is an improved specification with recommended Cheqd idea to publish the resource metadata in the DID Document.

The main difference with the previous specification is an introduction of parameters that can discover the resource (instead of using `resource` field only) and simplification of the Cheqd's approach by skipping the `collection` abstraction.

The DID Document refers to the associated resource via linked resource metadata.

The changes to the DID method are also required (described in the Verifiable Data Registry and DID Method Requirements)

The current status of the document is a draft, but it's going to be published in the did-core specification.

The list of resource parameters with descriptions is the following:

- `resourceUri` (required): A string or a map that conforms to the rules of [RFC3986] for URIs which SHOULD directly lead to a location where the resource can be accessed from.
- `resourceCollectionId` (optional): A string that conforms to a method-specific unique identifier format.
- `resourceId` (optional): A string that conforms to a method-specific unique identifier format.
- `resourceName` (required): A string that uniquely names and identifies a resource. This property, along with the resourceType below, can be used to track version changes within a resource.
- `resourceType` (required): A string that identifies the type of resource. This property, along with the `resourceName` above, can be used to track version changes within a resource. Not to be confused with the media type. (TBC to add to DID Spec Registries)
- `resourceVersionId` (optional): A string that uniquely identifies the version of the resource provided by the resource creator as a tag.
- `mediaType` (required): A string that identifies the IANA-registered Media Type for a resource.
- `created` (required): A JSON String serialized as an XML DateTime normalized to UTC 00:00:00 and without sub-second decimal precision.
- `checksum` (optional): A string that provides a checksum (e.g. SHA256, MD5) for the resource to facilitate data integrity.
- `previousVersionId` (optional): The value of the property MUST be a string. This is the previous version of a resource with the same resourceName and resourceType. The value must be 'null' if there is no previous version.
- `nextVersionId` (optional): The value of the property MUST be a string. This is the previous version of a resource with the same resourceName and resourceType. The value must be 'null' if there is no previous version.

This specification describes many important aspects:

- the list of the query parameters in the DID URL for dereferencing the resource and error messages,
- DID Method and VDR requirements, and
- DID Resolver requirements

#### Positive Consequences

- interoperability: the resource is resolved in a standard way according to the ToIP specification following W3C specification for DID URL dereferencing
- discoverability: the resource defined in DID URL is resolved and fetched dynamically
- scalability: compared to W3C specification, the DID Document is not required to fetch the resource, so instead of 2-3 steps (calls), the resource resolution should be completed in a single step. The behavior must be described in the DID Method and implemented by the DID resolver.
- trust: publishing the `checksum` of the resource inside of the DID Document allows other SSI system to check the resource validity.

#### Negative Consequences

- scalability: the specification is inspired by the Cheqd approach to store the linkedResourceMetadata inside of the DID Document. ToIP specification describes this functionality as optional ("Through associating the resource with a DID Document, the DID Document may generate associated metadata about the resource")
- complexity: the specification is the most complex to fetch the resource, so it's not trivial to implement it for all DIDs in the SSI ecosystem.
- specification: the resolution logic of resources must be described in the DID method and implemented in the DID resolver. As a consequence of this approach, the solution must either communicate directly with the DLT or rely on the SaaS layer for fetching the resources.

#### Out of the Scope

- longevity, and technology stack are not specified in this solution but must be guaranteed by the underlying DLT


### RootsID - Cardano AnonCreds (Implementation of ToIP at the Cardano stack)
RootsID adopted the AnonCreds specification to store the credential schema and credential definition on the Cardano blockchain.

Links to the implementation and the method description are in the #Links section for this ADR

It is a proof of concept implementation of ToIP specification of DID URL dereferencing for resolving the resources linked to the DID in TypeScript and Python using Blockfrost REST API to the Cardano blockchain. The Blockfrost SaaS middle layer is used for publishing and fetching the Tx from the Cardano blockchain.

The solution is limited to storing AnonCreds entities only but can be extended to store the general resources.

For more details, please refer to the source code.

As the solution is based on the latest ToIP specification, it derives all positive and negative consequences from the previous but contains the concrete implementation for the Cardano blockchain that solves the trust and longevity aspects of the technical solution.

#### Positive Consequences

- interoperability: the resource is resolved in a standard way according to the ToIP specification following W3C specification for DID URL dereferencing
- discoverability: the resource metadata is published in the DID Document
- trust & longevity: is guaranteed by the underlying Cardano blockchain
- technology stack: the solution is leveraging Blockfrost REST API for communicating with the Cardano blockchain
- technology stack: the solution is stateless and is much cheaper in terms of the infrastructure cost
- technology stack: the solution is implemented in Python and TypeScript (mobile platforms can use the same approach as well)


#### Negative Consequences
- scalability: the specification is inspired by the Cheqd approach to store the linkedResourceMetadata inside of the DID Document
- the convention for references and the logic must be carefully reviewed:
  - `schemaId` in this solution is `{didRef}/resources/{cardano_transaction_id}`, so it doesn't refer to the `id` but to the Tx where everything else is stored (it's an interesting idea for a stateless design)
  - resource metadata is built according to the ToIP specification but for AnonCreds entities only: credential schema and credential definition.
- technology stack: it doesn't fit to current platform, but can be used for inspiration.


### Hyperledger AnonCreds

According to the AnonCreds specification, such kinds of resources as credential schema and credential definition are stored on-chain. Indy blockchain is used by the Hyperledger technology stack.

The credential schema and definition are not signed by the issuer, but the transaction with the underlying resource is published by the issuer. So, the integrity of the resource is guaranteed by the fact that it's published inside of the transaction signed by the issuer.

Example of the credential schema transaction:

```
{
  "txn": {
    "data": {
      "data": {
        "attr_names": [
          "birthlocation",
          "facephoto",
          "expiry_date",
          "citizenship",
          "name",
          "birthdate",
          "firstname",
          "uuid"
        ],
        "name": "BasicIdentity",
        "version": "1.0.0"
      }
    },
    "metadata": {
      "digest": "06bf8a90335563826154700bf80003598932c8ffaffd4f3656fd8ed604bbb639",
      "endorser": "Ar1YzNwcM74M2Z4XKUWXMW",
      "from": "Y6LRXGU3ZCpm7yzjVRSaGu",
      "payloadDigest": "44e0181c9f9d5080434f9bf11801f1b0768a6b985195e14d56e5dab06fde0cb8",
      "reqId": 1632381635230531300,
      "taaAcceptance": {
        "mechanism": "at_submission",
        "taaDigest": "8cee5d7a573e4893b08ff53a0761a22a1607df3b3fcd7e75b98696c92879641f",
        "time": 1632355200
      }
    },
    "protocolVersion": 2,
    "type": "101",
    "typeName": "SCHEMA"
  },
  "txnMetadata": {
    "seqNo": 73904,
    "txnId": "Y6LRXGU3ZCpm7yzjVRSaGu:2:BasicIdentity:1.0.0",
    "txnTime": "2021-09-23T07:20:40.000Z"
  }
}
```

The resource (credential schema) in the current example can be discovered using Indy SDK by the following id:
```
Y6LRXGU3ZCpm7yzjVRSaGu:2:BasicIdentity:1.0.0
```

Technical details and flows are described in the [AnonCreds](https://hyperledger.github.io/anoncreds-spec/) specification.

#### Positive Consequences

- discoverability: the credential schema or any other resource is discovered using the Indy SDK
- scalability and longevity: are guaranteed by the underlying blockchain technology
- trust: is achieved by underlying blockchain technology, the transaction with the resource contains the hashes and contains the `digest`, `taaDigest`, and `payloadDigest` fields

#### Negative Consequences

- interoperability: the current solution for storing the resources on-chain is coupled with the Indy blockchain and SDK (it will be mitigated by decoupling the AnonCreds specification from the Indy blockchain)
- vendor lock: the solution is tightly coupled to the Indy blockchain (it will be mitigated in the future by decoupling the Aries project from the underlying Indy blockchain)

**Note**: there is a new specification of the AnonCreds that is decoupled from the Hyperledger stack. The specification can describe more details about the resource publishing.

### Trinsic Solution  

The Trinsic solution is built on top of the Hyperledger Aries platform on the Indy blockchain
The main benefit is the Trinsic application layer that defines the domain models, entities, REST API and SDK for working with these.

The resource, such as credential schema, is stored on-chain, but the technical complexity and low-level details are hidden under `Template` and [`Template Service`](https://docs.trinsic.id/reference/services/template-service/#template-service)

#### Positive & Negative Consequences

Are similar to the Hyperledger AnonCreds solution

The main benefit of the Trinsic approach to storing resources is a good abstraction layer, documentation, REST API and a variety of supported programming languages in SDKs for dealing with underlying resources.


### Solution #1 (W3C with dynamic resource resolution)

The solution for storing the resources linked to the DID depends on two decisions that are described in the Context and Problem Statement:

- where the resource is stored
- how the resource is discovered and fetched

Taking into account the advantages and disadvantages of the existing solutions the decision about the solution for the Identus platform might be the following:

-the resource is linked to the DID by convention specified in the W3C specification, so specifying the resource in the DID URL and defining the service endpoint that exposes the resource allows to discover and fetch the resource using the Universal Resolver
- as an option, the same resource can be discovered and fetched by the Identus platform backend and SDK without loading the Universal resolver
- the resource integrity must be guaranteed by one of the following options:
  - by signing the payload with one of the DID's keys or
  - by publishing the resource metadata that contains the information about the resource (id, type, name, media type, hash) on-chain or
  - for the resource that is less than the blockchain limitation (up to 64KB) by publishing the resource together with the hash, and/or signature
- the resource can be stored in the cloud storage - PostgreSQL database - for indexing and lookup API

As the Identus platform can leverage the Cardano blockchain and there is a strong requirement for longevity and security - the resource together with the signature and/or hash must be stored in the Cardano blockchain.

An example of this solution will be the following (concerning the current infrastructure and services):

- prism-node must be able to store the generic resource payload, signature and/or hash on-chain and restore the given resource in the underlying database (PostgreSQL) for indexing and lookup API
- credential schema (or any other resource module) must be a part of the Atala SSI infrastructure and allow
  - publishing the concrete resource as a generic resource using the prism-node API
  - expose the API for discovery and fetching the resource by URL
  - expose the API for managing the resources (create, publish, lookup with pagination)
- the Universal Resolver for the DID Method must be able to discover and fetch the resource by DID URL
- is needed, SDK and backend services can fetch the resources directly (not via the Universal Resolver)

Example:

Given the credential schema with the signature:

```
{
  "$id": "driving-license-1.0",
  "$schema": "http://json-schema.org/draft-07/schema#",
  "description": "Driving License",
  "type": "object",
  "properties": {
    "credentialSubject": {
      "type": "object",
      "properties": {
        "emailAddress": {
          "type": "string",
          "format": "email"
        },
        "givenName": {
           "type": "string"
        },
        "familyName": {
           "type": "string"
        },
        "dateOfIssuance": {
           "type": "datetime"
        },
        "drivingLicenseID": {
           "type": "string"
        },
        "drivingClass": {
           "type": "integer"
        },
        "required": [
          "emailAddress",
          "familyName",
          "dateOfIssuance",
          "drivingLicenseID",
          "drivingClass"
        ],
        "additionalProperties": true
      }
    }
  },
  "proof": {
    "type": "RsaSignature2018",
    "created": "2023-04-18T10:30:00Z",
    "jws": "eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJpZCI6Imh0dHBzOi8vZXhhbXBsZS5jb20vY3JlZGVudGlhbHMvc3ViamVjdCIsInR5cGUiOiJWZXJpZmljYWxpY0NyZWRlbnRpYWwiLCJpc3N1ZXIiOiJodHRwczovL2V4YW1wbGUuY29tL2lzc3VlciIsImlzc3VlckRhdGUiOiIyMDIzLTA0LTE4VDEwOjMwOjAwWiIsImV4cGlyYXRpb25EYXRlIjoiMjAyNC0wNC0xOFQxMDowOTo1MFoiLCJjcmVkZW50aWFsU3ViamVjdCI6eyJpZCI6Imh0dHBzOi8vZXhhbXBsZS5jb20vY3JlZGVudGlhbHMvc3ViamVjdC9zdWJqZWN0IiwibmFtZSI6IkpvaG4gRG9lIiwic2lnbmF0dXJlIjoxMH19.OesuS6eC0gVh8SZpESM7Z4Yln9sGSsJHQ8s0LlcsD99H6_7U6vukUeT2_GZTtuTf9SwIfdtgViFTOfhzGTyM6oMGEeUJv6Umlh6TQ1fTm9XEDQV7JDBiaxRzV7S_vS6i",
    "alg": "RS256"
  }
}
```

In order to store it as a general resource on-chain, it should be binary encoded into CBOR format (or Base64 encoded string) and the metadata must be added to it.

For example, it might look like the following JSON object:

```
{
        "id": "f3d39687-69f5-4046-a960-3aae86a0d3ca",
        "name": "DrivingLicense-1.0",
        "resourceType": "CredentialSchema",
        "mediaType": "application/json", // MIME Type of the resource
        "data": "SGVsbG8sIHdvcmxk", // base 64 encoded or CBOR file
        "did": "did:prism:abdcefg", // the DID reference to link the resource to the DID and create the anchor to the DID
}
```

... and published on the Cardano blockchain as a payload of the AtalaOperation object, so can be retrieved from the blockchain and added to the indexed database for resolution by the REST API

Given there is an Agent or CredentialSchema service that exposes the REST API for fetching the credential schema by ID (in the current implementation it corresponds to the Cloud Agent `/schema-registry/credential-schema/{uuid}`, but later might be changed to `/credential-schema/{didRef}/{id}?version={version}` )

So, the services of the Identus platform and SDKs can resolve the given schema by URL and use the convenient lookup API with filtering and pagination to manage the credential schema in the Web application.

To define the `schemaId` in the message of Issue Credential and Present Proof protocols the following DID URL can be used:

```
did:prism:abcdefg1234567890?service=credentialschema&relativeRef=%2Ff3d39687-69f5-4046-a960-3aae86a0d3ca
```

The version is skipped as for resolving the single resource we don't need a `version` parameter
`f3d39687-69f5-4046-a960-3aae86a0d3ca` - is a unique identifier that is derived from the triple: didRef, id and version.

So, having the following service endpoint definition in the DID Document:


```
{  
  "service": [
    {
      "id": "did:prism:abcdefg#credentialschema",
      "type": "CredentialSchemaService",
      "serviceEndpoint": "https://agent.example.com/schema-registry/schemas"
    }
  ]
}
```

And having the logic for dereferencing the DID URL in the PRISM DID Resolver, any system in the SSI ecosystem can fetch this resource and validate its authorship.

Storing resources larger than 64KB is out of the scope of this ADR. These must be stored in a slightly different way, for instance, the image ~10MB, can be stored and linked to the DID Document in the following way:

- the image is stored in the cloud database in a binary format
- the metadata and the hash of the image are stored on-chain
- optionally, the signature of the owner DID can be generated for the payload and stored together with the hash
- to prove the integrity of the image file, the hash of the binary representation must be the same and/or the signature must be verified
- the resource can be fetched in the same way and the credential schema from the previous example

#### Positive Consequences

- discoverability: the credential schema or any other resource is discovered using the Universal Resolver
- scalability: the size of DID document doesn't grow when a new resource is added, the number of the resources is limited by the scalability of the underlying database and the blockchain
- longevity: for the resource that can be stored on-chain the longevity is 100% guaranteed by the underlying blockchain technology
- trust: is achieved by cryptography and the underlying blockchain technology, the transaction with the resource contains the hashes and the signatures that can be verified
- interoperability: any system that uses Universal Resolver can fetch the resource by dereferencing the DID URL (following the  W3C specification)
- vendor lock: by publishing the specifications and the algorithms for fetching the data, the resource can be resolved by any other SSI system

#### Negative Consequences

- longevity: for the resource that can not be stored on-chain because of the large size longevity is guaranteed by the cloud recovery procedures and data backup. As an option for mitigating this problem, the resource can be stored in IPFS (additional ADR is required for this)
- vendor lock: the solution is coupled to the Cardano blockchain

**NOTE:** one of the main concerns of this ADR is storing the resources on-chain because of size limitation, throughput, latency and cost. This option allows to postpone this decision and implement the DID-linked resources without the need of storing resources on-chain.

---

### Solution #2 (ToIP specification implementation)

ToIP specification can be used to implement the resource resolution.
To implement it the following things are required:

- specify in the DID method the logic of resolution of the resources from the DID URL
- specify the service mapping in the DID method and implement the resource resolution logic in the DID resolver
- add `didDocumentMetadata.linkedResourceMetadata` field to the DID method and implement the logic in the VDR layer
- implement the service layer according to the ToIP specification

ToIP solution specifies the requirements to the VDR (blockchain) that is not easy to achieve with the current implementation of the Identus platform.
According to this specification, the Universal Resolver must have the direct access to the blockchain or use a centralized layer for fetching the resources over REST API.
Before implementing this specification is the Identus platform we need to answer the following questions:

- who is hosting the `prism-node` infrastructure for the Universal Resolver and how it's managed?
- should we make the PRISM DID Method responsible for resource resolution logic?

#### Positive Consequences

- interoperability: the resource is resolved in a standard way according to the ToIP specification following W3C specification for DID URL dereferencing
- discoverability: the resource defined in DID URL is resolved and fetched dynamically
- scalability: compared to W3C specification, the DID Document is not required to fetch the resource, so instead of 2-3 steps (calls), the resource resolution should be completed in a single step. The behavior must be described in the DID Method and implemented by the DID resolver.

#### Negative Consequences

- complexity: the solution is complex and over-engineered. A lot of components and flows must be defined to fetch the resource.
- specification: current approach might be changed as it's still in the draft status, so implementing it is risky

## Decision Outcome

Each option has technical challenges and limitations, but it's possible to define the following decisions as an outcome:

- the resource MUST be stored on-chain to guarantee trust and longevity aspects, for the Identus platform it is the Cardano blockchain
- the resource SHOULD be indexed for quick lookup over the API
- the resource CAN be referenced in the DID Document for additional discoverability
- the resource MUST be dereferenced from the DID URL according to W3C or ToIP specification and implementation
- the resource resolution CAN be described in the DID Method (for the dynamic resource linking and W3C dereferencing algorithm it's not required)
- the complexity of the solution SHOULD be adequate to the original goal: get the resource linked to the DID
- the solution SHOULD be scalable
- the solution MUST be interoperable and easily adopted by the SSI ecosystem

The solution option #1 is considered a good option as it satisfies the requirements and the majority of the negative consequences are mitigated.
The following comparison table is a summary of the available options.

| Option                                     | Simplicity                           | Trust              | Scalability                          | Interop                              | Discoverability    | Decentalisation    |
|--------------------------------------------|--------------------------------------|--------------------|--------------------------------------|--------------------------------------|--------------------|--------------------|
| linkedResources field                      | :heavy_plus_sign:                    | :heavy_check_mark: | :heavy_minus_sign:                   | :heavy_minus_sign:                   | :heavy_plus_sign:  | N/A                |
| linkedResourceMetadata (Cheqd)             | :heavy_minus_sign:/:heavy_plus_sign: | :heavy_check_mark: | :heavy_minus_sign:/:heavy_plus_sign: | :heavy_plus_sign:                    | :heavy_plus_sign:  | :heavy_check_mark: |
| DID URL Dereferencing (W3C specification)  | :heavy_plus_sign:                    | N/A                | :heavy_plus_sign:                    | :heavy_plus_sign:                    | :heavy_minus_sign: | :heavy_check_mark: |
| DID URL Dereferencing (ToIP specification) | :heavy_minus_sign:                   | :heavy_check_mark: | :heavy_plus_sign:/:heavy_minus_sign: | :heavy_plus_sign:/:heavy_minus_sign: | :heavy_plus_sign:  | :heavy_check_mark: |
| RootsID - Cardano AnonCreds                | :heavy_plus_sign:                    | :heavy_check_mark: | :heavy_plus_sign:/:heavy_minus_sign: | :heavy_plus_sign:                    | :heavy_plus_sign:  | :heavy_check_mark: |
| Hyperledger AnonCreds                      | :heavy_plus_sign:                    | :heavy_check_mark: | :heavy_plus_sign:                    | :heavy_minus_sign:                   | :heavy_minus_sign: | :heavy_check_mark: |
| Trinsic                                    | :heavy_minus_sign:                   | :heavy_check_mark: | :heavy_plus_sign:/:heavy_minus_sign: | :heavy_minus_sign:                   | :heavy_minus_sign: | :heavy_check_mark: |
| Solution #1 W3C                            | :heavy_plus_sign:                    | :heavy_check_mark: | :heavy_plus_sign:                    | :heavy_plus_sign:                    | :heavy_minus_sign: | :heavy_check_mark: |
| Solution #2 ToIP                           | :heavy_minus_sign:                   | :heavy_check_mark: | :heavy_minus_sign:/:heavy_plus_sign: | :heavy_plus_sign:/:heavy_minus_sign: | :heavy_plus_sign:  | :heavy_check_mark: |

---

Each option reviewed in this ADR is a composition of the following architectural decisions:

- the resource is stored on-chain or off-chain
- the VDR layer is managed or unmanaged (for instance, leveraging the Blockfrost REST API can simplify the solution, but might be expensive at scale)
- domestic or official (W3C/ToIP specification implementation
- static/dynamic resource discoverability (is resource metadata stored in the DID Document or not)
- DID URL dereferencing algorithm and naming convention
- level of trust: Tx signature, resource hash, resource signature
- decentralized or SaaS solution
- SDK, Universal Resolver or REST API for fetching the resource

The main benefits of option #1 for the Identus platform are the following:

- the resource is stored on-chain
- the resource is published and indexed by the managed VDR layer (prism-node)
- the resource is available via REST API & SDK for the product-level applications
- the resource is dereferenced via the DID URL in the DID resolver
- the resource is linked to the DID dynamically (using DID URL + dereferencing algorithm)
- this solution is scalable and decentralized (anyone can deploy the Identus stack)
- level of trust can be guaranteed by the underlying VDR and enforced by hashes or signatures of the resource


## Links

- [Our Approach to DID-Linked Resources](https://blog.cheqd.io/our-approach-to-resources-on-ledger-25bf5690c975)
- [Context for Developing DID-Linked Resources](https://docs.cheqd.io/identity/guides/did-linked-resources/context)
- [ADR 002: DID-Linked Resources](https://docs.cheqd.io/identity/architecture/adr-list/adr-002-did-linked-resources)
- [Hyperledger Anoncreds - Schema Publisher: Publish Schema Object](https://hyperledger.github.io/anoncreds-spec/#schema-publisher-publish-schema-object)
- [ToIP - DID URL Resource Parameter Specification](https://wiki.trustoverip.org/display/HOME/DID+URL+Resource+Parameter+Specification)
- [ToPI - DID-Linder Resources Specification](https://wiki.trustoverip.org/display/HOME/DID-Linked+Resources+Specification)
- [DID-Core#did-parameters](https://www.w3.org/TR/did-core/#did-parameters)
- [DID-Resolution#dereferencing](https://w3c-ccg.github.io/did-resolution/#dereferencing)
- [RootsID AnonCreds Methods](https://github.com/roots-id/cardano-anoncreds/blob/main/cardano-anoncred-methods.md)
- [RootsID Cardano AnonCreds repo](https://github.com/roots-id/cardano-anoncreds)
</file>

<file path="documentation/adrs/decisions/2023-05-09-message-routing-for-multi-tenant.md">
# Routing Requests to the Correct Tenant

- Status: accepted
- Deciders: Yurii Shynbuiev,David Poltorak, Benjamin Voiturier, Shailesh Patil
- Date: [2023-05-09]
- Tags: multi-tenant, routing, message

## Context and Problem Statement
The Cloud Agent in multi-tenancy is still a single agent running, however, some of the resources are now shared between the tenants of the agent.
Each tenant has their own keys, with their own DIDs, connections. Transports and most of the settings are still shared between agents.
All the API endpoints are same from outside

Multi-tenancy, message routing can be used to ensure that messages are delivered only to the intended recipient or tenants, and not to unauthorized tenants.

Backend services: Cloud Agent use PostgreSQL. Authentication and authorization
<pre class="mermaid">
    sequenceDiagram
    autonumber
    actor H as Holder(DidComm)
    actor T as Tenant(Issuer)
    participant A as CloudAgent
    participant W as Wallet
    participant DB as Database[did \<- tenantId]
    T->>A: Register Tenant
    activate A
    A->>W: Create wallet
            activate W
                note over W: Each Tenant has his own wallet where keys and dids are stored
                W-->>A: tenantId
            deactivate W
            note over T, A: Subsequent requests include JWT header
            activate DB
                note over DB: did -> tenantId or did -> walletId
                T->>A: Create PeerDID[JWT Header]
                A->>A: authorised token extract tenantID
                alt JWT validation
                    A-->>T: 200 OK & JWT
                else No user
                    A-->>T: 401 Unauthorized
                end
                T-->>A: If authorised Create PeerDID
                A-->>DB: Update [DID(PeerDID) -> tenantID]
                A->>H: send DIDCOMM message to holder did
            deactivate DB
    deactivate A
    activate H
        H->>A: DIDCOMMV2 message to Agent(did)
        A-->>DB:lookup to Agent DID identify tenantId
        A-->>A:decrypt message
    deactivate H
</pre>

<script src="https://cdnjs.cloudflare.com/ajax/libs/mermaid/9.2.1/mermaid.min.js"/>
<!--
<script>
  mermaid.initialize({ startOnLoad: true });
</script>
-->
</file>

<file path="documentation/adrs/decisions/2023-05-15-mediator-message-storage.md">
# Mediator message storage

- Status: accepted
- Deciders: Yurii Shynbuiev, Benjamin Voiturier, Shailesh Patil, Fabio Pinheiro , David Poltorak
- Date: [2023-05-09]
- Tags: storage, db, message, mongo, postgres, sql

## Context and Problem Statement
Mediator storage
Relational databases like PostgreSQL store data in structured tables, with rows and columns that help establish relationships between various tables and entities.
SQL is used in PostgreSQL to save, retrieve, access, and manipulate the database data.
While PostgreSQL may be ideal for managing structured data streams, it tends to struggle when dealing with big unstructured data, as maintaining such relations can increase time complexity significantly.
Postgres SQL relies on relational data models that need to defined in advance
Change in any field in table requires lengthy process of migration scripts run and to maintain but it works there are tools available and we also use it PrismAgent,
But maintenance cost of software is higher.

Contrastingly, non-relational databases like MongoDB excel in these scenarios because data isn't constrained to a single table.
This approach permits massive data streams to be imported directly into the system without the burden of setting up increasingly complex relationships and keys.
MongoDB typically stores data within documents using JSON (JavaScript Object Notation) or BSON (Binary JavaScript Object Notation), which simplifies the handling of big data and complex data streams.
document database supports a rapid, iterative cycle of development the way that a document database turns data into code.
MongoDB is faster at inserting and for queries that use nested references instead of joins
In Mediator the data which we send or receive is json message and to process json message we can avoid the unnecessary serialization and deserialization by having it stored in document database.
Postgres SQL by default in vertically scalable where as Mongo can scale horizontally, This can help in scalability problem
Mediator messages store in simple and straight forward write there is no transactional workflow involved so we don't gain much by using relational db like postgres.

Below are the 2 options which we can use to reduce infrastructure management
MongoDB Atlas. Fully managed MongoDB in the cloud which can reduce the infrastructure management [https://www.mongodb.com/atlas/database]
Amazon DocumentDB (with MongoDB compatibility)  [https://aws.amazon.com/documentdb/]

## Decision Drivers
- DIDCOMM messages are json based
- flexibility to store the data
- Reduce serialisation deserilisation of the data
- Easy to write the json queries
- scalability
- low maintainance

## Considered Options
- PostgresSQL (Storing unstructured data (JSON) and quering data (JSON), scalability)
- MongoDB (Storing unstructured data (JSON) and quering data (JSON), scalability)
- Kafka Stream (Storing unstructured data (JSON) and quering data (JSON), scalability and streaming)

## Decision Outcome

Chosen option: MongoDB because of storing unstructured json data and json queries that requires minimal changes to the existing code and provides the benefits for the current use cases.
Is a NoSQL database that uses a document-oriented data model. Data is stored in a semi-structured format (BSON, similar to JSON), which can easily accommodate changes in data structure. This makes MongoDB particularly suitable for large volumes of data that may not be easily modeled in a relational schema.
Kafka Stream was also considered but current usecases are more towards storage of the messages, streaming is not the usecase for all the scenarios and the  mediator pickup protocol ([https://didcomm.org/pickup/3.0/]) requirements need the flexibily to delete the messages read.

### Positive Consequences

- flexility to store unstructured data in json format
- easy work with storage of unstrutured data and json queries
- low cost for with no sql migrations required.
- Uses a flexible, JSON-like query language. While this provides great flexibility
- Is designed for easy horizontal scalability, which is achieved through sharding.

### Negative Consequences

- Extra infrastructure and additional resource to be maintained  

## Pros and Cons of the Options

### MongoDB

MongoDB provides flexibility with json storage and queries

- Good, because it is horizontally scalable
- Good, because typically performs better with large, unstructured datasets and write-heavy applications
- Good, because have strong communities and extensive support materials, so you can expect to find help when you encounter issues.
- Bad, Is not full ACID compliance
- Bad, Doesn't natively support complex joins like a relational database


## Refrences used

- [https://www.plesk.com/blog/various/mongodb-vs-postgresql/]
- [https://www.dbvis.com/thetable/json-vs-jsonb-in-postgresql-a-complete-comparison/]
- [https://severalnines.com/blog/overview-json-capabilities-within-postgresql/]
- [https://www.mongodb.com/docs/manual/core/schema-validation/]
- [https://www.mongodb.com/compare/mongodb-dynamodb]
- [https://www.projectpro.io/article/dynamodb-vs-mongodb/826]
</file>

<file path="documentation/adrs/decisions/2023-05-16-hierarchical-deterministic-key-generation-algorithm.md">
# Hierarchical deterministic key generation algorithm

- Status: accepted
- Deciders: Jesus Diaz Vico, Ezequiel Postan, Pat Losoponkul, Yurii Shynbuiev
- Date: 2023-05-16
- Revision-date: 2024-06-06
- Tags: key management, hierarchical deterministic, key derivation

Technical Story:

Current ADR is based on the Research Spike [Evaluation of Using a Single Mnemonic both for Cryptocurrency and Identity Wallets](https://drive.google.com/file/d/1SRHWRqY1C88eVuaS1v_uIAt-LNTLi9aO/view) document was written by Atala engineers:

- Jesus Diaz Vico (Atala Semantics team)
- Christos KK Loverdos (Atala Technical Director)
- Ezequiel Postan (Atala Semantics team)
- Tony Rose (Atala Head of Product)

Reviewed in 2024 by Atala engineers:
- Jesus Diaz Vico
- Ezequiel Postan
- Pat Losoponkul
- Gonçalo Frade
- Yurii Shynbuiev

The document covers motivation, the overview of BIP32-based HD wallets, and the main concept and implementation details.

## Context and Problem Statement

The Identus Platform must provide the hierarchical deterministic (HD) key derivation algorithm for the identity wallets managed by the platform (Cloud Agent and SDKs)

HD key derivation can be used for both `managed` and `unmanaged` solutions. In both cases, the key material is created from the `seed`.

In the case of a `managed` solution, the keys are created by the `Agent` or `SDK` and stored in the `secured storage` that is managed by the Identus platform.

In the case of an `unmanaged` solution, the key material is created by the tools (for instance, `identus-cli`) following similar rules, and is stored on the client side in the `secret storage` managed by the client.

We are going to be using different derivation implementations for secp256k1 and ed25519.

## Out of the Scope

### `did:peer`

DID peer key material derivation is out of the scope of this ADR, so this type of DIDs is created dynamically from the secure random

### Recovery Procedure

Key material recovery procedure is out of the scope of this ADR, but the main idea is the following: having a `seed` and the latest versions of the DID Documents published on-chain it is possible to recover all the key materials related to the latest state of the identity wallet related to DIDs.

### Implementation details

The HD key derivation algorithm is a part of the Apollo building block, the choice of the programming language is up to the engineering team.

### Secure Storage

Secure store implementation is a matter of another ADR. By now, the Hashicorp Vault is going to be used by the Identus platform by default.

### Backward Compatibility with the PRISM v1.4

The current decision doesn't have backward compatibility with the PRISM v1.4, but it can be mitigated by switching to the `unmanaged` way of key management for the DIDs created in v1.4 or by implementing the backward compatibility module in the Identus Platform


## Decision Drivers

- Deterministic key derivation for the Identus Platform and in all components: Cloud Agent (JVM), Identity Wallets (Android, iOS, Web)
- Possibility to use the same `seed` value for `crypto` and `identity` wallets.
- Compliance with BIP32 specification
- Key material migration between the wallets
- Key material recovery in case of disaster
- We must use different derivation standards bip32 with secp256k1 and ed25519-bip32

## Considered Option

Implement the HD key derivation algorithm according to the research spike for all the components of the Identus Platform.
The derivation path contains the following segments/layers:

m/wallet-purpose'/did-method'/did-index'/key-purpose'/key-index'

`wallet purpose` is used to distinguish the wallet purpose for the identity wallet and is a constant for the Identus platform `0x1D`, which looks like ID

`did-method` - the DID method that the key will be generated for. The value of `did-method` should be registered. The following are available values for the `did-method`:

- PRISM DID method - `0x1d`

`did-index` - the index of the DID, it's possible to create

`key-purpose` - the purpose of the key associated with the DID. There are the following available values for the `key purpose`:

- master key 1 - is the most privileged key type, when any other key is lost, you could use this to recover the others
- issuing key 2 - is used for issuing credentials only, it should be kept in a safe place to avoid malicious credentials being issued.
- key-agreement key 3 - is used to establish a shared symmetric key for secure end-to-end communication, use this key type to encrypt the content.
- authentication key 4 - is used to authenticate requests or log into services.
- revocation key 5 - is used for revoking credentials only, it should be kept in a safe place to avoid malicious credentials being issued.
- capability-invocation key 6 - is used to specify a verification method that might be used by the DID subject to invoke a cryptographic capability, such as the authorization to update the DID Document.
- capability-delegation key 7 - is used to specify a mechanism that might be used by the DID subject to delegate a cryptographic capability to another party, such as delegating the authority to access a specific HTTP API to a subordinate.

`key-index` - the index of the key pair

In order to generate key material (private and public keys):
- Secp256k1 ellipstic curve will be used with standard bip32 derivation
- Curve25519 (Ed25519) will be used with the standard bip32 implementation for [ed25519](https://ieeexplore.ieee.org/document/7966967)
- Future implementations will require their own implementations of the derive function, and very potentially at some point we may want to rework bip32 implementation to make it more agnostic, because a high percentage of the code is going to be the same.

`Seed` entropy must be used for the HD algorithm is 256 bits which corresponds to 24 words mnemonic

## Decision Outcome

The PRIMS platform uses HD key derivation algorithm for `managed` and `unmanaged` solutions based on the research spike the current ADR.

### Positive Consequences

- deterministic key material derivation among all components of the Identus platform
- BIP32 compliance (for both secp256k1 and ed25519 with their corresponding implementations)
- key material migration capability
- key material recovery capability

### Negative Consequences

- backward compatibility with the key material created by PRISM v1.4 version (can be mitigated by removing the `wallet_purpose` from the derivation path)

## Links

- [Evaluation of Using a Single Mnemonic both for Cryptocurrency and Identity Wallets](https://drive.google.com/file/d/1SRHWRqY1C88eVuaS1v_uIAt-LNTLi9aO/view)
</file>

<file path="documentation/adrs/decisions/2023-05-18-data-isolation-for-multitenancy.md">
# Data isolation for multi-tenancy

- Status: accepted
- Deciders: Benjamin Voiturier, Yurii Shynbuiev, Shailesh Patil
- Date: 2023-05-10
- Tags: multi-tenancy, data-isolation, PostgreSQL

Technical Story:

The Identus platform must support the multi-tenancy, so the data of the tenants must be isolated from each other and the access control policies must be applied to the data of each tenant.

This ADR is about the data isolation for multi-tenancy that must be implemented in the Cloud Agent.

## Context and Problem Statement

In a multi-tenant architecture, where multiple clients or tenants share the same infrastructure or application, data isolation is crucial to ensure the privacy and security of each tenant's data.

The specific requirements for data isolation may vary depending on the system and its specific needs. However, here are some common requirements for achieving data isolation in a multi-tenant architecture:

### Logical Separation

Tenants' data should be logically separated from each other, meaning that each tenant should have their own isolated environment within the system. This can be achieved through logical partitions, such as separate databases, schemas, or tables.

### Physical Separation

In addition to logical separation, physical separation can provide an extra layer of isolation. This involves segregating tenants' data onto separate physical resources, such as servers, storage devices, or networks. Physical separation helps prevent data leakage or unauthorized access between tenants.

### Access Controls

Robust access controls should be implemented to restrict access to each tenant's data. This includes authentication mechanisms to verify the identity of users or applications accessing the data and authorization rules to define what actions they are allowed to perform. Each tenant's access should be limited to their own data and not extend beyond their boundaries.

### Data Encryption

To protect data at rest and in transit, encryption techniques should be employed. This ensures that even if unauthorized access occurs, the data remains unreadable without the appropriate decryption keys. Encryption can be applied at various levels, including the application, database, or file system.

### Tenant-Specific Configuration

Each tenant may have specific configuration requirements, such as custom data schemas, access controls, or data retention policies. The system should allow for flexible configuration options to accommodate these individual tenant needs while maintaining isolation.

### Auditing and Logging

Comprehensive logging and auditing mechanisms should be in place to track and monitor access to tenants' data. This helps in detecting any unauthorized activities or potential breaches and provides an audit trail for forensic analysis if needed.

### Data Backup and Recovery

Robust data backup and recovery processes should be implemented to ensure the availability and integrity of tenants' data. Regular backups should be taken and securely stored, with mechanisms in place to restore data in the event of data loss or system failures.

### Performance and Scalability

Data isolation mechanisms should be designed to minimize performance impacts and provide scalability. The architecture should be able to handle increasing numbers of tenants and their data without sacrificing performance or compromising isolation.

### Technology Stack

The Identus platform heavily uses the relational database PostgreSQL. Even having the abstraction as a Data Access Layer (DAL), introducing the alternative solution implies a lot of engineering efforts for refactoring and is not recommended at the current phase of the platform development.

## Decision Drivers

- Logical Separation: Tenants' data should be logically separated from each other, meaning that each tenant should have their own isolated space within the system. This can be achieved through logical partitions, such as separate databases, schemas, tables or Row Security Policies (RSP in the PostgreSQL)

- Physical Separation: In addition to logical separation, physical separation can provide an extra layer of isolation. This involves segregating tenants' data into separate physical resources, such as servers, storage devices, or networks. Physical separation helps prevent data leakage or unauthorized access between tenants.

Logical and Physical Separations define the level of `isolation` for storage, connection pool, and computation resources: CPU, RAM, and Disk I/0.

- Access Controls: Robust access controls should be implemented to restrict access to each tenant's data. This includes authentication mechanisms to verify the identity of users or applications accessing the data and authorization rules to define what actions they are allowed to perform. Each tenant's access should be limited to their data and not extend beyond their boundaries.

- Data Encryption: To protect data at rest and in transit, encryption techniques should be employed. This ensures that even if unauthorized access occurs, the data remains unreadable without the appropriate decryption keys. Encryption can be applied at various levels, including the application, database, or file system. Data encryption at rest and in transit is a matter of infrastructure configuration and shouldn't play a significant role in the following ADR.

- Tenant-Specific Configuration: Each tenant may have specific configuration requirements, such as custom data schemas, access controls, or data retention policies. The system should allow for flexible configuration options to accommodate these individual tenant needs while maintaining isolation. For simplification, in the scope of the following ADR, all tenants will not have the custom configuration, but it can be implemented by having one of the following physical separations, such as separated schemas or instances of the database.

- Auditing and Logging: Comprehensive logging and auditing mechanisms should be in place to track and monitor access to tenants' data. This helps in detecting any unauthorized activities or potential breaches and provides an audit trail for forensic analysis if needed. Auditing and Logging should provide visibility if the tenant tried to access the data that don't belong to it and the basic statistic to determine resource unitisation by the tenant (requests/sec, request latency).

- Data Backup and Recovery: Robust data backup and recovery processes should be implemented to ensure the availability and integrity of tenants' data. Regular backups should be taken and securely stored, with mechanisms in place to restore data in the event of data loss or system failures. The current aspect corresponds to the SRE area and will not be taken into account in the following ADR and can be guaranteed by the infrastructure layer.

- Performance and Scalability: Data isolation mechanisms should be designed to minimize performance impacts and provide scalability. The architecture should be able to handle increasing numbers of tenants and their data without sacrificing performance or compromising isolation.

- The Complexity of the Implementation: It's essential to build the multi-tenancy capability for the Identus platform without the introduction of unnecessary complexity at the application layer, operation layer, and maintenance, in a way that allows evolving the platform naturally along with the growth of the users, scalability requirements, and real business needs.

## Considered Options

All considered options are built using the PostgreSQL database

- Row Security Policies (RSP) - Shared Database, Shared Schemas, Shared Tables, Separate Rows
- Table per Tenant - Shared Instance, Shared Database, Shared Schemas, Separate Tables
- Schema per Tenant - Shared Instance, Shared Database, Separate Schemas
- Database per Tenant - Shared Instance, Separate Database
- Instance per Tenant - Separate Instance,
- Citus extension (sharding) - Sharded Instances
- AWS (sharding) - Sharded Instances

The first five options are the common architecture patterns for multi-tenancy built using PostgreSQL

The level of isolation grows up from the 1st to the 5th option

The maintenance cost grows up from the 1st to the 5th option

The infrastructure cost grows significantly using the 5th option

Sharding options (Citus extension and AWS sharding) must be used with the combination of one of the first five options to provide the scalability and performance aspects by sharding the instances of the database. These options must be considered for SaaS solutions when the number of tenants grows to millions and cannot be managed by a single instance of the database because of resource constraints: disk space, CPU, RAM, disk I/O, and network I/O.

## Decision Outcome

The `Row Security Policies` option is the easiest for implementation at the current phase of the Identus platform development.
A single instance of the PostgreSQL database can keep the data and handle requests of hundreds of thousands of tenants leveraging the Row Security Policies without additional operation and infrastructure costs.

At the same time, the Cloud Agent architecture can support `Instance per Tenant` or `Database per Tenant` configuration by isolating the DAL under the repository interface. So, these options also can be considered for organizations with a lot of tenants to provide better isolation and data protection guarantees.
These two options are excellent for a group of tenants under a single organisation or can be considered for tenants that require geographical separation of data, but should not be used for a single tenant.

Moreover, for the SaaS application to manage thousands of organizations and millions of tenants, the `Row Security Policies` option will not be enough because of the resource limitations and amount of requests to the database. In this case, one of the PostgreSQL sharding options is required together with `Row Security Policies`. So, either `Citus extension` or Amazon RDS sharding should be used. `Citus extension` is a preferred way for an on-premise environment, but, it probably, can be used in AWS as well.

### Out of the Scope

- Access Controls - must be enforced by the access control at the application level by implementing a policies enforcement point (PEP) and resolving the tenant.
- Auditing and Logging - must be implemented at the application layer, but basic statistics can be implemented at the database level (by using triggers on the tables)
- Performance and Scalability - are not covered by this option and must be implemented on top of it. Performance and Load tests at the particular infrastructure configuration must be performed in order to know the limitations of the PostgreSQL instance.

### Positive Consequences

- Logical Separation - PostgreSQL RSP allows to separate of the tenant data at the database level end and enforces the ACL using the policies
- The Complexity of the Implementation - this option can be implemented on top of the current codebase without significant refactoring of the codebase and additional work for infrastructure engineers.


### Negative Consequences

- Physical Separation - is not covered by this option

## Pros and Cons of the Options

### Row Security Policies (RSP)

In addition to the SQL-standard privilege system available through GRANT, tables can have row security policies that restrict, on a per-user basis, which rows can be returned by normal queries or inserted, updated, or deleted by data modification commands. This feature is also known as Row-Level Security. By default, tables do not have any policies, so if a user has access privileges to a table according to the SQL privilege system, all rows within it are equally available for querying or updating.

Having the Wallet abstraction and the `tenantId` that is resolved from JWT issued by PEP, it's possible to logically isolate the rows of the table in the database.

- Good, because logical isolation is performed at the application and database layer
- Good, because it's easy to implement based on the current architecture
- Good, because no additional overhead for DevOps engineers
- Good, because the migration is going to be executed within the same time as for the single tenant solution
- Good, because the cost of the solution is going to be the same as for single-tenant
- Bad, because `noisy neighbors` issue might occur when some tenant is actively using the Wallet and occupies the resources
- Bad, because it might not fit the clients that require a higher level of data isolation.

### Table per Tenant and Schema per Tenant

In this option, the data are logically isolated either in the table or the schema that belongs to the tenant.

For each Wallet abstraction, the tenant must have the table or the schema with the `suffix` as a `tenantId` and routing to the concrete tenant is resolved at the application layer.

- Good, because logical isolation is performed at the application layer
- Good, because it's possible to implement it based on the current architecture (but more work is required compared to option 1)
- Good, because no additional overhead for DevOps engineers
- Good, because the cost of the solution is going to be the same as for single-tenant
- Bad, because all isolation must be figured out at the application layer, so all components must use the same approach to the data isolation
- Bad, because the migration time and maintenance complexity is going to grow with the number of tenants
- Bad, because `noisy neighbors` issue might occur when some tenant is actively using the Wallet and occupies the resources


### Database per Tenant and Instance per Tenant

In this option, the data are physically isolated by using the database or the server instance per tenant.
The logic for data isolation must be implemented at the application layer, so per each tenant, the table with the mapping of the tenant to the database or the instance must be configured.

- Good, because of the physical data isolation
- Good, because of granular control over the resources per each tenant, so `noisy neighbors` issue will not happen in this option
- Bad, because the cost of running the additional database or server instance with grow with the number of tenants
- Bad, because the migration time and maintenance complexity is going to grow with the number of tenants
- Bad, because more work is required for engineers to implement this solution

### Citus extension and AWS - Sharded Instances

Current options must be applied for SaaS solutions with a high number of tenants which requires sharding of the databases.

Both options serve the same goal - horizontal scaling of the instances of PostgreSQL

The main advantages of Citus:
- fits for on-premise deployments
- provides additional monitoring and statistics to manage the tenants
- routing to the shard is managed by Citus using the `hash` of the table index (compared to AWS sharding option, the routing is done at the application layer and the system table contains the information about the mapping of the tenant to the instance of the database)

One of the previously described options `should` be implemented behind these options.

- Good, because can manage millions of tenants
- Good, because can manage the isolation (logical or physical bases on the configured mapping of the tenant to the database)
- Good, `noisy neighbors` issue can be monitored and solved (Citus)
- Bad, because additional work is required from engineers
- Bad, because the cost of the solution is higher compared to the single-tenant

## Links

- [Strategies for Using PostgreSQL as a Database for Multi-Tenant Services](https://dev.to/lbelkind/strategies-for-using-postgresql-as-a-database-for-multi-tenant-services-4abd)
- [Multi‑Tenant Data Architecture](https://renatoargh.files.wordpress.com/2018/01/article-multi-tenant-data-architecture-2006.pdf)
- [PostgreSQL Row Security Policies](https://www.postgresql.org/docs/current/ddl-rowsecurity.html)
- [Sharding with Amazon Relational Database Service](https://aws.amazon.com/blogs/database/sharding-with-amazon-relational-database-service/)
- [Citusdata](https://www.citusdata.com/)
- [Citusdata Multi-Tenant Applications](https://www.citusdata.com/use-cases/multi-tenant-apps)
</file>

<file path="documentation/adrs/decisions/2023-05-27-use-keycloak-and-jwt-tokens-for-authentication-and-authorisation-to-facilitate-multitenancy-in-cloud-agent.md">
# Use Keycloak and JWT tokens for Authentication and Authorisation to facilitate multitenancy in the Cloud Agent

- Status: accepted
- Deciders: David Poltorak, Yurii Shynbuiev, Shailesh Patil, Ben Voiturier
- Date: 2023-05-27
- Tags: multitenancy, authorisation, authentication

Technical Story: [Research Spike - 1d: find a way to authenticate and authorise the Cloud Agent instance administrator | https://input-output.atlassian.net/browse/ATL-4362]

## Context and Problem Statement

Prior to this Architectural Decision Record (ADR) and the related Value Brief, authentication (AuthN) and authorisation (AuthZ) for API consumers of an agent are implemented using a pre-shared key, supplied as an API token within each request header.

An agent can support a single-tenant only.

Each single-tenant agent is accessed via a shared API gateway layer (APISIX) that enforces a consumer restriction list. Only assigned consumers, identified through the pre-shared key, can access specific agent instances.

This authentication/authorisation mechanism poses a significant security risk. If the pre-shared key is leaked, we lack the means to detect its misuse by a nefarious actor, as there is no proof-of-possession mechanism or additional authentication factor in place.

In our Multi-tenant Value Brief, we propose modifications to the agent, enabling it to host multiple tenants within a single instance. Here, a tenant is defined as a unique set of private keys and configurations shared by multiple API consumers.

As we transition to multi-tenancy, several critical questions emerge:

1. How should the Cloud Agent authenticate, or verify the identities of, its API consumers?
2. How should the Cloud Agent authorise a particular identity to use a specific instance of the agent?
3. As the Cloud Agent becomes capable of hosting multiple tenants whose workloads must remain isolated, how should it become tenant-aware? That is, how should it determine which tenant an API consumer belongs to, and authorise them to manage and operate within that tenant?
4. How can we mitigate the security risk associated with a leaked pre-shared key/token?"

## Decision Drivers

- The complexity of the solution to implement, run and maintain
- Ability to offer solution as SaaS offering as well as self-hosted option
- Use industry standard approaches for frictionless adoption
- Not having to roll our own AuthN/AuthZ implementations [Engineering principle: build differentiating value]
- Ability to effectively mitigate pre-shared key security risk

## Considered Options

All options use OIDC and the Client Credentials Grant flow which is suitable for machine-to-machine use.

We have not included an option where we write our own AuthN/AuthZ implementation. All options require an additional component to be added to the stack to store identity related data [Users, roles etc] and to potentially act as a Policy Decision Point (PDP), Policy Administration Point (PAP) and a Policyf Information Point (PIP).

### Keycloak as AuthN/AuthZ

- Keycloak with opaque tokens (without digital signatures)
- Keycloak with JWT tokens (without digital signatures)

### Keycloak as AuthN, another system as AuthZ

- Keycloak with JWT tokens and Open Policy Agent (OPA) (without digital signatures)

### Digital Signatures/Proof of Possession

- Keycloak with any token type with Demonstration of Proof of Possession (DPoP)
- Keycloak with any token type with a custom scheme using Decentralized Identifiers (DIDs)
- Keycloak with any token type using Mutual TLS (mTLS)

## Decision Outcome

Chosen option: "Keycloak with JWT tokens (without digital signatures)", because it provides a balance between security, complexity, and maintainability while using industry-standard approaches, and reduces the need to develop custom AuthN/AuthZ implementations. Application layer can decode JWT and use scope and claims to identify tenant of the consumer.

### Positive Consequences

- Industry standard OAuth2/OIDC is used for authentication, ensuring compatibility with many services.
- Utilizes Keycloak as an Identity Provider (IdP), providing a centralized and robust service for managing identities.
- JWT tokens allow claims and scopes to be directly embedded in the token, which helps in authorization.
- APISIX, acting as the Policy Enforcement Point (PEP), can validate JWT tokens without a round trip to Keycloak.
- Risk of key/token leakage is reduced as compared to pre-shared keys.

### Negative Consequences

- Complexity of JWT token management, including key rotation and revocation.
- Need for a caching and refresh strategy when verifying JWT in the APISIX and application layer.
- Possible increased latency due to JWT verification at both APISIX and application layers.
- Reupidation threat minimised by short OIDC access token lifetime but not fully mitigated as no digital signature implemented.

## Pros and Cons of the Options

### The use of Keycloak in general

- Good, becasue APISIX and Keycloak are easy to integrate with well documented plugins.
- Bad, because of the need to run Keycloak [compute resources and management overhead].

### Keycloak with opaque tokens (without digital signatures)

*Keycloak is utilized for authentication, whereas authorisation requires APISIX and the application layer to make a call to Keycloak. This is because the opaque token, which cannot be decoded outside of Keycloak, doesn't contain any permission-related information, necessitating the authorisation check.*

- Good, because it simplifies token management.
- Good, because tokens are not self-contained and therefore don't expose any information.
- Bad, because it requires a round trip to Keycloak to validate each token and perform authorisation checks, increasing latency.

### Keycloak with JWT tokens (without digital signatures)

*Keycloak is utilized for authentication, while authorisation is handled by APISIX and the application layer. Both the APISIX and application layer need to call Keycloak's JSON Web Key Set (JWKS) endpoint to retrieve public keys to decode and validate JWTs. However, the actual authorisation process is handled internally, leveraging data added to JWTs as part of scope and claims. This approach reduces latency compared to the authorisation checks required for opaque tokens.*

- Good, because JWT tokens can be validated by APISIX without a round trip to Keycloak.
- Good, because claims and scopes can be embedded directly in the token.
- Bad, because it introduces complexity around JWT management, including key rotation and revocation.

### Keycloak with JWT tokens and Open Policy Agent (OPA) (without digital signatures)

*Keycloak is utilized for authentication, while APISIX and the application layer make a call to an OPA service for authorisation. Additionally, they need to contact Keycloak's JWKS endpoint to retrieve public keys, enabling them to decode and validate JWTs. Authorisation policies are articulated using the powerful OPA language.*

- Good, because it provides a powerful and flexible approach to authorisation.
- Good, because it works well with JWT tokens, enabling authorization checks to be performed based on JWT claims.
- Bad, because it introduces additional complexity and another component to maintain (in addition to Keycloak).

### Keycloak with any token type with DPoP

*Only works in oAuth2/OIDC flow

- Good, because DPoP provides a method for binding access tokens to a particular client.
- Good, because it enhances the security by reducing the threat of token theft.
- Bad, because it introduces additional complexity around token management.

### Keycloak with any token type with a custom scheme using DIDs

- Good, because DIDs provide a self-sovereign method of identity verification.
- Good, because it enhances security by ensuring that only the valid owner of a DID can authenticate.
- Bad, because it adds a considerable amount of complexity to token management, and DIDs are still relatively new and may not be widely adopted or fully standardized.

### Keycloak with any token type using Mutual TLS (mTLS)

- Good, because it provides a strong method of security by requiring both client and server to authenticate each other.
- Good, because it mitigates repudiation threats.
- Bad, because it introduces complexity around certificate management and may add additional overhead in terms of performance.

## Links

- [Keycloak documentation](https://www.keycloak.org/docs/latest/)
- [APISIX documentation](https://apisix.apache.org/docs/)
- [Open Policy Agent (OPA) documentation](https://www.openpolicyagent.org/docs/)
- [JWT (JSON Web Tokens) Introduction](https://jwt.io/introduction/)
- [OAuth 2.0 documentation](https://oauth.net/2/)
- [Information on OAuth 2.0 Token Binding - DPoP](https://tools.ietf.org/id/draft-ietf-oauth-dpop-03.html)
- [Decentralized Identifiers (DIDs) documentation](https://www.w3.org/TR/did-core/)
- [JWT vs Opaque Tokens](https://zitadel.com/blog/jwt-vs-opaque-tokens)
</file>

<file path="documentation/adrs/decisions/2023-06-28-apollo-as-centralised-and-secure-cryptography-management-module.md">
# Apollo as centralised and secure cryptography management module

- Status: accepted
- Deciders: Yurii Shynbuiev, Javier Ribó, Gonçalo Frade, Bart, Ahmed Mousa
- Date: 2023-06-28
- Tags: apollo, cryptography

Technical Story: [Apollo Cryptographic Module KMM | https://input-output.atlassian.net/browse/ATL-5006]

## Context and Problem Statement

<br/>

### 1. Summary
This proposal sets out to crystallize a long-term plan for Identus' cryptographic functionality. Rather than constructing an entirely new cryptographic functionality, our focus is on integrating robust, secure and tested libraries, meeting several key requirements in the process.

By leveraging the flexibility of Kotlin Multiplatform, this library will ensure strong, provable security, centralized management of all cryptography, easy upgrades, and efficient code reuse across multiple platforms.

A significant additional advantage of our chosen framework, particularly for the JavaScript version of this library, is the future potential to export to WebAssembly (WASM).

<br/>

### 2. Introduction
This proposal outlines a comprehensive plan to develop a cryptographic library using Kotlin Multiplatform. This library will meet our defined requirements and strategically position us for future technological advancements.

#### 2.1 Provable Security
Our cryptographic library will provide engineers with high assurances of security. This will be accomplished by using cryptographic primitives that are secure, with this security being provable through rigorous mathematical proofs. Documentation will accompany these proofs to offer transparency and enable a deeper understanding of the underlying logic and assurances.

#### 2.2 Centralized Cryptography Management
We propose the creation of a cryptographic library that serves as the central management hub for all cryptographic operations within the Identus platform. By preventing "DIY" implementations, we decrease potential vulnerabilities and establish a standard, thus enhancing overall security across our organization.

#### 2.3 Easy Upgrade Path
In light of emerging cryptographic needs such as the introduction of quantum-resistant cryptography, our library will be designed with easy upgrades in mind. Its modular design will allow for the seamless introduction of new cryptographic primitives as they become necessary or advisable. This adaptability will ensure that cryptographic upgrades across all of Identus' components are consistent and efficient.

#### 2.4 Code Reusability
Our library will make the most of Kotlin Multiplatform's capabilities for code reuse across different platforms. We aim to design cryptographic functions that promote this potential, thus minimizing the development effort required for adding new functionality or adapting to different platforms.

<br/>

### 3. Advantages of Kotlin Multiplatform
Choosing Kotlin Multiplatform for this project affords us several advantages, notably its potential to export to WASM. Not only this stack, but it significantly enhances the utility and versatility of our library, especially for our JavaScript version. While other languages like Rust offer similar capabilities, the use of Kotlin Multiplatform aligns more closely with our resource allocation and current technological strategy.

<br/>

### 4. Trade Offs
The trade-off gap analysis does not come from the debate between having 1 single language (agnostic) or multiple native platform implementations as this discussion could end super quickly by just reading the  4 points in the Introduction section (Easy upgrade path, code reusability).

The real debate is between choosing the right language that suits us best. We have been analyzing the potential use of Rust or KMM to build the Apollo module.

#### 4.1 Advantages of KMM
Easier to have same interface for the cryptographic functionalities in all platforms
Single Unit test suit to verify platform compatibility between platforms
Version lock for supported library versions
Less pron to compatibility errors between platforms
When a new Platform is added to KMM we can easily test and verify with our code quality standards, if it passes we can add a new platform support

#### 4.2 Disadvantages of KMM
KMM is very powerful but still has some issues the more complex the project is, in Apollo it should be quite straightforward
High dependency on a single point of failure (This can be advantage and disadvantage)

#### 4.3 Advantage of KMM against rust
KMM in this situation has great advantageous against Rust
It might be more difficult to find libraries in Rust that can do all we require for all the platforms.
Testing would not be so straightforward and By platform. It would instead run only on the platform that is building the code.
Uniffi doesnt work for all platforms we provide, so some wrappers would have to be written by hand with C bindings (Not safe and very error pron)

<br/>

### 5. Implementation Details
We have established several key requirements for our team:

1. Ownership of Apollo and its roadmap is clearly defined. If any issues, doubts, or concerns arise or if any decisions need to be made about the roadmap, tech debt, or any other related aspects, stakeholders know who to contact.

2. We aim to rebuild the current implementations into a more robust system rather than re-using old code from 1.4.

3. Anyone inquiring about the features or algorithms we support will have a clear and accessible resource to answer their questions. We plan to provide a complete list of features that Apollo offers, document them, and make this information accessible to everyone.

4. Apollo will fully document its public APIs in terms of cryptography, compliance, supported algorithms, and key curves.

5. While TypeScript could temporarily survive without WASM, we would significantly benefit from offering native performance for cryptographic libraries through WASM for browsers.

6. Apollo will be tested using best practices, and over time we aim to raise the minimum test coverage threshold. Best practices and quality assurance will be addressed in a collaborative way between Cryptography and engineering, Cryptography engineers will be in charge of making sure our security is robust enough and properly tested and potentially helping us drive unit testing better.

<br/>

### 6. Definition of Done
In order to consider this completed or done the existing SDK's must have integrated with this new Module.

<br/>

### 7. Implementation plan

1. Initial Phase: We will establish clear ownership of Apollo and announce it to the rest of the team. From this point forward, the SDK team will take charge of these tasks. This step will facilitate decision-making and clarify who to contact regarding various issues and understanding the roadmap.

2. Phase 1: We will implement wrappers around basic and required functionality for secp256k1, ed25519, and x25519 (generating keyPairs, signing and verifying). We aim to standardize the interface and ensure that it exposes all the functionality that the SDKs need to work.

3. Implementation phase: Phase1 will be by then fully documented, tested and available for other engineers to pick up on their platforms. Engineers will then replace that functionality that was done by third party packages in swift, kmm and typescript with the new Apollo package.

<br/>

#### Implementation resources
| Engineer                              | Role                                        | Availability |
|---------------------------------------|---------------------------------------------|--------------|
| Francisco Javier Ribó                 | Engineering Lead + Developer                | Part time    |
| Yurii                                 | Engineering + CoreDID Integration Lead      | Part time    |
| Gonçalo Frade                         | SDK Project Lead + Roadmap Lead + Developer | Part time    |
| Alexandros Zacharakis Jesus Diaz Vico | Cryptography Engineers + Roadmap Lead       | Part time    |
| Ahmed Moussa                          | KMM Lead / architect + Developer            | Fulltime     |
| Cristian Gonzalez                     | Developer                                   | Fulltime     |
| Curtis HArding                        | Developer                                   | Part time    |

<br/>

### 8. Triage & future roadmap
The main goal of this section is to describe the process where we choose what comes next in Apollo and how we take those decisions.

**Comments**


1. There is a risk of starting to add to Apollo "anything that looks like cryptography". For instance, the Anoncreds part that takes care of formatting the credentials (which is what anoncreds-rs does) should not go into Apollo.
2. But the underlying cryptographic functionality (for which anoncreds-rs calls libursa) should go into Apollo.
3. Maybe something similar applies to HD wallets.

Owners of this triage process are the engineering + roadmap leads. Those will take decisions based on what has been defined here:

1. Provable Security
2. Centralized Cryptography Management
3. Easy Upgrade Path
4. Code Reusability

<br/>

### 9. Conclusion

This proposed Kotlin Multiplatform cryptographic library will ensure that Identus remains at the forefront of secure digital operations by providing strong, provable security, centralized cryptographic management, easy upgradeability, and efficient code reuse. By addressing these critical constraints and harnessing the benefits of Kotlin Multiplatform, we are set to create a library that will set a new standard for cryptographic operations within the Identus platform.
</file>

<file path="documentation/adrs/decisions/2023-07-14-performance-framework-for-atala-prism.md">
# Performance framework for the Identus platform

- Status: accepted
- Deciders: Anton Baliasnikov, Shota Jolbordi, David Poltorak
- Date: 2023-07-14
- Tags: benchmarks, performance, k6, load testing

Technical Story: [Performance Management](https://input-output.atlassian.net/browse/ATL-4119)

## Context and Problem Statement

There are multiple great solutions today on the market for load testing. We need to choose one that corresponds to our needs, infrastructure, and price. The objective is to assess our requirements and explore the range of available load-testing solutions. Subsequently, we will provide a proposal recommending the load-testing framework that best suits our needs.

## Decision Drivers <!-- optional -->

What are our needs? Let’s try to sum up the required capabilities based on [RFC-0028](https://input-output.atlassian.net/wiki/spaces/ATB/pages/3819044870/RFC+0028+-+PI2+-+Performance+Testing+Guidance+Framework), we need:

1. Create required performance scenarios on various levels, such as endpoint level (e.g. get connections) and  flow level (calling endpoints one by one in order, e.g. issuing credentials flow)
2. Do checks for each request to make sure achieved statuses and response data are correct
3. Custom metrics support as we have custom scenarios connected
4. Protocols support: HTTP (rest API), WebSocket (if we support it in the future for Mediator), gRPC (if we need PRISM Node direct benchmarks on gRPC level)
5. Create the required load over time depending on the scenario and type of test, the more different things are supported - the better
6. Easy to understand, read and share reports
7. Something that does not require too much RAM/CPU resources on the host machine to run, so we can use our custom GitHub runners for performance testing and possible Cloud Execution
8. The fast learning curve for everybody to contribute
9. Good documentation and examples to be able to develop complex scenarios faster
10. Cloud support for tests running
11. Cloud support for tests analysis to make results visible and present them easily as per request
12. Support for GitHub actions to integrate into CI/CD
13. Open source to be able to customize if required
14. Cheap or free

## Considered Options

What’s on the market?
Here is the list of TOP10 load test frameworks currently available on the market:

* Apache JMeter
* LoadRunner
* PFLB Platform
* Gatling
* K6
* LoadNinja
* WebLOAD
* BlazeMeter
* NeoLoad
* Locust

We can read multiple papers about their comparison, for example, [this one for reference](https://pflb.us/blog/best-load-testing-tools/), but we need to understand the difference between them all and which pros and cons they have, and why they were created.

There are 3 main classes of frameworks:

1. enterprise - huge enterprise solutions in clouds with advanced tools to generate all kinds of load testing scenarios targeted for non-technical testing engineers
2. lightweight - small, highly optimized solutions, open-sourced, and easily extendable
3. mixed - developer-friendly frameworks that are usually open-sourced with some free version providing cloud-paid plans, a kind of a medium between enterprise and lightweight

All frameworks are using different technologies and approaches: JVM-based, JS-based, GO-based, etc; GUI-oriented solutions to simplify tests development; cloud-only, etc.

Keeping in mind that we are not interested in enterprise solutions, because:

1. They are very expensive
2. We are smarter than they think we are (for example, we don’t need GUI simplifications and blocks to generate scenarios, we would like to use our coding power here)

Based on that, we can conclude that it makes sense for us to analyze and compare the following load-testing frameworks:

1. Gatling
2. K6
3. Locust

## Decision Outcome

Native support for CI, Grafana, lots of output formats, custom metrics, distributed modes, amazing native Cloud integration, but most importantly the extensive documentation with tutorials and videos makes K6 the best choice for us.

## Pros and Cons of the Options <!-- optional -->

### K6

Strengths:

* Modern, JavaScript-based: K6 is built on modern web technologies and uses JavaScript as its scripting language, which makes it accessible to a wide range of developers.
* Cloud-native: K6 is designed to work seamlessly with cloud-based infrastructure, making it easy to scale tests and generate high loads.
* Open-source: K6 is fully open-source, with an active community and extensive documentation.

Weaknesses:

* Limited protocol support: K6 currently only supports HTTP, WebSocket, and gRPC protocols, which may not be ideal for testing more complex systems.
* Limited customization: While K6 is highly customizable, it does not allow for as much customization as Gatling or Locust.
* JavaScript knowledge required: Developers need to be familiar with JavaScript or TypeScript to use K6 effectively.

Pros:

* JavaScript-based, easy to start using for everyone
* Native integration with Grafana that we’re using in our infrastructure
* A nice Cloud solution that can be used inside our infrastructure
* Custom metrics support
* Tons of output formats
* Native CI/CD support for GitHub actions
* Extensive, easy-to-read docs with tutorials and youtube videos

Cons:

* Not as extendable as Gatling or Locust
* Quite an expensive Cloud solution
* New for us, some learning curve is expected


### Gatling

Strengths:

* High performance and scalability: Gatling can simulate thousands of virtual users with high performance and low resource usage.
* User-friendly DSL: Gatling uses a domain-specific language (DSL) that is easy to read and write, making it accessible to developers and testers with varying levels of experience.
* Comprehensive reporting: Gatling provides detailed and customizable HTML reports that make it easy to analyze test results.

Weaknesses:

* Java-based: Gatling requires Java to run, which may not be ideal for some developers or organizations.
* Steep learning curve: Although the DSL is user-friendly, a learning curve is still associated with mastering Gatling's features and functionality.
* Limited community support: Gatling has a smaller community than some other load-testing tools, which may make it harder to find answers to specific questions or issues.

Pros:

* We already have the performance setup on Gatling and used it as part of 1.4
* Nice concepts and solid base DSL
* Performance scenarios are written in Scala or Kotlin which gives us 2 advantages: re-use data models from our Scala libraries in our benchmarks if required, and we don’t need to learn any new language to use the framework (most of the team is Scala, Kotlin programmers)
* Extendable with plugins
* Custom Cloud solutions available

Cons:

* [subjective opinion writing the code] DSL is not that good, lots of boilerplates to achieve simple things, not intuitive
* Custom metrics are available only in Enterprise
* No native integration with Grafana that we use in our infrastructure
* Documentation is not that good, many examples are implicit, no good video tutorials
* No native CI/CD integration
* Supports a lot less of output formats than K6
* Distributed load generation is very complex, not natively integrated


### Locust

Strengths:

* Python-based: Locust is built on Python, which is a popular and widely used programming language.
* Simple syntax: Locust uses a clean and intuitive syntax that is easy to read and write, even for beginners.
* Highly customizable: Locust allows you to customize your test scenarios using Python code, which gives you more flexibility and control over your tests.

Weaknesses:

* Limited protocol support: Locust currently only supports HTTP, WebSockets, and MQTT protocols, which may not be ideal for testing more complex systems.
* Limited reporting: Locust provides basic reporting out of the box, but more advanced reporting features require third-party plugins.
* Python knowledge required: While Locust's syntax is simple, developers still need to be familiar with Python to use it effectively.

Pros:

* Python-based, very intuitive, and easy-to-write scenarios
* Already used in IO for benchmarking in other projects
* Very lightweight and powerful with capabilities for distributed load testing out of the box
* Nice docs and examples
* Highly and easily extendable

Cons:

* No Cloud solution is available
* New for us, some learning curve is expected, but easier
</file>

<file path="documentation/adrs/decisions/2023-09-26-use-keycloak-authorisation-service-for-managing-wallet-permissions.md">
# Use keycloak authorisation service for managing wallet permissions

- Status: accepted
- Decider: Pat Losoponkul, Yurii Shynbuiev, David Poltorak, Milos Dzepina
- Date 2023-09-26
- Tags: multitenancy, authorisation, authentication

Technical Story: [External IAM provider integration for Authentication and Authorisation Layers MVP | https://input-output.atlassian.net/browse/ATL-5149]

## Context and Problem Statement

As we move forward with multi-tenancy, it's essential to give extra attention to authentication and authorisation.
Currently, our authentication and authorisation processes are managed by a simple built-in IAM implementation within the Cloud Agent.
While this setup is straightforward and functional, it's a somewhat basic and proprietary approach.
Transitioning to an industry-standard IAM system like Keycloak represents an important step towards a more robust authentication and authorisation framework.

Within our multi-tenant Cloud Agent, we have some key concepts:
wallets (representing resources), entities (representing users), and authentication methods.
These models are integrated into our current IAM implementation within the agent allowing a loose coupling of users and resources, as well as resource access.

As we consider the shift towards an external IAM system, several important questions arise:

1. Where to draw the boundary of AuthN/AuthZ across different components?
2. How to ensure smooth communication of wallet permissions across these components?
3. What will be the impact on the process of onboarding new users and wallets?

These questions are essential as we explore the integration options of an external IAM  that suits our needs.

## Decision Drivers

- Complexity to implement, operate and maintain
- Must allow self-hosted option as well as future SaaS offering
- Must allow flexible management of resources, users and permissions
- Should adhere to standards in AuthN/AuthZ space
- Should promote clear boundary between application and IAM concerns

## Considered Options

1. Keycloak for authentication and associate the wallet permissions on the Agent.

   In this option, the agent validates the JWT from Keycloak, extracting the payload and obtain the user identity.
   Then, the agent stores the user's associated wallet information in its database.
   Keycloak's responsibility lies solely in user authentication, as permissions management is internally managed by the agent.

2. Keycloak for authentication and embed custom permission claims in the `access-token`.

   In this option, the agent validates the JWT from Keycloak, extracting the payload containing custom claims related to the user's accessible wallets.
   Wallet permissions are managed in Keycloak as user metadata, which needs to be configured to include this claim in the issued JWT.

3. Keycloak for authentication and authorisation service  managing permissions.

   In this option, the agent handles wallet resources on Keycloak using the [Protection API](https://www.keycloak.org/docs/latest/authorization_services/index.html#_service_protection_api).
   Wallet permissions are managed through Keycloak's [authorisation service](https://www.keycloak.org/docs/latest/authorization_services/index.html).
   When users intend to use the agent, it verifies wallet permissions by checking the Keycloak permission endpoint.
   Similarly, Keycloak can also issue a self-contained JWT directly to users contains all the permission claims required by the agent.

## Decision Outcome

Use Keycloak authorisation service for managing wallet permissions.
Keycloak authorisation service offers a robust abstraction based on an industry-standard specification called UMA (User-Managed Access).
It allows us to define resources, resource owners, permissions, and policies with ease.
These concepts are tried and tested, and Keycloak provides them right out of the box, making it a suitable choice.
It is also the only option to adhere to the standard.

In this setup, applications are responsible for managing their resources and rely on
Keycloak for managing permissions on those resources.
To determine whether a user can access a specific resource, the application can simply utilise Keycloak's permission endpoint.

### On-boarding sequence diagram

```mermaid
sequenceDiagram
    actor Admin
    actor User
    participant Client
    participant CloudAgent
    participant Keycloak

    autonumber

    Admin ->> CloudAgent: Create a new wallet
    CloudAgent ->> Keycloak: Register a new resource
    Admin ->> Keycloak: Create a new user
    Admin ->> Keycloak: Create a new user-credential
    Admin ->> Keycloak: Create a new permission
    Admin ->> Keycloak: Associate permission(s) with a resource
```

### Authorisation sequence diagram

```mermaid
sequenceDiagram
    actor Admin
    actor User
    participant Client
    participant CloudAgent
    participant Keycloak

    autonumber

    User ->> Client: First visit
    Note over Client: User is not logged in
    Client ->> Keycloak: Login with preconfigured flow
    Keycloak ->> Client: JWT AccessToken
    User ->> Client: Check my VC
    Client ->> CloudAgent: Get CredentialRecord

    opt Bearer token is not RPT
      CloudAgent ->> Keycloak: Get permissions
      Keycloak ->> CloudAgent: Permitted resource(s)
    end

    alt is permitted
        CloudAgent ->> Client: CredentialRecord
    else is not permitted
        CloudAgent ->> Client: 403 Forbidden
    end
```

Optionally, users or downstream applications can directly call Keycloak permission endpoint to get a RPT (requesting-party token)
to obtain a self-contained `access-token` which already include permissions.

__Endpoint references__

- Agent checks the user permissions using [permission endpoint](https://www.keycloak.org/docs/22.0.0/authorization_services/#_service_obtaining_permissions)
  - [optional] Client may also directly call this endpoint on keycloak
- Agent registers a new resource using [resource endpoints](https://www.keycloak.org/docs/22.0.0/authorization_services/#_service_protection_resources_api)
  - [obtain a token for the resource endpoints](https://www.keycloak.org/docs/22.0.0/authorization_services/#_service_protection_whatis_obtain_pat)
- Admin manages the wallet permissions using both
  - [Permission API](https://www.keycloak.org/docs/22.0.0/authorization_services/#_service_protection_permission_api_papi)
  - [Policy API](https://www.keycloak.org/docs/22.0.0/authorization_services/#_service_authorization_uma_policy_api)

### Positive Consequences

- Good separation between IAM and application concerns
- Powerful and proven abstraction for managing permissions
- Implementation is ready out of the box
- Easy to migrate to different IAM vendor that supports UMA specification

### Negative Consequences

- Require additional network call to obtain a permission token
- Involves more moving parts in managing wallet permissions
- Each IAM systems may have a slight variations to the UMA endpoints

## Pros and Cons of the Options

### Keycloak for authentication and associate the wallet permissions on the Agent

- Good, because permission logic is on application and allowing any IAM solution to be used regardless of authorisation feature
- Bad, because permissions are managed on the application and IAM boundary is blurred
- Bad, because engineering effort is spent on non-differentiating value in order to have feature parity with other authorisation system

### Keycloak for authentication and embed custom permission claims in the `access-token`

- Good, because the `access-token` is self-contained and doesn't require extra network call
- Good, because IAM systems are more likely to support custom claims feature
- Bad, because the custom claims are directly linked to user instead of resource preventing flexible mangement of permission

## Links

- [Keycloak authorisation service](https://www.keycloak.org/docs/latest/authorization_services/index.html)
</file>

<file path="documentation/adrs/decisions/2023-09-28-revocation-status-list-expansion-strategy.md">
# JWT credential revocation status list expansion strategy

- Status: accepted
- Decider: Benjamin Voiturier, Yurii Shynbuiev, Ezequiel Postan, Shota Jolbordi
- Date 2023-09-28
- Tags: revocation, revocation-status-list, jwt-vs-revocation, statusList2021

Technical Story: [Revocation status list expansion strategy decision | https://input-output.atlassian.net/browse/ATL-6012]

## Context and Problem Statement

In the process of devising a mechanism for revoking JWT credentials, we've opted to implement the [statusList2021](https://www.w3.org/TR/vc-status-list/) method.
This approach entails the creation of a "status list credential" that incorporates a gZip compressed status list in a form a bitString.
The specification recommends a minimum size of 16 KB for the status list included in a credential.
However, it does not delineate a maximum size, nor does it provide guidance on how to proceed if the selected status list surpasses its capacity to store information about revoked credentials.
Put differently, if more credentials are issued than can be accommodated by a 16 KB status list, no specific instructions are provided.


## Decision Drivers

We must determine a strategy for expanding the revocation status list to accommodate the increasing number of revoked credentials in the future.

It's crucial to keep in mind that this status list will be part of a "status list credential."
This credential will be frequently requested through the REST API during verification by verifiers and will be downloaded over the network.
Therefore, we need to ensure that the status list remains reasonably small in size to prevent any slowdowns in the verification process.

In the future, there might be a need to reorganize the state and possibly move status lists to another public registry for verifiers to depend on. This is not the current scenario, as each Cloud Agent currently maintains status lists specific to their respective tenants.

Absolutely, it's crucial to avoid overengineering the solution. This ensures that the code remains manageable and easy to maintain in the long run.


## Considered Options


Option 1: Increment status list size as we approach its limit:

We'll enhance the status list by simply doubling its size.
If we opt for the minimum size of 16 KB for the revocation status list, it can manage up to 131,072 revocable credentials before requiring expansion.
It's important to note that this number represents the potential capacity for revocable credentials to be issued, not necessarily the actual number of credentials that have been revoked.
Even unrevoked credentials still occupy space.

Option 2: Generate multiple revocation status lists as the previous one reaches its limit

With this approach, we'll generate and store multiple status list credentials.
It will be crucial to ensure that each credential is linked to a specific status list, allowing us to track where the revocation information is stored.
If we stick with the smallest recommended status list size, one revocation status list can hold information about 131,072 revocable credentials.


## Pros and Cons of the Options


#### Option 1
Option 1 offers the primary advantage of being straightforward to implement.
It is also important to note that Option 2 isn't significantly more challenging to implement, so we shouldn't overly prioritize this consideration.

One potential drawback of Option 1 is that the size of the status list could potentially become too large, leading to slower verification due to the increased payload of the verification status list credential.
However, it's worth noting that the verification status list credential is gZipped.
This means that consecutive 0s and 1s will be compressed.
For example, a sequence of 5 zeros (00000) will be stored as 5(0), indicating five consecutive zeros.
Assuming that most credentials won't be revoked and will have an index of 0 in the status list, the gzipped status list in the status list credential should be very compact.
This is the most crucial factor to consider in the end.

#### Option 2

Option 2 slightly reduces privacy compared to Option 1 in certain scenarios.
For example, in cases where the number of Verifiable Credentials (VCs) starts small but grows over time.
Initially, both options face the same issue with a small anonymity set due to the limited number of VCs issued.
As the number of VCs increases, Option 1 maintains a continuously growing anonymity set.
However, in Option 2, when the issuer reaches the 16KB limit and creates a new list, there will be a period where the new list has only a few VCs, resulting in a smaller anonymity set for VCs in the second list.


Option 2 however, has a big advantage considering upcoming need for AnonCreds revocation.
AnonCreds doesn't allow for expanding the status list size once defined during revocation registry creation.
Pushing back Option 2 for AnonCreds and starting with an initial capacity of 1 million credentials may not be efficient.
The size of the attached TAILS FILE grows rapidly with capacity (e.g., 8.4MB for 32,768 VCs!).
This file needs to be resolved/downloaded by the holder during the issuance process.


## Decision Outcome

Given that the implementation of Option 2 is not significantly more complicated than Option 1, and considering that JWT credentials, specifically statusList2021, are not inherently private, we have decided to proceed with Option 2.
This choice is more future-proof, especially in light of the anticipated need to implement AnonCreds revocation in the future.
</file>

<file path="documentation/adrs/decisions/2024-01-03-use-jwt-claims-for-agent-admin-auth.md">
# Use JWT claims for agent admin access control

- Status: accepted
- Deciders: Pat Losoponkul, Yurii Shynbuiev, David Poltorak, Shailesh Patil
- Date: 2024-01-03
- Tags: multitenancy, authorisation, authentication

Technical Story: [Allow agent admin role to be authenticated by Keycloak JWT | https://input-output.atlassian.net/browse/ATL-6074]

## Context and Problem Statement

Administrators currently rely on a static API key configured in the agent.
Employing a static API key for administrative operations poses a security challenge for the entire system.
A standardized centralized permission management system for administrative operations should be integrated,
to ensure the solution is security-compliant, yet remains extensible and decoupled.

The existing tenant authorization model relies on UMA (user-managed access) to protect the wallet.
In addition to the wallet usage, the agent also handles wallet management,
a functionality utilized by both tenants and administrators.
While administrators don't directly use the wallet, they oversee its management.
Integrating an auth model to distinguish between admins and tenants presents a new challenge.

- Where and how to define the role of admin and tenant?
- What should be the authorization model for the admin role?
- What boundary the admin role should be scoped to?
- How to support different deployment topologies?
- How does it interact with the wallet UMA model?

## Decision Drivers

- Must not prevent us from using other IAM systems
- Must not prevent us from supporting fine-grained tenant wallet access in the future
- Should not mix admin access with tenant access
- Should be easy to setup, configure and maintain

## Considered Options

1. Use `ClientRole` for defining roles in Keycloak

    In this option, the `ClientRole` is configured at the client level,
    and the user is mapped to the `ClientRole` using a role mapper.
    The role claim will be available in the JWT token at `resource_access.<client_id>.roles`.

2. Use `RealmRole` for defining roles in Keycloak

    In this option, the `RealmRole` is configured at the realm level,
    and the user is mapped to the `RealmRole` using a role mapper.
    The role claim will be available in the JWT token at `realm_acces.roles`.

3. Use custom user attribute for defining roles in Keycloak

    In this option, the role is defined as a user attribute.
    Then the user attribute will be included in a token using a token mapper at any pre-configured path.

## Decision Outcome

Option 1: Use `ClientRole` for defining roles in keycloak.

Example JWT payload containing `ClientRole`. (Some claims are omitted for readability)

```json
{
  "exp": 1704267723,
  "aud": [
    "cloud-agent",
    "account"
  ],
  "realm_access": {
    "roles": [
      "default-roles-atala-demo",
      "offline_access",
      "uma_authorization"
    ]
  },
  "resource_access": {
    "cloud-agent": {
      "roles": [
        "admin"
      ]
    },
    "account": {
      "roles": [
        "manage-account",
        "manage-account-links",
        "view-profile"
      ]
    }
  }
}
```
The claim is available at `resource_access.<client_id>.roles` by default.
The path to the claim should be configurable by the agent to avoid vendor lock
and remain agnostic to the IAM configuration.

After introducing the role claim, there will be two distinct access control concepts.

  1. Wallet access scope, where the UMA resource defines specific scopes,
     providing fine-grained access to wallet operations.
     For instance, Alice can update a DID but not deactivate a DID on wallet#1.

  2. Agent role, which manages agent-level permissions.
     For example, Alice is an admin for agent #1 and can onboard new tenants,
     but this authority doesn't extend to agent #2.

__Proposed agent role authorization model__

Role is a plain text that defines what level of access a user has on a system.
For the agent, it needs to support 2 roles:

1. __Admin__: `admin`. Admin can never access a tenant wallet.
   Agent auth layer must ignore any UMA permission to the wallet.
2. __Tenant__: `tenant` or implicitly inferred if another role is not specified.
   Tenant must have UMA permission defined to access the wallet.

### Positive Consequences

- Naturally align the boundary of the agent-level role per agent instance
- Ready to use abstraction, minimal configuration to use and include the claim
- Token can be reused across clients, enabling SSO use case
- Keep the wallet access and agent-level role separated

### Negative Consequences

- The `ClientRole` is not part of the standard, other IAM systems may provide different abstraction.
- In some cases, `ClientRole` can be redundant to configure. `RealmRole` may be preferred in those scenarios.

## Pros and Cons of the Options

### Option 2: Use `RealmRole` for defining roles in Keycloak

- Good, because minimal effort is required to define the role and include it in the JWT
- Bad, because roles are at the realm level, making it hard to support some topology

*Note: This option is equally applicable as Option 1, depending on the required topology.*
### Option 3: Use custom user attribute for defining roles in Keycloak

- Bad, because role abstraction is already provided by Keycloak. Engineering effort is spent to reinvent the same concept
- Bad, because it requires more effort to configure the attribute value and map it down to the token

## Links

- [Keycloak ClientRole](https://www.keycloak.org/docs/latest/server_admin/#con-client-roles_server_administration_guide)
</file>

<file path="documentation/adrs/decisions/2024-01-15-Error-handling-report-problem-agent.md">
# Error Handling Report Problem for Agent

- Status: accepted
- Deciders: Shailesh Patil, Fabio Pinheiro, Yurii Shynbuiev, Benjamin Voiturier, David Poltorak
- Date: 2024-01-16
- Tags: report, problem, error, handling, agent

Technical Story: [Error Handling Report Problem for Agent]

## Context and Problem Statement

In decentralized systems like those with remote collaborating agents, effectively reporting errors and warnings is a complex task.
It's crucial to not only highlight problems but also provide the necessary context to those who need this information,
which might include different groups: those who should be informed and those who can actually resolve the issues.

It is more challenging when a problem is identified significantly later or in a different location from where it originated,
involving collaboration among various parties for a solution.
For DIDComm, the aspect of interoperability is especially critical in the context of problem reporting.
In DIDComm, an agent developed by one team is required to be adept at interpreting errors reported by an agent from a completely different team,
presenting a unique challenge in this area.

As of the time of writing, the Cloud Agent supports 3 DIDComm flows: `Connection`, `Issuance` and `Verification.`
Each Cloud Agent operates a background thread that facilitates interactions between two agents.
For each flow, the agent tracks a protocol state, and depending on this state, it triggers a DIDComm(V2) message to communicate with the other agent.
The communication between two agents, Agent A and Agent B, occurs asynchronously in the background job.
If an error occurs in this background job over DIDComm in Agent A, it is recorded in Agent A's logs. However, Agent B remains uninformed about any such errors.

## Decision Drivers <!-- optional -->

What are our needs? Let’s try to sum up the required capabilities based on
[Report Problem 2.0](https://identity.foundation/didcomm-messaging/spec/#problem-reports), we need:


The Cloud Agent is designed to perform three distinct roles: `Issuer`, `Holder`, and `Verifier`. Within these roles,
it operates across three protocol flows, namely `Connection`, `Issuance`, and `Verification`.

| Agent(Protocol) Flows       | Protocols                                                                                            |
|-----------------------------|------------------------------------------------------------------------------------------------------|
| Connection                  | [https://github.com/hyperledger/aries-rfcs/tree/main/features/0160-connection-protocol]              |
| Issuance                    | [https://github.com/decentralized-identity/waci-didcomm/tree/main/issue_credential]                  |
| Verification(Present proof) | [https://github.com/decentralized-identity/waci-didcomm/blob/main/present_proof/present-proof-v3.md] |

  Custom Behavior table
  This table defines the expected behavior of the Agent in different scenarios not covered by the specifications.

 | Agent       | Behaviour                      | Action              |
 |-------------|--------------------------------|---------------------|
 | Scenario G1 | Send a problem report          | e.p.msg.unsupported |
 | Scenario G2 | Send a problem report          | e.p.msg.unsupported |
 | Scenario G3 | Send a problem report          | e.p.error           |
 | Scenario G4 | (Sync ?) Send a problem report | e.p.trust.crypto    |
 | Scenario G5 | (Sync ?) Send a problem report | e.p.did             |
 | Scenario G6 | (Sync ?) Send a problem report | e.p.did.malformed   |
 | Scenario G7 | Send a problem report          | e.p.msg.\<PIURI\>     |

### Scenarios Description

#### General Scenarios

- **G1** - Receive a message for an unsupported protocol
- G1 - Send a problem report `e.p.msg.unsupported`

- **G2** - Receive a message for an unsupported version of the protocol.
- G2 - Send a problem report `e.p.msg.unsupported` and say what version(s) its supported

- **G3** - When an internal error or any unexpected error happens.
- G3 - Send a problem report "e.p.error"

- **G4** - If the message is tampered or got any crypto errors when decoding.
- G4 -  (sync!) Send a problem report "e.p.trust.crypto"

- **G5** - If the DID method is not supported (`did.peer` in this case)
- G5 - (sync?) e.p.did

- **G6** - If the DID method is malformed.
- G6 - (sync?) e.p.did.malformed

- **G7** - When a parsing error from the decrypted message.
- G7 - Send a problem report `e.p.msg.<PIURI>` if the plaintext message is malformed or if parsing into a specific protocol's data model fails.

## Connection Flow Scenarios

[https://github.com/hyperledger/aries-rfcs/tree/main/features/0160-connection-protocol]

| Agent            | Behaviour                                           | Action                       |
|------------------|-----------------------------------------------------|------------------------------|
| Scenario C1      | Send a problem report (Invitation expired)          | e.p.msg.invitation-expired   |
| Scenario C2      | Send a problem report (Invitation parsing decoding) | e.p.msg.malformed-invitation |
| Scenario C3      | Send a problem report (DB connection issues)        | e.p.me.res.storage           |
| Scenario C4      | Send a problem report (After max retries)           | e.p.req.max-retries-exceeded |
| Scenario C5 (G3) | Send a problem report Any other error               | e.p.error                    |

- **C1** - OOB Invitation has expired
- **C2** - OOB is tampered / decoding error
- **C3** - Database connection or related issue
- **C4** - Max retries (Cascading Problems): Connection state cannot be moved after max retries
- **C5** - See G3


## Issuance Flow Scenarios

[https://github.com/decentralized-identity/waci-didcomm/tree/main/issue_credential]

| Agent           | Behaviour                                    | Action                             |
|-----------------|----------------------------------------------|------------------------------------|
| Scenario I1     | Send a problem report                        | e.p.msg.credential-format-mismatch |
| Scenario I2     | Send a problem report                        | e.p.msg.invalid-signature          |
| Scenario I3     | Send a problem report                        | e.p.msg.schema-mismatch            |
| Scenario I4     | Send a problem report (DB connection issues) | e.p.me.res.storage                 |
| Scenario I5     | Send a problem report (After max retries)    | e.p.req.max-retries-exceeded       |
| Scenario I6(G3) | Send a problem report Any other error        | e.p.error                          |

- **I1** - Credential format mismatch: The Holder expects a credential format schema, but the credential issued is different format
- **I2** - Credential signature that cannot be verified. This might be due to the credential being tampered with, or the public key used for signing being incorrect or expired.
- **I3** - Schema Mismatch: The Holder expects a credential that adheres to a certain schema, but the credential presented follows a different schema.
- **I4** - Database connection or related issue
- **I5** - Max retries (Cascading Problems): Issuance state cannot be moved after max retries
- **I6** - See G3


## Verification(Present proof)  Flow Scenarios

[https://github.com/decentralized-identity/waci-didcomm/blob/main/present_proof/present-proof-v3.md]

| Agent           | Behaviour                                    | Action                                |
|-----------------|----------------------------------------------|---------------------------------------|
| Scenario V1     | Send a problem report                        | e.p.msg.credential-format-mismatch    |
| Scenario V2     | Send a problem report                        | e.p.msg.invalid-signature             |
| Scenario V3     | Send a problem report                        | e.p.msg.revoked-credentials           |
| Scenario V4     | Send a problem report                        | e.p.msg.expired-credentials           |
| Scenario V5     | Send a problem report                        | e.p.msg.proof-mismatch                |
| Scenario V6     | Send a problem report                        | e.p.msg.schema-mismatch               |
| Scenario V7     | Send a problem report                        | e.p.msg.revoked-or-expired-issuer-key |
| Scenario V8     | Send a problem report (After max retries)    | e.p.req.max-retries-exceeded          |
| Scenario V9(G3) | Send a problem report Any other error        | e.p.error                             |
| Scenario V10    | Send a problem report (DB connection issues) | e.p.me.res.storage                    |

- **V1** - Credential Format Mismatch: The verifier expects a credential in a specific format, but the prover presents it in a different format.
- **V2** - Credentials presented have signatures that cannot be verified. This might be due to the credential being tampered with, or the public key used for verification being incorrect.
- **V3** - Revoked Credentials: The prover presents a credential that has been revoked by the issuer
- **V4** - Expired Credentials: The credentials presented are past their expiry date, making them invalid for the transaction.
- **V5** - Mismatch in the Proof Request and Presentation: The verifier's proof request asks for certain attributes or predicates, but the presentation from the prover doesn't match these requirements.
- **V6** - Schema Mismatch: The verifier expects a credential that adheres to a certain schema, but the credential presented follows a different schema.
- **V7** - Credential Format Mismatch: The verifier expects a credential in a specific format, but the prover presents it in a different format.
- **V8** - Max retries (Cascading Problems): Verification State cannot be moved after max retries
- **V9** - See G3
- **V10** - Database connection or related issue

## Decision Outcome

In the event of an issue in the Cloud Agent, the following actions are taken:

1. The error is logged, including the X-Request-ID and the thread ID (thid).

2. A problem report message is generated and sent out as outlined in the previously mentioned table
  This message is sent in accordance with the return route defined here:
   [Return Route Extension for DIDComm Messaging](https://github.com/decentralized-identity/didcomm-messaging/blob/main/extensions/return_route/main.md).
3. [Implement the Problem Reporting](https://didcomm.org/report-problem/2.0/)

### Out of the Scope

1. The problem report generated is not stored in the database along with the associated X-Request-ID.

2. [Replying to Warnings](https://identity.foundation/didcomm-messaging/spec/#replying-to-warnings)

3. [ACKs](https://identity.foundation/didcomm-messaging/spec/#acks)
</file>

<file path="documentation/adrs/decisions/2024-03-07-handle-errors-in-bg-jobs-by-storing-on-state-records-and-sending-via-webhooks.md">
# Handle errors in background jobs by storing on state records and sending via webhooks

- Status: accepted
- Deciders: David Poltorak, Yurii Shynbuiev, Benjamin Voiturier, Fabio Pinheiro
- Date: 2024-03-07
- Tags: error handling, background jobs, DIDComm, messaging

## Context and Problem Statement

In our system, background jobs play a crucial role in handling tasks such as processing DIDComm messages for establishing connections between agents. Currently, if an error occurs in a background job, the original caller is not notified of this error, leading to potential issues in tracking and managing the state of operations. If a ZIO Failure is encountered, a serialised version of the error (string) is stored in the database along the background job record, however, this attribute is not available on the API when checking the status of an operation.

While the DIDComm Error Reporting protocol effectively handles errors in peer-to-peer communications between agents, our system lacks a robust mechanism for notifying clients of errors occurring in background jobs. This gap in error handling affects transparency and the ability to respond to issues promptly. How can we ensure clients are effectively notified of errors in background jobs, especially when such errors cannot be communicated via DIDComm Error Reporting?

## Decision Drivers

- Transparency and traceability of errors
- System reliability and robust error handling, reliability of background job processing
- Minimising the impact of errors on user experience
- Complementing the DIDComm Error Reporting protocol
- Future scalability is not hampered by the solution we put in place

## Considered Options

1. Storing error information in database records
 - Storing in RFC 9457 Problem Details for HTTP APIs format
 - Storing in proprietary format
 - Storing as ZIO.Failure string (as is)
 - Enhancing the API to return this attribute of the record when checking the status of an operation
2. Creating a central registry of errors
3. Using existing webhook system to send errors to clients
4. Implementing event-driven error notifications

## Decision Outcome

Chosen Option: Combined 1 and 3

We have opted for enhanced error handling by storing error details on background job records in the RFC 9457 Problem Details for HTTP APIs format and leveraging webhooks for error notifications. This approach is selected because it aligns with our objectives of enhancing system reliability and error handling for background jobs. It provides a transparent mechanism for users to be informed about errors and leverages the existing webhook system, thereby avoiding the introduction of unnecessary complexity through event-driven architecture or a central registry.

### Implementation Strategy

- Storing Error Information on Job Records: We will serialise error details into a JSON object that adheres to the RFC 9457 structure and store this information in a dedicated field within the background job records. This method is intended to enhance the visibility of errors and assist in the debugging process.
- Include error attribute in API responses: We will ensure that the error object is returned on any object which represents the state of a background job (Connection, Issuance or Present Proof)
- Webhook Notifications: Our strategy includes making use of the existing webhook system to notify clients of errors in real-time. This utilisation ensures that clients are promptly informed of any issues, enhancing the overall user experience and system responsiveness to error conditions.

### Positive Consequences

- Clients receive immediate, standardised notifications of errors, improving transparency.
- Error details are systematically logged, enhancing system monitoring and debugging capabilities.
- The approach scales well with system growth and can adapt to future requirements for error handling.
- **Error messages logged on records will be secured by the active WalletContext**

### Negative Consequences

- No central place to access error instances as per RFC 9457 specification.
- Clients will need to manage potential unknown failures of webhook calls to their system (from agent to client) as webhook events are not persisted.
- Error information will need to be made available when retrieving background job processing records through the operation API that generated them.

### Storing error information in database records

- Good, because it provides a persistent record of errors.
- Good, RFC 9457 Problem Details for HTTP APIs format, as it provides a standardised way to represent errors.
- Bad, proprietary format, as it requires converting to RFC 9457 to be sent to the user.
- Bad, because it requires manual checking and doesn't proactively notify interested parties.

### Creating a central registry of errors

- Good, because it centralises error management.
- Bad, because it can become a bottleneck and still lacks proactive notification.
- Bad, because it carries a lot of complexity that may not get used as data can be made available on other RESTFul APIs in a more cohesive way (such as retrieving a connection record can include the errors about creating that connection).

### Using existing webhook system to send errors to clients

- Good, because it leverages existing implementation and communication channels to send events.
- Bad, because it is limited by the existing system (events not persisted).

### Implementing event-driven error notifications

- Good, because it provides real-time, scalable notifications.
- Bad, because it requires the setup and management of an event-driven system.

## Links

- [DIDComm Messaging Specification](https://identity.foundation/didcomm-messaging/spec/)
</file>

<file path="documentation/adrs/decisions/2024-05-20-use-did-urls-to-reference-resources.md">
# Storage for SSI related resources 

- Status: accepted 
- Deciders: Javi, Ben, Yurii 
- Date: 2024-05-20
- Tags: Verifiable Data Registry (VDR), decentralized storage


## Context and Problem Statement

The main question to answer is: What is the most practical way to store resources related to VC verification and revocation?

In the context of SSI, there are resources such as credential definitions, schemas, revocation lists, etc., that are referenced inside VCs. These resources need to be accessible to different parties in order to verifiy the credentials. In this ADR, we discuss the trade-offs of different storage alternatives.

## Decision Drivers

A desired solution should balance

- Methods for data integrity and authenticity validation: For instance, if we are referring to a credential definition, the user retrieving the resource should be able to validate that the resource hasn't been altered since its creation. In the case of more dynamic resources, such as revocation lists, which are actually updated throughout time, the recipient party would need to validate that the resource was created by the issuer.
- Data availability: It is important for resources to be highly available. If a resource is missing, it can lead to an inability to validate a VC.
- Decentralization: There must be a consideration to avoid innecesary central points of failure
- Historical data requests: Some use cases may require support for querying historical data. For example, retreive a revocation list at certain point in the past.
- Write access control: Most generally issuers (as they create most of the resources), require to have control of the data they store in order to be able to update it when needed, and also to avoid third parties to make un-authorized changes.
- Latency, throughput, deployment costs: Any solution should provide a reasonable balance of non functional requirements, such as achieving enough throughput, or having low enough latency for the system to be practical.

## Considered Options

We considered the following alternatives, which contemplate the approaches currently discussed by the broad SSI ecosystem at the time of this writing.

- URLs and traditional HTTP servers: with no surprises, in this approach, each resource is identified with a URL and stored in traditional servers. The URLs will encode hashes as query parameters to enforce integrity for static resources. Dynamic resources will be signed by the resource creator's key.
- DID URLs and traditional HTTP servers: in this variation, resources are still stored in servers. Resources are identified by DID URLs that dereference services of the associated DID document. The services will contain the final URL to retrieve the corresponding resources. Once again, hashes will be associated to static resources as DID URL query parameters, while dynamic resources will be signed adequately. 
- IPFS: An IPFS approach would be useful for storing static resources using IPFS identifiers for them. Dynamic resources however become more challenges. Even though we recognize the existence of constructions like IPNS or other layers to manage dynamic resources, we find them less secure in terms of availability and consistency guarantees.
- Ledger based storage (Cardano in particular): In this approach, resources would be stored in transactions' metadata posted on-chain. The data availability and integrity can be inherited from the underlying ledger. 
- A combination of previous methods and the use of a ledger: Similar as above, data references are posted on-chain, but the actual resources are stores in servers. The servers could be traditional HTTP servers or IPFS nodes.

## Decision Outcome

After a careful analysis we concluded the following points:
- There is an architectural need to develop a "proxy" component, a.k.a. VDR proxy, that would work as a first phase for resource resolution. Behind the VDR proxy, different storage implementations could be added as extensions
- With respect to specific implementations
  + ledger based storage at this stage introduces latency, throughput bottlenecks, costs and other issues not suitable for most use cases. 
  + Hybrid solutions that make use of a ledger share similar drawbacks. 
  + Decentralized Hash Tables (such as IPFS) do not provide efficient handling for dynamic resources (such as revocation lists).
  + We concluded that a reasonable first iteration could be delivered using DID URLs to identify resources while they would be, a priori, stored in traditional HTTP servers.

### Positive Consequences

- The implementation of a VDR proxy enables a transparent abstraction that allows to extend the possible methods to retrieve resources.
- DID URLs allow for a fair decentralization level at issuer's disposal to control the location of resources

### Negative Consequences

- There is a level of under-specification at W3C specifications with respect to DID URL dereferencing. This forces us to define the under-specificied behaviour or simply creata-our-own solution.

## Links 

We leave a list of useful links for context

- [AnonCreds Methods Registry](https://hyperledger.github.io/anoncreds-methods-registry/)
- [AnonCreds Specification](https://hyperledger.github.io/anoncreds-spec/)
- [W3C DID resolution algorithm](https://w3c-ccg.github.io/did-resolution/)
</file>

<file path="src/components/atala-graphic/index.js">
import React, {useLayoutEffect, useRef, SVGSVGElement, useMemo, useState, useEffect} from 'react';
import styles from './index.module.css';
export default function AtalaGraphic() {
    const [text, setText] = useState("");

    function generateRandomHash(length) {
        const characters = 'ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789';
        let result = '';

        for (let i = 0; i < length; i++) {
            result += characters.charAt(Math.floor(Math.random() * characters.length));
        }

        return result;
    }

    useEffect(() => {
        setInterval(() => {
            setText(generateRandomHash(5500));
        }, 200)
    }, [])

    return (
        <div className={styles.hero__graphic__wrapper}>
            <div className={styles.hero__graphic}>
                {text}
            </div>
        </div>
    )
}
</file>

<file path="src/components/blob/index.js">
import React, {useCallback, useEffect, useRef} from 'react';
import styles from './index.module.css'

export default function Blob() {
    const blob = useRef(null);
    const handlePointerMove = useCallback((event) => {
        const {clientX, clientY} = event;
        blob.current.animate({
            top: `${clientY}px`,
            left: `${clientX}px`,
        }, {duration: 20000, fill: "forwards"})
    }, [blob])
    useEffect(() => {
        window.addEventListener("pointermove", handlePointerMove);
        return () => {
            window.removeEventListener("pointermove", handlePointerMove)
        }
    }, [])
    return (
        <div style={{
            position: "fixed",
            zIndex: -1,
            width: "100%",
            height: "100%",
            top: 0,
            overflow: "hidden",
            right: 0,
            left: 0,
            pointerEvents: "none",
            filter: "blur(150px)",
        }}>
            <div ref={blob} className={styles.blob}/>
            <div className={styles.blob__blur}/>
        </div>
    )
}
</file>

<file path="src/components/blob/index.module.css">
.blob {
    margin: 0;
    padding: 0;
    width: 600px;
    height: 600px;
    border-radius: 50%;
    top: 50%;
    left: 60%;
    background: var(--ifm-blob-background);
    position: absolute;
    translate: -50% -50%;
    animation: rotate 5s infinite;
    opacity: .2;
    z-index: 0;
    pointer-events: none;
}

.blob__blur {
    width: 100%;
    height: 100%;
    pointer-events: none;
    z-index: 1;
    right: 0;
    bottom: 0;
}

@keyframes rotate {
    from {
        transform: rotate(0deg);
    }
    25% {
        scale: 1 1.5;
    }
    50% {
        scale: 1.5 1;
    }
    to {
        transform: rotate(360deg);
    }
}
</file>

<file path="src/components/button/index.js">
import React, {useCallback, useRef} from 'react';
import Link from '@docusaurus/Link';
import styles from './styles.module.css'
export default function Button({to, className, children}) {
    const lightRef = useRef(null);
    const handleMouseMove = useCallback((event) => {

        const {clientX, clientY, currentTarget} = event;
        const {
            x: offsetX,
            y: offsetY
        } = currentTarget.getBoundingClientRect();

        const mouseX = (clientX - offsetX);
        const mouseY = (clientY - offsetY);

        lightRef.current.animate({
            top: `${mouseY}px`,
            left: `${mouseX}px`,
        }, {fill: "forwards", duration: 500})
    }, [])

    const handleMouseLeave = useCallback(() => {

        lightRef.current.animate({
            top: `50%`,
            left: `50%`,
        }, {fill: "forwards", duration: 500})
    }, [])
    return (
        <Link to={to} className={`${styles.button} ${className}`} onMouseMove={handleMouseMove} onMouseLeave={handleMouseLeave}>
            {children}
            <div className={styles.light} ref={lightRef}/>
        </Link>
    )
}
</file>

<file path="src/components/button/styles.module.css">
.button {
    position: relative;
    margin-top: 1.5rem;
    cursor: pointer;
    border-radius: 5rem;
    background-color: var(--ifm-color-primary);
    padding: .7rem 1.5rem;
    color: #fff;
    font-weight: 500;
    overflow: hidden;
}

.button:hover {
    color: white;
}

.button:hover .light {
    opacity: 1;
}

.light {
    top: 50%;
    left: 50%;
    opacity: 0;
    position: absolute;
    width: 8rem;
    height: 8rem;
    transform: translateX(-50%) translateY(-50%);
    pointer-events: none;
    border-radius: 50%;
    background: radial-gradient(circle, rgba(255, 255, 255, .1) 0%, transparent 70%);
    z-index: 10;
}
</file>

<file path="src/plugins/remarkLinkFixer.js">
const { visit } = require('unist-util-visit');

/**
 * @typedef {'exact' | 'prefix' | 'regex'} RuleType
 *
 * @typedef {Object} ExactRule
 * @property {'exact'} type
 * @property {string} from
 * @property {string} to
 *
 * @typedef {Object} PrefixRule
 * @property {'prefix'} type
 * @property {string} from
 * @property {string} to
 *
 * @typedef {Object} RegexRule
 * @property {'regex'} type
 * @property {string} from
 * @property {string} to
 * @property {string} [flags]
 *
 * @typedef {ExactRule | PrefixRule | RegexRule} LinkRule
 *
 * @typedef {Object} RemarkLinkFixerOptions
 * @property {LinkRule[]} [rules]
 * @property {boolean} [debug]
 */

/**
 * Remark plugin that rewrites markdown links based on configurable rules.
 *
 * Supports:
 * - exact match rules
 * - prefix rules (preserve the rest of the URL)
 * - regex rules (via String.replace)
 *
 * @param {RemarkLinkFixerOptions} options
 * @returns {import('unified').Transformer}
 */
function remarkLinkFixer(options = {}) {
  const { rules = [], debug = false } = options;

  /** @type {(ExactRule | PrefixRule | (RegexRule & { pattern: RegExp }))[]} */
  const compiledRules = rules.map(rule => {
    if (rule.type === 'regex') {
      return {
        ...rule,
        pattern: new RegExp(rule.from, rule.flags || ''),
      };
    }
    return rule;
  });

  return (tree, file) => {
    // Docusaurus populates vfile.path; history[0] is original path if present
    const filePath =
      (file && Array.isArray(file.history) && file.history[0]) ||
      file.path ||
      undefined;

    const applyRules = (url) => {
      let result = url;

      for (const rule of compiledRules) {
        if (!result) break;

        if (rule.type === 'exact') {
          if (result === rule.from) {
            result = rule.to;
          }
        } else if (rule.type === 'prefix') {
          if (result.startsWith(rule.from)) {
            const rest = result.slice(rule.from.length);
            if (rest && rule.to.endsWith('/') && rest.startsWith('/')) {
              result = rule.to + rest.slice(1);
            } else {
              result = rule.to + rest;
            }
          }
        } else if (rule.type === 'regex') {
          result = result.replace(rule.pattern, rule.to);
        }
      }

      if (debug && result !== url) {
        // eslint-disable-next-line no-console
        console.log(
          `[remark-link-fixer] ${filePath ?? '<unknown file>'}: ${url} -> ${result}`,
        );
      }

      return result;
    };

    visit(tree, ['link', 'definition'], (node) => {
      if (!node.url || typeof node.url !== 'string') {
        return;
      }

      const next = applyRules(node.url);
      if (next && next !== node.url) {
        node.url = next;
      }
    });
  };
}

module.exports = remarkLinkFixer;
</file>

<file path="src/theme/Footer/index.js">
import React from 'react';
import {useThemeConfig, useColorMode} from '@docusaurus/theme-common';
import styles from './index.module.css';

const Logo = ({color = "#fff"}) => (
    <svg xmlns="http://www.w3.org/2000/svg" width={145} height={38} fill="none"></svg>)

function Footer() {
    const {footer} = useThemeConfig();
    const {colorMode} = useColorMode()
    if (!footer) {
        return null;
    }
    const {copyright} = footer;

    return (
        <div className="container">
            <footer className={styles.footer}>
                <Logo color={colorMode === 'light' ? "#101828" : "#ffffff"}/>
                <span className={styles.copyright}>{copyright}</span>
            </footer>
        </div>
    );
}

export default React.memo(Footer);
</file>

<file path="src/theme/Footer/index.module.css">
.footer {
    display: flex;
    width: 100%;
    height: auto;
    justify-content: space-between;
    padding: 2rem 0 3rem;
    border-top: 1px solid var(--ifm-footer-hr-color);
    flex-wrap: wrap;
    gap: 1rem;
}

.copyright {
    color: var(--ifm-color-gray-500);
}
</file>

<file path="documentation/adrs/decisions/2023-01-18-quill-library-for-sql-statement-generation.md">
# Quill library for SQL statement generation and validation

- Status: accepted
- Deciders: Yurii Shynbuiev, Fabio Pinheiro, Benjamin Voiturier
- Date: [2023-01-17]
- Tags: DAL, SQL, Postrgresql, Typesafe

## Context and Problem Statement

PostgreSQL is essential to the Identus platform technology stack, where most entities are stored.

Backend services: Identus Cloud Agent, Identus Mediator, and PRISM Node use PostgreSQL.

[Doobie](https://tpolecat.github.io/doobie/index.html) library is currently used in Scala code to communicate with Postgresql. Quotes from the website

```
Doobie is a pure functional JDBC layer for Scala and Cats. It is not an ORM, nor is it a relational algebra; 
it simply provides a functional way to construct programs (and higher-level libraries) that use JDBC
doobie is a Typelevel project. 
This means we embrace pure, typeful, functional programming, and provide a safe and friendly environment for teaching, learning, and contributing as described in the Scala Code of Conduct.
```
Doobie is a good choice for DAL, and this ADR is about something other than replacing it.

Writing the SQL statement and mapping the row to the case class is a boilerplate and error-prone activity that the Quill library can optimize.

**Writing the code for mapping a table row to a case class and writing the low-level SQL statement is an error-prone and boilerplate thing**

**Using the [Quill](https://getquill.io/) library on top of Doobie can optimize and improve these things.**
Quote from the website:

```
Quill provides a Quoted Domain Specific Language (QDSL) to express queries in Scala and execute them in a target language. The library’s core is designed to support multiple target languages, currently featuring specializations for Structured Query Language (SQL) and Cassandra Query Language (CQL).

1. Boilerplate-free mapping: The database schema is mapped using simple case classes.
2. Quoted DSL: Queries are defined inside a quote block. Quill parses each quoted block of code (quotation) at compile time and translates them to an internal Abstract Syntax Tree (AST)
3. Compile-time query generation: The ctx.run call reads the quotation’s AST and translates it to the target language at compile time, emitting the query string as a compilation message. As the query string is known at compile time, the runtime overhead is very low and similar to using the database driver directly.
4. Compile-time query validation: If configured, the query is verified against the database at compile time, and the compilation fails if it is not valid. The query validation does not alter the database state.
```

There are [Slick](https://scala-slick.org/) and [ScalikeJDBC](http://scalikejdbc.org/) libraries as well.

Comparison of these libraries is not a goal of this ADR, but it's essential to know the differences.

There are good references to take a look at in the [Links](#links) section.

Overall, all libraries have differences in the following aspects:

- Metamodel (how to define the schema and type mapping)
- Static SQL statement (how and where does the SQL statement is written/generated)
- Dynamic SQL statement (how and where does the dynamic SQL statement written/generated)
- Connection Management (thread and connection pooling)
- Asynchronous API (the high-level API to execute queries blocking or non-blocking)
- Asynchronous IO (is IO operation blocking or asynchronous)
- Effect library that is used (free-monad, Future, Task, ZIO)

## Decision Drivers

- Generate and validate SQL statement based on the convention-over-configuration approach in compile time (type-safe queries)
- Reduce boilerplate and error-prone code
- Easy to write the dynamic queries

## Considered Options

- Doobie (Quill for the connection pooling, SQL statement execution, and SQL statement writing)
- Doobie + Quill (Quill for the connection pooling, SQL statement execution, and SQL statement writing + Quill for the SQL statement generation)
- Quill (Quill for the connection pooling, SQL statement execution, and SQL statement writing and generation)

## Decision Outcome

Chosen option: "Doobie + Quill" because it's the simplest solution that requires minimal changes to the existing code and brings the benefits of automatic SQL statement generation and validation in compile time (see below).

### Positive Consequences

- convention-over-configuration approach for the generation and validation of SQL statements using macros in the compile time
- easy work with dynamic queries
- backward compatible solution (minimum changes are required for the current code base)

### Negative Consequences

- DTO case classes are required for each table to generate the SQL statement based on the convention

## Pros and Cons of the Options

### Doobie

Doobies library is used as it is right now without any changes

- Good, because it is a solid FP library for Postgresql
- Good, because it has good documentation and a large community of developers who contribute to the library
- Good, because it is built using Free Monad, which makes it composable and easy to integrate with any popular effects library
- Bad, because it has a low-level API for writing the SQL statement (boilerplate and error-prone code)
- Bad, because it uses blocking IO at the network level

### Doobie+Quill

Doobie library is used as it is right now, and Quill library is used for SQL statement generation and validation in compile time

- Good, because it ss a solid FP library for Postgresql
- Good, because it has good documentation and a large community of developers who contribute to the library
- Good, because it is built using Free Monad, which makes it composable and easy to integrate with any popular effects library
- Good, because Quill library is used for SQL statement generation at the compile time
- Good, because Quill library extends the current solution, and no changes to the code base are required
- Bad, because the DTO case class must be created for each table
- Bad, because it uses blocking IO at the network level

### Quill

Quill is used instead of Doobie

- Good, because it is a solid FP library for Postgresql
- Good, because it has good documentation and a large community of developers who contribute to the library
- Good, because it is built using Free Monad, which makes it composable and easy to integrate with any widespread effects library
- Good, because it is used for SQL statement generation at the compile time instead of using Doobie low-level API
- Good, because it can be configured to use non-blocking IO at the network level
- Good, because it get rid of the `cats` ecosystem that comes with `doobie` (simplify the dependency management)
- Bad, because significant refactoring of all DAL is required
- Bad, because the DTO case class must be created for each table

## Examples

### Doobie

```
import doobie._
import doobie.implicits._
import doobie.postgres._

case class Person(id: Int, name: String)

val q = sql"SELECT id, name FROM person WHERE id = 1".query[Person]

val result: ConnectionIO[List[Person]] = q.to[List].transact(Transactor.fromDriverManager[IO](
  "org.postgresql.Driver", "jdbc:postgresql:world", "username", "password"
))
```

### Quill

```
import io.getquill._

val ctx = new SqlMirrorContext(PostgresDialect, "ctx")

case class Person(id: Int, name: String)

val q = quote {
  query[Person].filter(p => p.id == 1)
}

val result: List[Person] = ctx.run(q)
```

### Slick

```
import slick.jdbc.PostgresProfile.api._

val db = Database.forConfig("database")

case class Person(id: Int, name: String)

val q = TableQuery[Person].filter(_.id === 1)

val result: Future[Seq[Person]] = db.run(q.result)
```

#### Two more real example of Doobie and Quill usage are in the [Links](#links) section

## Links

- [Comparing Scala relational database access libraries](https://softwaremill.com/comparing-scala-relational-database-access-libraries/)
- [Comparison with Alternatives](https://scala-slick.org/docs/compare-alternatives)
- [Doobie vs Quill](https://www.libhunt.com/compare-doobie-vs-zio-quill)
- [Slick vs Doobie](https://www.libhunt.com/compare-slick--slick-vs-doobie?ref=compare)
- [Database access libraries in Scala](https://medium.com/@takezoe/database-access-libraries-in-scala-7aa7590aa3db)
- [Typechecking SQL queries with doobie](https://godatadriven.com/blog/typechecking-sql-queries-with-doobie/)
- [Typechecking SQL in Slick and doobie](https://underscore.io/blog/posts/2015/05/28/typechecking-sql.html)
- [Doobie example in the Pollux library](https://github.com/hyperledger-identus/cloud-agent/blob/pollux-v0.17.0/pollux/lib/sql-doobie/src/main/scala/io/iohk/atala/pollux/sql/repository/JdbcCredentialRepository.scala)
- [Quill example in the Pollux library](https://github.com/hyperledger-identus/cloud-agent/blob/pollux-v0.17.0/pollux/lib/sql-doobie/src/main/scala/io/iohk/atala/pollux/sql/model/VerifiableCredentialSchema.scala)
</file>

<file path="documentation/adrs/decisions/2024-01-16-use-zio-failures-and-defects-effectively.md">
# Use ZIO Failures and Defects Effectively

- Status: accepted
- Deciders: Fabio Pinheiro, Shailesh Patil, Pat Losoponkul, Yurii Shynbuiev, David Poltorak, Benjamin Voiturier
- Date: 2024-03-29
- Tags: error-handling, zio

## Context and Problem Statement

ZIO is a powerful and purely functional library for building scalable and resilient applications in Scala. However,
effectively handling errors within the context of ZIO presents challenges that must be addressed.

Within our software development projects utilising ZIO, the management and handling of errors have emerged as areas
requiring more clarity and strategy. The existing practices have shown limitations in terms of their efficiency and
comprehensiveness.

The key issues are:

1. **Lack of Consistent Error Handling Strategies**: There's inconsistency in error handling across different modules
   and components of our ZIO-based applications, making it challenging to maintain a unified approach.
2. **Understanding and Communicating Errors**: There's a need for a clearer method to categorise errors and communicate
   these effectively within the team and across various layers, facilitating quicker identifications and resolutions.
3. **Optimising Error Recovery Mechanisms**: While ZIO provides powerful abstractions for error recovery, there's a
   necessity to optimise these mechanisms to ensure graceful degradation and resilience in our applications.

This ADR aims to explore and define strategies for utilising ZIO's capabilities more effectively in handling errors. It
will outline decision drivers, available options, their pros and cons, and ultimately, the recommended approach to
enhance our error management practices with the ZIO framework.

Effective management of errors directly impacts the reliability, maintainability, and customer experience of our
applications.

By addressing the challenges of consistent error handling, we aim to enhance the stability of our products, ensuring
reduced downtime, clearer communication with customers through structured error messages, and quicker issue
resolution.

This not only improves the overall customer experience but also accelerates feature delivery as developers can focus
more on implementing new functionalities rather than troubleshooting ad-hoc errors. The resulting streamlined
development process contributes to cost reduction and optimised resource allocation.

In essence, these efforts are customer-centric, aiming to deliver a reliable, efficient, and customer-friendly service
interface that positively impacts the overall customer experience and product adoption.

## Decision Drivers

1. **Consistency and Standardization**: A consistent and standardised approach to error handling across different
   modules and components of our ZIO applications is crucial. This consistency will ease code readability, maintenance,
   and team collaboration.
2. **Robustness and Resilience**: A key driver is to harden our applications against failures by leveraging ZIO's
   powerful error recovery mechanisms. Enhancing the robustness of our applications will improve their resilience in
   adverse conditions.
3. **Traceability and Debugging**: Reducing debugging time and efforts associated with error resolution is another
   driving factor. An efficient error-handling strategy should enable traceability and quicker identification,
   communication, and resolution of errors.
4. **User Experience and Reliability**: Improving the quality and clarity of error messages reported to our users is a
   key driver. We aim to refine error messages to enable users to better understand what’s going on and respond to
   issues,
   thereby enhancing the user experience and the overall reliability of our applications.
5. **Alignment with Best Practices**: Aligning our error-handling strategies with industry best practices and leveraging
   the full potential of ZIO's error management features is a driver. Adhering to established standards can lead to more
   effective and maintainable code.

## Considered Options

### Option 1: Continue With The Current Error Handling Strategy

Continuing with the existing "so-so" error handling strategy currently in place within our ZIO applications without
significant modifications or improvements.

Pros:

- Maintains continuity with current practices, potentially requiring minimal adjustments and avoiding immediate
  disruptions to ongoing projects.

Cons:

- Inconsistencies in the code base across the components and services may persist, leading to potential challenges in
  maintenance and scalability.
- Engineers may spend time reinventing error-handling solutions rather than leveraging established best practices or
  frameworks.
- No significant improvement in terms of traceability and problem debugging, potentially hindering the resolution of
  issues and defects.

### Option 2: Leverage ZIO Failures And Defects Effectively

Adopting best practices for error handling within ZIO applications and effectively utilising ZIO Failures and Defects,
defining and implementing stricter guidelines and protocols for error handling.

Pros:

- Ensures adherence to established best practices, promoting code consistency, reliability, and scalability across
  various components and services.
- Reduces the need for developers to reinvent error-handling solutions, allowing them to leverage proven strategies and
  frameworks.
- Improves traceability and problem debugging by employing standardised error-handling practices, facilitating quicker
  issue identification and resolution.

Cons:

- Will require adjustments to current code and practices, necessitating time and effort for implementation.Actual impact
  and workload still needs to be identified.

## Decision Outcome

It has been decided to pursue Option 2: **Leverage ZIO Failures and Defects Effectively**. This decision aligns with our
commitment to enhancing the reliability, scalability, and maintainability of our applications.

While we acknowledge this option requires more refactoring, its long-term benefits in terms of code quality, developer
efficiency, and robust error management outweigh the associated refactoring efforts.

## Option 2 - General Implementation Rules and Principles

### Case 1: When designing a component or service

#### Carefully Segregate Error Types

When designing a new component or service, the nature of anticipated errors should be carefully considered, and a clear
distinction between expected errors (i.e. ZIO Failures or domain-specific errors) and unexpected errors (i.e. ZIO
Defects) should be made.

This segregation should be done according to the principles outlined in
the [ZIO Types of Errors](https://zio.dev/reference/error-management/types) documentation section.
That is, carefully distinguishing between:

- **ZIO Failures**
  - The expected/recoverable errors (i.e. domain-specific errors).
  - Declared in the Error channel of the effect => ZIO[R, E, A].
  - Supposed to be handled by the caller to prevent call stack propagation.

- **ZIO Defects**
  - The unexpected/unrecoverable errors.
  - Not represented in the ZIO effect.
  - We do NOT expect the caller to handle them.
  - Propagated throughout the call stack until converted to a Failure or logged for traceability and debugging
      purposes by
      the uppermost layer.

#### Use ADT to Model ZIO Failures

Use **Algebraic Data Types** (ADTs) to model domain-specific errors as ZIO failures within the component/service
interface.

_Implementation tips:_

- **Use a sealed trait or abstract class** to represent the hierarchy of ZIO Failures, allowing for a well-defined set
  of error possibilities.
- **Define specific error cases within the companion object** of the sealed trait. This practice prevents potential
  conflicts when importing errors with common names (e.g. RecordNotFoundError), allowing users to prefix them with the
  name of the parent sealed trait for better code clarity.

_Example:_

```scala
sealed trait DomainError

object DomainError {
  final case class BusinessLogicError(message: String) extends DomainError

  final case class DataValidationError(message: String) extends DomainError
}
```

#### Use Scala 3 Union Types to Be More Specific About ZIO Failure Types

Using the Scala 3 Union Types feature to declare the expected failures of a ZIO effect should be preferred over using
the broader and more generic top-level sealed trait. This allows for a more explicit and detailed definition of
potential failure scenarios and enhances error handling accuracy on the caller side.

This principle is outlined in
the [following section](https://zio.dev/reference/error-management/best-practices/union-types) of the ZIO Error
Management Best Practices documentation.

_Example:_

```scala
trait MyService {
  def myMethod(): ZIO[Any, BusinessLogicError | DataValidationError, Unit]
}
```

#### Don’t Type Unexpected Errors (i.e. ZIO Defects)

When we first discover typed errors, it may be tempting to put every error into the ZIO failure type parameter/channel.
That is a mistake because we can't recover from all types of errors. When we encounter unexpected errors we can't do
anything with, we should let the application die (i.e. the ZIO fiber). This is known as the “Let it Crash” principle,
and it is a good approach for all unexpected errors.

This principle is outlined in
the [following section](https://zio.dev/reference/error-management/best-practices/unexpected-errors) of the ZIO Error
Management Best Practices documentation.

### Case 2: When calling an existing component or service

#### Only Catch Failures You Effectively Handle

As a user of an existing component or service, you should exclusively catch failures that you are prepared to
effectively handle. Any unhandled failures should be transformed into defects and propagated through the call stack. You
should not expect callers of your component to handle lower-level failures that you do not handle.

#### Use Failure Wrappers To Prevent Failures Leakage From Lower Layers

Do not directly expose their failure types in your component interface when invoking lower-level components. Use
wrappers to encapsulate and abstract failure types originating from lower-level components, thus enhancing
loose coupling and safeguarding against leakage of underlying implementation details to the caller.

Using failure wrappers and propagating them **should not be the default strategy**. Lower-level failures should
primarily
be managed at your component implementation level, ensuring that it appropriately handles and recovers them.

Failures not handled within the component's boundaries should preferably be transformed into defects whenever possible.

#### Do not reflexively log errors

Avoid catching a ZIO failure or defect solely for the purpose of logging it. Instead, consider allowing the error to
propagate through the call stack. It's preferable to assume that the uppermost layer, commonly known as **'the end of
the world' will handle the logging** of those errors. This practice promotes a centralised and consistent logging
approach for better traceability and debugging.

This principle is outlined in
the [following section](https://zio.dev/reference/error-management/best-practices/logging-errors) of the ZIO Error
Management Best Practices documentation.

#### Adopt The “Let it Crash” Principle For ZIO Defects

Adopt the “Let it Crash” principle for ZIO defects. Let them bubble up the call stack and crash the current ZIO fiber.
They will be handled/recovered at a higher level or logged appropriately “at the end of the world” by the uppermost
layer.

## Option 2 - Practical Implementation

### Repository Layer

#### Try using defects only (`UIO` or `URIO`)

The repository layer leverages Doobie,
which [natively relies on unchecked exceptions](https://tpolecat.github.io/doobie/docs/09-Error-Handling.html#about-exceptions).
Doobie will report any database error as a subclass of `Throwable`, and its specific type will be directly
linked to the underlying database implementation (i.e. PostgreSQL). Handling it this way means there is no deterministic
way to recover
from an SQL execution error in a database-agnostic way.

A good approach is to use ZIO Defects to report repository errors, declaring all repository methods as `URIO`
or `UIO`([example](https://github.com/hyperledger-identus/cloud-agent/blob/main/connect/lib/core/src/main/scala/io/iohk/atala/connect/core/repository/ConnectionRepository.scala)).
Conversely, declaring them as `Task` assumes that the caller (i.e. service) can properly handle and
recover from the low-level and database-specific exceptions exposed in the error channel, which is a fallacy.

```scala
trait ConnectionRepository {
  def findAll: URIO[WalletAccessContext, Seq[ConnectionRecord]]
}
```

Converting a ZIO `Task` to ZIO `UIO` can easily be done
using `ZIO#orDie`([example](https://github.com/hyperledger-identus/cloud-agent/blob/eb898e068f768507d6979a5d9bab35ef7ad4a045/connect/lib/sql-doobie/src/main/scala/io/iohk/atala/connect/sql/repository/JdbcConnectionRepository.scala#L114)).

```scala
class JdbcConnectionRepository(xa: Transactor[ContextAwareTask], xb: Transactor[Task]) extends ConnectionRepository {
  override def findAll: URIO[WalletAccessContext, Seq[ConnectionRecord]] = {
    val cxnIO =
      sql"""
           | SELECT
           |   id,
           |   created_at,
           |   ...
           | FROM public.connection_records
           | ORDER BY created_at
        """.stripMargin
        .query[ConnectionRecord]
        .to[Seq]

    cxnIO
      .transactWallet(xa)
      .orDie
  }
}
```

For those cases where one has to generate a defect, a common way to do this is by using the following ZIO
construct ([example](https://github.com/hyperledger-identus/cloud-agent/blob/eb898e068f768507d6979a5d9bab35ef7ad4a045/connect/lib/sql-doobie/src/main/scala/io/iohk/atala/connect/sql/repository/JdbcConnectionRepository.scala#L212)):

```scala
class JdbcConnectionRepository(xa: Transactor[ContextAwareTask], xb: Transactor[Task]) extends ConnectionRepository {
  override def getById(recordId: UUID): URIO[WalletAccessContext, ConnectionRecord] =
    for {
      maybeRecord <- findById(recordId)
      record <- ZIO.fromOption(maybeRecord).orDieWith(_ => RuntimeException(s"Record not found: $recordId"))
    } yield record
}
```

#### Apply the `get` vs `find` pattern

Follow the `get` and `find` best practices in the repository interface for read operations:

- `getXxx()` returns the requested record or throws an unexpected exception/defect when not
  found ([example](https://github.com/hyperledger-identus/cloud-agent/blob/eb898e068f768507d6979a5d9bab35ef7ad4a045/connect/lib/core/src/main/scala/io/iohk/atala/connect/core/repository/ConnectionRepository.scala#L36)).
- `findXxx()` returns an `Option` with or without the request record, which allows the caller service to handle
  the `found`
  and `not-found` cases and report appropriately to the end
  user ([example](https://github.com/hyperledger-identus/cloud-agent/blob/eb898e068f768507d6979a5d9bab35ef7ad4a045/connect/lib/core/src/main/scala/io/iohk/atala/connect/core/repository/ConnectionRepository.scala#L32)).

```scala
trait ConnectionRepository {
  def findById(recordId: UUID): URIO[WalletAccessContext, Option[ConnectionRecord]]

  def getById(recordId: UUID): URIO[WalletAccessContext, ConnectionRecord]
}
```

#### Do not return the affected row count

The `create`, `update` or `delete` repository methods should not return an `Int` indicating the number of rows affected
by the operation but either return `Unit` when successful or throw an exception/defect when the row count is not what is
expected, like i.e. an update operation resulting in a `0` affected row
count ([example](https://github.com/hyperledger-identus/cloud-agent/blob/eb898e068f768507d6979a5d9bab35ef7ad4a045/connect/lib/sql-doobie/src/main/scala/io/iohk/atala/connect/sql/repository/JdbcConnectionRepository.scala#L85)).

```scala
class JdbcConnectionRepository(xa: Transactor[ContextAwareTask], xb: Transactor[Task]) extends ConnectionRepository {
  override def create(record: ConnectionRecord): URIO[WalletAccessContext, Unit] = {
    val cxnIO =
      sql"""
           | INSERT INTO public.connection_records(
           |   id,
           |   created_at,
           |   ...
           | ) values (
           |   ${record.id},
           |   ${record.createdAt},
           |   ...
           | )
        """.stripMargin.update

    cxnIO.run
      .transactWallet(xa)
      .ensureOneAffectedRowOrDie
  }
}

extension[Int](ma: RIO[WalletAccessContext, Int]) {
  def ensureOneAffectedRowOrDie: URIO[WalletAccessContext, Unit] = ma.flatMap {
    case 1 => ZIO.unit
    case count => ZIO.fail(RuntimeException(s"Unexpected affected row count: $count"))
  }.orDie
}
```

#### Do not reflexively log errors

The upper layer will automatically do so appropriately and consistently using Tapir interceptor customization.

### Service Layer

#### Reporting `404 Not Found` to user

How can a service appropriately report a `404 Not Found` to a user that i.e. tries to update a record
that does not exist in the database? Following the above rules, the `update` method will throw a defect that will be
caught at the upper level and returns a generic `500 Internal Server Error` to the user.

For those cases where a specific error like `404` should be returned, it is up to the service to first call `find()`
before `update()` and construct a `NotFound` failure, propagated through the error channel, if it gives
a `None` ([example](https://github.com/hyperledger-identus/cloud-agent/blob/eb898e068f768507d6979a5d9bab35ef7ad4a045/connect/lib/core/src/main/scala/io/iohk/atala/connect/core/service/ConnectionServiceImpl.scala#L149)).

Relying on the service layer to implement it will guarantee consistent behavior regardless of the underlying database
type (could be different RDMS flavor, No-SQL, etc.).

```scala
class ConnectionServiceImpl() extends ConnectionService {
  override def markConnectionRequestSent(recordId: UUID):
  ZIO[WalletAccessContext, RecordIdNotFound | InvalidStateForOperation, ConnectionRecord] =
    for {
      maybeRecord <- connectionRepository.findById(recordId)
      record <- ZIO.fromOption(maybeRecord).mapError(_ => RecordIdNotFound(recordId))
      updatedRecord <- updateConnectionProtocolState(
        recordId,
        ProtocolState.ConnectionRequestPending,
        ProtocolState.ConnectionRequestSent
      )
    } yield updatedRecord
}
```

#### Do not type unexpected errors

Do not wrap defects from lower layers (typically repository) in a failure and error case class declarations
like [this](https://github.com/hyperledger-identus/cloud-agent/blob/b579fd86ab96db711425f511154e74be75583896/connect/lib/core/src/main/scala/io/iohk/atala/connect/core/model/error/ConnectionServiceError.scala#L8)
should be prohibited.

Considering that failures are viewed as **expected errors** from which users can potentially recover, error case classes
like `UnexpectedError` should be
prohibited ([example](https://github.com/hyperledger-identus/cloud-agent/blob/b579fd86ab96db711425f511154e74be75583896/connect/lib/core/src/main/scala/io/iohk/atala/connect/core/model/error/ConnectionServiceError.scala#L12)).

#### Extend the common `Failure` trait

Make sure all service errors extend the shared
trait [`org.hyperledger.identus.shared.models.Failure`](https://github.com/hyperledger-identus/cloud-agent/blob/main/shared/src/main/scala/io/iohk/atala/shared/models/Failure.scala).
This allows handling "at the end of the world“ to be done in a consistent and in generic way.

Create an exhaustive and meaningful list of service errors and make sure the value of the `userFacingMessage` attribute
is chosen wisely! It will present "as is" to the user and should not contain any sensitive
data ([example](https://github.com/hyperledger-identus/cloud-agent/blob/eb898e068f768507d6979a5d9bab35ef7ad4a045/connect/lib/core/src/main/scala/io/iohk/atala/connect/core/model/error/ConnectionServiceError.scala#L14)).

```scala
trait Failure {
  val statusCode: StatusCode
  val userFacingMessage: String
}

sealed trait ConnectionServiceError(
                                     val statusCode: StatusCode,
                                     val userFacingMessage: String
                                   ) extends Failure

object ConnectionServiceError {
  final case class InvitationAlreadyReceived(invitationId: String)
    extends ConnectionServiceError(
      StatusCode.BadRequest,
      s"The provided invitation has already been used: invitationId=$invitationId"
    )
}
```

#### User Input Validation

Enforcing user input validation (business invariants) should primarily sit at the service layer and be implemented using
the
[ZIO Prelude framework](https://zio.dev/zio-prelude/functional-data-types/validation).

While partially implementing user input validation at the REST entry point level via OpenAPI specification, it is
crucial to enforce validation at the service level as well. This implementation ensures consistency and reliability
across all interfaces that may call our services, recognizing that REST may not be the sole interface interacting with
our services.

```scala
class ConnectionServiceImpl() extends ConnectionService {
  def validateInputs(
                      label: Option[String],
                      goalCode: Option[String],
                      goal: Option[String]
                    ): IO[UserInputValidationError, Unit] = {
    val validation = Validation
      .validate(
        ValidationUtils.validateLengthOptional("label", label, 0, 255),
        ValidationUtils.validateLengthOptional("goalCode", goalCode, 0, 255),
        ValidationUtils.validateLengthOptional("goal", goal, 0, 255)
      )
      .unit
    ZIO.fromEither(validation.toEither).mapError(UserInputValidationError.apply)
  }
}
```

Modeling validation errors should use a dedicated error case class and, when possible, provide validation failure
details. One could use a construct like the following:

```scala
sealed trait ConnectionServiceError(
                                     val statusCode: StatusCode,
                                     val userFacingMessage: String
                                   ) extends Failure

object ConnectionServiceError {
  final case class UserInputValidationError(errors: NonEmptyChunk[String])
    extends ConnectionServiceError(
      StatusCode.BadRequest,
      s"The provided input failed validation: errors=${errors.mkString("[", "], [", "]")}"
    )
}
```

#### Use Scala 3 Union Types

Use Scala 3 union-types declaration in the effect’s error channel to notify the caller of potential
failures ([example](https://github.com/hyperledger-identus/cloud-agent/blob/eb898e068f768507d6979a5d9bab35ef7ad4a045/connect/lib/core/src/main/scala/io/iohk/atala/connect/core/service/ConnectionServiceImpl.scala#L178))

````scala
class ConnectionServiceImpl() extends ConnectionService {

  override def receiveConnectionRequest(request: ConnectionRequest, expirationTime: Option[Duration] = None):
  ZIO[WalletAccessContext, ThreadIdNotFound | InvalidStateForOperation | InvitationExpired, ConnectionRecord] = ???

}
````

#### Do not reflexively log errors

The upper layer will automatically do so appropriately and consistently using Tapir interceptor customization.

### Controller Layer

#### Reporting RFC-9457 Error Response

All declared Tapir endpoints must
use [`org.hyperledger.identus.api.http.ErrorResponse`](https://github.com/hyperledger-identus/cloud-agent/blob/main/cloud-agent/service/server/src/main/scala/io/iohk/atala/api/http/ErrorResponse.scala)
as their output error
type ([example](https://github.com/hyperledger-identus/cloud-agent/blob/eb898e068f768507d6979a5d9bab35ef7ad4a045/cloud-agent/service/server/src/main/scala/io/iohk/atala/connect/controller/ConnectionEndpoints.scala#L45))
This type ensures that the response returned to the user complies with
the [RFC-9457 Problem Details for HTTP APIs](https://www.rfc-editor.org/rfc/rfc9457.html).

```scala
object ConnectionEndpoints {

  val createConnection: Endpoint[
    (ApiKeyCredentials, JwtCredentials),
    (RequestContext, CreateConnectionRequest),
    ErrorResponse,
    Connection,
    Any
  ] = ???

}
```

#### Use "Failure => ErrorResponse" Implicit Conversion

If all the underlying services used by a controller comply with the above rules, then the only error type that could
propagate through the effect’s error channel is the
parent [`org.hyperledger.identus.shared.models.Failure`](https://github.com/hyperledger-identus/cloud-agent/blob/main/shared/src/main/scala/io/iohk/atala/shared/models/Failure.scala)
type and its conversion
to the ErrorResponse type is done automatically
via [Scala implicit conversion](https://github.com/hyperledger-identus/cloud-agent/blob/eb898e068f768507d6979a5d9bab35ef7ad4a045/cloud-agent/service/server/src/main/scala/io/iohk/atala/api/http/ErrorResponse.scala#L44).

#### Do not reflexively log errors

The upper layer will automatically do so appropriately and consistently using Tapir interceptor customization.
</file>

<file path="documentation/developers/cloud-agent/_category_.json">
{
    "label": "Cloud Agent",
    "position": 2,
    "collapsed": false,
    "collapsible": false,
    "link": {
      "type": "generated-index",
      "title": "Cloud Agent"
    }
  }
</file>

<file path="documentation/developers/cloud-agent/authentication.md">
# Authentication and Authorisation

Authentication is the process of verifying an entity's identity and the tenant associated with the entity.
Authorization verifies that an entity has the necessary permissions to access the requested information.
resource.

Authentication and authorization are critical to any identity management system, ensuring only authorized
users can access the system and its resources.

Cloud Agent provides the following authentication and authorization methods to ensure that the identity of the
entity is verified and authenticated during interactions with the platform:

- apikey authentication with internal IAM service
- admin-api-key authentication
- JWT token authentication and authorization with external IAM service Keycloak

## Default Entity and Wallet

The Cloud Agent uses the Default Entity and the Default Wallet for all interactions with the Agent over the REST API and DIDComm in the single-tenant mode.
A Default Entity is an entity with the id `00000000-0000-0000-0000-000000000000`, and a Default Wallet is a wallet with the id `00000000-0000-0000-0000-000000000000`.


## API Key Authentication

### Introduction

API Key Authentication is a straightforward method used to authenticate entities by utilizing a secret key. This method requires the inclusion of an `apikey` header in HTTP requests, with the value corresponding to the issued secret key. The configuration of API Key Authentication for an entity is managed by the Administrator using the Entity API methods.


### Security and Restrictions
- **API Key Length**: To maintain robust security, the length of the API Key value must exceed 16 bytes (128 bits). This length requirement is essential for enhancing the security of your API Key against potential attacks. The max length of the API Key value is limited to 128 bytes. Unique API Keys: Each API Key is unique to a specific entity. It cannot be shared or reused by other entities. If an attempt to assign the same API Key value to another entity, the API Key is considered compromised and must be considered unusable.
- **Unique API Keys:** Each API Key is unique to a specific entity. It cannot be shared or reused by other entities. If an attempt to assign the same APIKey value to another entity, the APIKey is considered compromised and must be considered unusable.
- **Revocation:** In case of a tenant's API Key revocation, it becomes invalid for authentication.


### Agent Responsibilities

The Agent manages API Keys for each tenant and maintains the security of the system:

- **API Key Storage:** The Agent maintains each tenant's APIKeys list. However, it is essential to note that the original value of the APIKey is not stored in the Agent, ensuring additional security.
- **Hashing and Authentication:** The Agent securely stores the hash of the APIKey in the database and uses it to authenticate the entity. The hashing process employs the `SHA-256` algorithm and a `salt` value to compute the hash value, ensuring data integrity and security during authentication. The length of the `salt` value must exceed 16 bytes (128 bits)



Based on the configuration API Key authentication, the Cloud Agent can support the following interaction models:

### Single Tenant without apikey authentication

Disable API key authentication and use the Default Wallet for all interactions with the Cloud Agent over the REST API and DIDComm.

| Environment Variable | Value |
|----------------------|-------|
| API_KEY_ENABLED      | false |



### Single Tenant with apikey authentication

Enable API key authentication and use the Default Wallet for all interactions with the Cloud Agent over the REST API and DIDComm. 

| Environment Variable                 | Value |
|--------------------------------------|-------|
| API_KEY_ENABLED                      | true  |
| API_KEY_AUTHENTICATE_AS_DEFAULT_USER | true  |

### Multi-Tenant with apikey authentication

Enable APIKey authentication and use the Entity and the Wallet associated with the API-Key for all interactions with the Cloud Agent. The Administrator must register the Entity and Wallet for the tenant before the first interaction with the Cloud Agent over the REST API.

| Environment Variable                 | Value |
|--------------------------------------|-------|
| API_KEY_ENABLED                      | true  |
| API_KEY_AUTHENTICATE_AS_DEFAULT_USER | false |


### Multi-Tenant with apikey authentication and auto-provisioning

Enable APIKey authentication and use the Wallet associated with the APIKey for all interactions with the Cloud Agent. Automatically register the tenant's Entity, Wallet, and API key during the first interaction with the Cloud Agent over the REST API.

| Environment Variable                 | Value |
|--------------------------------------|-------|
| API_KEY_ENABLED                      | true  |
| API_KEY_AUTHENTICATE_AS_DEFAULT_USER | false |
| AUTO_PROVISIONING_ENABLED            | true  |

## Admin-Api-Key Authentications

Admin-Api-Key authentication is an authentication method that uses a secret key to authenticate the Administrator. Adding the `x-admin-api-key` header to the HTTP request with the value of the issued secret is required. The value of the `x-admin-api-key` header gets configured in the Cloud Agent at the startup.

| Environment Variable | Value  |
|----------------------|--------|
| ADMIN_API_KEY        | secret |

The following REST APIs get protected by the Admin-Api-Key authentication methods:

- Wallet Management REST API
- Entity Management REST API


## JWT Token Authentication and Authorisation with the Keycloak

### Introduction

[Keycloak](https://www.keycloak.org/) is an open-source identity and access management solution that provides robust
authentication, authorization, and user management capabilities.
It allows organizations to secure their applications and services by centralizing user authentication and authorization
processes.
Keycloak supports various identity protocols, including OAuth 2.0 and OpenID Connect, making it a versatile solution for
securing modern web applications.

The Cloud Agent utilizes the following Keycloak features:

- Authentication with JWT Token with a configured flow according to
  the [ADR](https://staging-docs.atalaprism.io/adrs/adr/20230527-use-keycloak-and-jwt-tokens-for-authentication-and-authorisation-to-facilitate-multitenancy-in-cloud-agent/)
- Authentication with JWT Token
  using [token exchange](https://www.keycloak.org/docs/latest/securing_apps/index.html#_token-exchange)
- Authorization with `roles` claim supporting both Keycloak [RealmRole](https://www.keycloak.org/docs/latest/server_admin/#proc-creating-realm-roles_server_administration_guide) and [ClientRole](https://www.keycloak.org/docs/latest/server_admin/#con-client-roles_server_administration_guide)
- Authorization with JWT Token and RPT according
  to [authorization services](https://www.keycloak.org/docs/latest/authorization_services/index.html#authorization-services)
  based
  on [UMA 2.0 specification](https://docs.kantarainitiative.org/uma/wg/rec-oauth-uma-grant-2.0.html#:~:text=This%20specification%20defines%20a%20means,a%20resource%20owner%20authorizes%20access.)
  and [ADR](https://staging-docs.atalaprism.io/adrs/adr/20230926-use-keycloak-authorisation-service-for-managing-wallet-permissions/)

When the JWT Token authentication is enabled, it includes protection for all multi-tenant and administrative endpoints of the Cloud Agent.
Two mechanisms get utilized in JWT authorization:

1. __Role-based authorization__  
   It implements this [ADR](https://staging-docs.atalaprism.io/adrs/adr/20240103-use-jwt-claims-for-agent-admin-auth/), which can authorize both the administrator and tenant role.
   Each role is allowed to operate on different parts of the Agent
   Administrators are permitted to oversee the wallet management, while tenants are allowed to utilize the wallet and engage in SSI interactions.

2. __UMA resource permission__  
   When the role is tenant, the Agent must know which wallet the tenant can access.
The UMA permission model is employed to configure the tenant's permissions for the wallet.

### Sequence Diagrams

The following sequence diagrams illustrate the Cloud Agent and Keycloak interaction during the authentication and authorization processes.

#### User onboarding and configuration

```mermaid
sequenceDiagram
  participant Admin
  participant Keycloak
  participant CloudAgent

  Admin->>Keycloak: 1. Creates User
  Keycloak-->>Admin: 2. User Created

  Admin->>Keycloak: 3. Configures Authentication Flow
  Keycloak-->>Admin: 4. Authentication Flow Configured

  Admin->>CloudAgent: 5. Creates Wallet in Agent
  CloudAgent-->>Admin: 6. Sends Wallet ID

  Admin->>Keycloak: 7. Registers Wallet Resource
  Keycloak-->>Admin: 8. Resource Registered

  Admin->>CloudAgent: 9. Configures UMA Policy
  CloudAgent-->>Admin: 10. UMA Policy Configured
```

#### User Authentication

```mermaid
sequenceDiagram
  participant User
  participant Keycloak
  participant Agent

  User->>Keycloak: 1. Initiates Authentication
  Keycloak->>Keycloak: 2. Authenticates User
  Keycloak-->>User: 3. Sends JWT Token
  User->>Agent: 4. Requests API with JWT Token
  Agent->>Agent: 5. Validates JWT Token
  Agent->>Keycloak: 6. Requests RPT
  Keycloak->>Keycloak: 7. Validates JWT Token
  Keycloak-->>Agent: 8. Sends RPT
  Agent->>Agent: 9. Validates RPT
  Agent->>Agent: 10. Performs Operation
  Agent-->>User: 11. Returns Result
```

### Role-based authorization

The Agent has a simple mechanism for role-based authorization.
It expects the JWT to include a `roles` claim in the payload, identifying the role such as `admin` or `tenant`.
The user must belong to one of these roles but not both. If unspecified, the Agent infers the `tenant` role.
The location of the `roles` claim is also configurable, allowing flexible IAM configuration.
As a result, the Agent can support both Keycloak `RealmRoles` and `ClientRoles` based on the configuration.
Using the environment variable, set the path as: `KEYCLOAK_ROLES_CLAIM_PATH`.

If Keycloak `RealmrRole` is used, follow this [instruction](https://www.keycloak.org/docs/latest/server_admin/#proc-creating-realm-roles_server_administration_guide) on Keycloak
and the `KEYCLOAK_ROLES_CLAIM_PATH` should be set to `realm_access.roles`.

If Keycloak `ClientRole` is used, follow this [instruction](https://www.keycloak.org/docs/latest/server_admin/#con-client-roles_server_administration_guide) on Keycloak
and the `KEYCLOAK_ROLES_CLAIM_PATH` should be set to `resource_access.<KEYCLOAK_CLIENT_ID>.roles`.

### UMA permission configuration

The current authorization logic is built on the UMA specification, configuring permissions as user A controls resource W. In this model, user A is a registered user in Keycloak, and wallet W is a resource registered in Keycloak.

These configurations are managed manually in Keycloak. However, for a better experience, the Agent can execute these actions through the REST API endpoint. 
For each wallet, a corresponding resource gets created with the name wallet-`wallet_id` in Keycloak. 
Similarly, a corresponding policy is created for each user and resource with the name user `userId` on wallet `wallet_resource_name` permission.

The REST API endpoint to configure permissions in Keycloak is:

```
POST /wallets/{walletId}/uma-permissions
```

with a body containing subject as UUID.

Authenticated users (admin or the user owning the wallet) can configure permissions in Keycloak for the wallet.

### Migration from `apikey` authentication

The `apikey` authentication can work in parallel with the JWT authentication to call the REST API endpoints together with the JWT token.

The migration from API key authentication to JWT authentication is possible using the following steps:

- Create a user in Keycloak.
- Configure the authentication flow in Keycloak.
- Call the REST API endpoint to configure permissions in Keycloak with a valid API key token.
- Authenticate with Keycloak and obtain the JWT token.
- Call the REST API endpoints with a valid JWT token.

## Self-Service Wallet Registration

The Cloud Agent supports self-service wallet registration for the users.
When the user is registered in the Keycloak and has the valid JWT token, the user can register the wallet in the Cloud
Agent.
You can use the wallet management endpoints of the REST API for wallet registration:

```
POST /wallets 
{
    "seed": "secured-seed-value",
    "name": "wallet-name"
}
```

The user can register the wallet only for himself, so the `x-api-key` header is not required and the `seed` value is not
shared.

The corresponding wallet resource is created in Keycloak, and the user is granted access to this resource.
</file>

<file path="documentation/developers/cloud-agent/building-blocks.md">
---
sidebar_position: 1
--- 

# Building Blocks

Identus is a toolset for implementing decentralized identity and consists of several core building blocks. These building blocks are modular components that can be easily combined and configured to meet various use cases and product requirements. By abstracting these fundamental components into modular building blocks, Identus allows for rapid development and deployment of new features and capabilities for its [Cloud Agents](/home/concepts/glossary#cloud-agent).

This modular architecture also provides excellent flexibility and customization options, as different building blocks can be combined in various ways to fit specific use case requirements. However, in the future, some unique use cases may arise that require designing and developing new building blocks. The Atala team continuously improves and expands the suite's capabilities to meet its users' evolving needs.

Below is a brief overview of the current building blocks of Identus:

**Building Block**|**Definition**|**Code Name**
:-----:|:-----:|:-----:
Cryptography|A set of cryptographic primitives that ensure the integrity, authenticity, and confidentiality of all data we store and process in a provable secure way.|Apollo
[Decentralized Identifiers (DID)](/home/concepts/glossary#decentralized-identifier)|A set of DID operations to create, manage, and resolve standards-based DIDs in a user-controlled manner.|Castor
[Verifiable Credentials](/home/concepts/glossary#verifiable-credential)|A set of credential operations to issue, manage, and verify standards-based [verifiable credentials](/home/concepts/glossary#verifiable-credential) in a privacy-preserving manner.|Pollux
[DIDComm](/home/concepts/glossary#didcomm) V2|A set of secure, standards-based communications protocols to establish and manage trusted, peer-to-peer connections and interactions between DIDs in a transport-agnostic and interoperable manner.|Mercury

## Apollo - Cryptography Module
Apollo is one of the building blocks of Identus. It is a suite of cryptographic primitives ensuring data security during storage and handling. Cryptographic primitives are mathematical algorithms and protocols that form the foundation for secure communication and data protection.

The primary purpose of Apollo is to provide data integrity, authenticity, and confidentiality, which are essential components of secure information systems. Apollo ensures that tampering, unauthorized access, and other security threats do not compromise the protection of user data, giving users confidence in its security.

Apollo utilizes cryptographic hash functions to create digital fingerprints of data for detecting any changes or modifications to the original data. These functions ensure that the data stored and processed remains authentic and unaltered.

For authenticity, Apollo uses digital signatures to authenticate the identity of the sender and recipient of data and ensure that the data has not been tampered with during transmission.

Finally, for confidentiality, Apollo uses encryption algorithms to protect sensitive data from unauthorized access and exposure. This feature helps ensure that sensitive information remains confidential and protected against cyber-attacks and data breaches.

Overall, the Apollo building block is an essential component of Identus, providing a secure and reliable means of protecting data stored and processed within the platform.

## Castor - DID Module

Castor is a building block of Identus that provides a set of DID operations, allowing users to create, manage, and resolve DIDs.

A [DID Document](/home/concepts/glossary#did-document) is a JSON-LD document that contains information about a DID. It includes information such as the public keys associated with the DID, the authentication mechanisms, and linked services. Castor consists of a resolver that resolves a DID to its associated DID Document. The lookup service allows users to retrieve the DID Document for a given DID.

Castor supports a variety of [DID methods](/home/concepts/glossary#did-method), including full support of the in-house `did:prism` method and other widely-used methods such as the [Peer DID method](/home/concepts/glossary#peer-did-method), enabling users to choose the DID method that best suits their needs and allows for interoperability with other systems and platforms.

Castor gives users complete control over their DIDs, including creating new DIDs, updating existing ones, and deleting or deactivating them if necessary.

## Pollux - Verifiable Credential Module

Pollux is a building block of Identus that provides a set of operations for managing and verifying verifiable credentials (VCs). This building block is a vital aspect of the [self-sovereign identity](/home/concepts/glossary#self-sovereign-identity) paradigm. With Pollux, users can issue, manage, and verify VCs in a privacy-preserving manner. Pollux ensures that users can share their credentials with others while maintaining control over their personal information and protecting it from unauthorized access and exposure.

In terms of issuing credentials, Pollux provides the necessary functions to create and issue VCs, including the ability to specify the type of credential, the attributes it represents, and the trusted party attesting to the credentials.

For managing credentials, Pollux allows users to store and manage their VCs in a secure and easily accessible manner, which includes adding, updating, or revoking credentials as needed.

Finally, for verifying credentials, Pollux provides the tools to check a given credential's validity and authenticate that a trusted party has issued it. Pollux includes the ability to check the status of the credential, such as whether it has been revoked or not, and to validate that the attributes it represents are accurate and up-to-date.

## Mercury - DIDComm Module

Mercury is a building block of Identus that provides a set of secure, standards-based communication protocols and infrastructure for establishing and managing trusted, peer-to-peer connections between decentralized identifiers. DIDs allow their owners to manage their identity and interact with others in a privacy-preserving manner.

Additionally, using standards-based protocols ensures that the communication between DIDs is interoperable, meaning that different systems and platforms can communicate seamlessly.
</file>

<file path="documentation/developers/cloud-agent/did-management.md">
# DID management

## Managed PRISM DID

The [Cloud Agent](/home/concepts/glossary#cloud-agent) simplifies identity management by effectively handling PRISM [DIDs](/home/concepts/glossary#decentralized-identifier) for users. Relying on a Cloud Agent-managed DID, users entrust the Cloud Agent with tasks such as creating, updating, storing, and using the PRISM DID.

The Cloud Agent employs hierarchical deterministic key derivation to manage the keys of the PRISM DID. The Cloud Agent calculates and stores the derivation path whenever an activity involves new key information, such as creating a new DID or adding a new key. It internally tracks the DID counter and key counter to increment the derivation path accordingly. Details about the Cloud Agent constructs key material from the seed are on the HD-key derivation decision record.

## Managed Peer DID

The Cloud Agent also manages [Peer DIDs](/home/concepts/glossary#peer-did) for [DIDComm](/home/concepts/glossary#didcomm) activities. The Key materials for Peer DIDs are randomly generated and stored securely in [secret storage](/home/concepts/glossary#secrets-storage), allowing control of the Peer DID.

## Storing key material

### PRISM DID

PRISM DID management involves deriving key materials from a seed and derivation path. A corresponding derivation path is generated and stored in the database when creating a new DID or updating a new key. Only the derivation path, not the key material itself, is stored. The Cloud Agent can reconstruct and use key materials for PRISM DIDs by combining the stored derivation path with the provided seed at runtime.

The Cloud Agent itself does not store the wallet seed. However, the infrastructure hosting the Cloud Agent may choose to keep it based on operation requirements and security considerations.

### Peer DID
The Cloud Agent securely stores the key materials of peer DIDs using secret storage.
</file>

<file path="documentation/developers/cloud-agent/environment-variables.md">
---
title: Environment variables
sidebar_position: 2
--- 

Here is the table with the environment variables sorted in alphabetical order:

# Environment Variables

The following environment variables can be used to configure Identus Cloud Agent (in alphabetical order):

| Variable Name                              | Description                                                                                                                                                  | Type                  | Default                                       |
|--------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------|-----------------------------------------------|
| ADMIN_TOKEN                                | Admin token for the admin API key authentication method.                                                                                                     | String                | admin                                         |
| AGENT_DB_APP_PASSWORD                      | Agent database application user password for login.                                                                                                          | String                | password                                      |
| AGENT_DB_APP_USER                          | Agent database application user for login.                                                                                                                   | String                | agent-application-user                        |
| AGENT_DB_AWAIT_CONNECTION_THREADS          | Number of threads to wait for database connection.                                                                                                           | Int                   | 4                                             |
| AGENT_DB_HOST                              | Hostname of the server where the Cloud Agent database is running.                                                                                            | String                | localhost                                     |
| AGENT_DB_NAME                              | Database name where the agent database will store data.                                                                                                      | String                | agent                                         |
| AGENT_DB_PASSWORD                          | Agent database password for login.                                                                                                                           | String                | postgres                                      |
| AGENT_DB_PORT                              | Port of the Cloud Agent database.                                                                                                                            | Int                   | 5432                                          |
| AGENT_DB_USER                              | Agent database username for login.                                                                                                                           | String                | postgres                                      |
| AGENT_DIDCOMM_PORT                         | Port on which DIDComm service runs.                                                                                                                          | Int                   | 8090                                          |
| AGENT_HTTP_CLIENT_CONNECTION_POOL_SIZE     | Size of the HTTP client connection pool.                                                                                                                     | Int                   | 0                                             |
| AGENT_HTTP_CLIENT_CONNECTION_TIMEOUT       | HTTP client connection timeout duration.                                                                                                                     | String                | 5 seconds                                     |
| AGENT_HTTP_CLIENT_IDLE_TIMEOUT             | HTTP client idle timeout duration.                                                                                                                           | String                | 5 seconds                                     |
| AGENT_HTTP_PORT                            | Port on which Cloud Agent runs.                                                                                                                              | Int                   | 8085                                          |
| API_KEY_AUTHENTICATE_AS_DEFAULT_USER       | Whether or not to authenticate all API keys as the default user.                                                                                             | Boolean               | true                                          |
| API_KEY_AUTO_PROVISIONING                  | Whether or not to enable auto-provisioning for API keys and register the owner of the api-key automatically.                                                 | Boolean               | false                                         |
| API_KEY_ENABLED                            | Whether or not to enable API key authentication.                                                                                                             | Boolean               | false                                         |
| API_KEY_SALT                               | Salt used to hash the API key.                                                                                                                               | String                | JLXTS4J2qkMOgfO8                              |
| CONNECT_BG_JOB_RECURRENCE_DELAY            | Interval at which the background job will try to process connection records.                                                                                 | String                | 2 seconds                                     |
| CONNECT_BG_JOB_RECORDS_LIMIT               | Maximum number of connection records the background job will try to process at the same time.                                                                | Int                   | 25                                            |
| CONNECT_DB_APP_PASSWORD                    | Connect database application user password for login.                                                                                                        | String                | password                                      |
| CONNECT_DB_APP_USER                        | Connect database application user for login.                                                                                                                 | String                | connect-application-user                      |
| CONNECT_DB_AWAIT_CONNECTION_THREADS        | Number of threads to wait for database connection.                                                                                                           | Int                   | 4                                             |
| CONNECT_DB_HOST                            | Hostname of the server where the Connect database is running.                                                                                                | String                | localhost                                     |
| CONNECT_DB_NAME                            | Database name where the Connect database will store data.                                                                                                    | String                | connect                                       |
| CONNECT_DB_PASSWORD                        | Connect database password for login.                                                                                                                         | String                | postgres                                      |
| CONNECT_DB_PORT                            | Port of the Connect database.                                                                                                                                | Int                   | 5432                                          |
| CONNECT_DB_USER                            | Connect database username for login.                                                                                                                         | String                | postgres                                      |
| CONNECT_INVITATION_EXPIRY                  | The connect invitation expiry duration e.g 300 seconds. After which the OOB Connect Invitation will expire.                                                  | String                | 300 seconds                                   |
| CREDENTIAL_LEEWAY                          | Time leeway when verifying credential dates; if the time difference is less than the leeway, the credential will still be considered valid.                  | String                | 0 seconds                                     |
| CREDENTIAL_SD_JWT_EXPIRY                   | Expiry duration for SD-JWT credentials.                                                                                                                      | String                | 30 days                                       |
| CREDENTIAL_VERIFY_DATES                    | Whether or not to verify credential dates (expiration).                                                                                                      | Boolean               | false                                         |
| CREDENTIAL_VERIFY_SIGNATURE                | Whether or not to verify a credential signature.                                                                                                             | Boolean               | true                                          |
| DEFAULT_JWT_VC_OFFER_DOMAIN                | Default domain for JWT VC offers. Must be set to the value of the Cloud Agent endpoint.                                                                      | String                | default-domain                                |
| DEFAULT_KAFKA_ENABLED                      | Whether or not to enable Kafka integration.                                                                                                                  | Boolean               | false                                         |
| DEFAULT_WALLET_AUTH_API_KEY                | The authentication API key to be used for default entity that uses default wallet.                                                                           | String                | default                                       |
| DEFAULT_WALLET_ENABLED                     | Whether or not to initialize the default wallet.                                                                                                             | Boolean               | true                                          |
| DEFAULT_WALLET_SEED                        | The BIP32 wallet seed to be used for default wallet represented by a hexadecimal string.                                                                     | String                | Null                                          |
| DEFAULT_WALLET_WEBHOOK_API_KEY             | The optional API key (bearer token) to use as the Authorization header for the default wallet webhook.                                                       | String                | Null                                          |
| DEFAULT_WALLET_WEBHOOK_URL                 | The default wallet webhook endpoint URL to which notifications will be sent.                                                                                 | String                | Null                                          |
| DID_STATE_SYNC_TRIGGER_RECURRENCE_DELAY    | Triggering DID state sync delay in Hocon duration format.                                                                                                    | String                | 30 seconds                                    |
| DIDCOMM_SERVICE_URL                        | URL of the DIDComm server that also runs for this agent.                                                                                                     | String                | http://localhost:8090                         |
| ENABLE_ANONCRED                            | Enable support for the AnonCred credential type via API and DIDComm.                                                                                         | Boolean               | false                                         |
| GLOBAL_WEBHOOK_API_KEY                     | The optional API key (bearer token) to use as the Authorization header for the global wallet webhook.                                                        | String                | Null                                          |
| GLOBAL_WEBHOOK_URL                         | The global webhook endpoint URL to which notifications will be sent.                                                                                         | String                | Null                                          |
| ISSUANCE_INVITATION_EXPIRY                 | The issuance invitation expiry duration e.g 300 seconds. After which the OOB Credential Offer will expire.                                                   | String                | 300 seconds                                   |
| KEYCLOAK_CLIENT_ID                         | The Keycloak client ID.                                                                                                                                      | String                | `prism-agent`                                 |
| KEYCLOAK_CLIENT_SECRET                     | The Keycloak client secret.                                                                                                                                  | String                | `prism-agent-demo-secret`                     |
| KEYCLOAK_ENABLED                           | Whether or not to enable Keycloak authentication and authorisation.                                                                                          | Boolean               | false                                         |
| KEYCLOAK_REALM                             | The Keycloak realm name.                                                                                                                                     | String                | `atala-demo`                                  |
| KEYCLOAK_URL                               | The Keycloak server URL.                                                                                                                                     | String                | http://localhost:9980                         |
| KEYCLOAK_UMA_AUTO_UPGRADE_RPT              | Whether or not to enable automatic upgrade of RPT tokens. If disabled, `accessToken` must be RPT and include the permission claims.                          | Boolean               | true                                          |
| KEYCLOAK_ROLES_CLAIM_PATH                  | The json path to the `roles` claim in the JWT payload. Used for role-based authorization (e.g. admin or tenant).                                             | String                | `resource_access.<KEYCLOAK_CLIENT_ID>.roles`  |
| LOG_LEVEL                                  | Cloud Agent log level. The default log level is INFO. Possible values: `TRACE`, `DEBUG`, `INFO`, `WARN`, `ERROR`, `OFF`. Values are case-insensitive.        | String                | `INFO`                                        |
| POLLUX_DB_APP_PASSWORD                     | Pollux database application user password for login.                                                                                                         | String                | password                                      |
| POLLUX_DB_APP_USER                         | Pollux database application user for login.                                                                                                                  | String                | pollux-application-user                       |
| POLLUX_DB_AWAIT_CONNECTION_THREADS         | Number of threads to wait for database connection.                                                                                                           | Int                   | 4                                             |
| POLLUX_DB_HOST                             | Hostname of the server where the Pollux database is running.                                                                                                 | String                | localhost                                     |
| POLLUX_DB_NAME                             | Database name where the Pollux database will store data.                                                                                                     | String                | pollux                                        |
| POLLUX_DB_PASSWORD                         | Pollux database password for login.                                                                                                                          | String                | postgres                                      |
| POLLUX_DB_PORT                             | Port of the Pollux database.                                                                                                                                 | Int                   | 5432                                          |
| POLLUX_DB_USER                             | Pollux database username for login.                                                                                                                          | String                | postgres                                      |
| POLLUX_STATUS_LIST_REGISTRY_PUBLIC_URL     | URL of the status list registry used to verify the revocation of JWT credentials.                                                                            | String                | http://localhost:8085                         |
| PRESENTATION_INVITATION_EXPIRY             | The presentation invitation expiry duration e.g 300 seconds. After which the OOB Request Presentation will expire.                                           | String                | 300 seconds                                   |
| PRESENTATION_LEEWAY                        | Time leeway when verifying challenge dates.                                                                                                                  | String                | 0 seconds                                     |
| PRESENTATION_VERIFY_DATES                  | Whether or not to verify challenge dates during presentation.                                                                                                | Boolean               | false                                         |
| PRESENTATION_VERIFY_HOLDER_BINDING         | Whether or not to verify holder binding when verifying presentations (ensures the presenter is the holder).                                                  | Boolean               | false                                         |
| PRESENTATION_VERIFY_SIGNATURE              | Whether or not to verify the signature of a challenge used during credential presentation.                                                                   | Boolean               | true                                          |
| PRISM_NODE_HOST                            | Hostname of the server where the Prism Node is running.                                                                                                      | String                | localhost                                     |
| PRISM_NODE_PORT                            | Port of the Prism Node.                                                                                                                                      | Int                   | 50053                                         |
| PRISM_NODE_USE_PLAIN_TEXT                  | Whether or not to use plain text for Prism Node communication gRPC protocol.                                                                                 | Boolean               | true                                          |
| REST_SERVICE_URL                           | URL of the REST service.                                                                                                                                     | String                | https://host.docker.internal:8080/cloud-agent |
| SECRET_STORAGE_BACKEND                     | Secret storage backend for keys and credentials. If Vault is used, the Vault server must be running; otherwise, a database can be used for development purposes only. | Enum(vault, postgres) | vault                                         |
| STATUS_LIST_SYNC_TRIGGER_RECURRENCE_DELAY  | Triggering status list revocation sync for revoked credentials delay in Hocon duration format.                                                               | String                | 30 seconds                                    |
| VAULT_ADDR                                 | URL of the vault service for Cloud Agent to use for secret management.                                                                                       | String                | http://localhost:8200                         |
| VAULT_APPROLE_ROLE_ID                      | The `role_id` for HashiCorp Vault authentication with AppRole                                                                                                | String                | Null                                          |
| VAULT_APPROLE_SECRET_ID                    | The `secret_id` for HashiCorp Vault authentication with AppRole                                                                                              | String                | Null                                          |
| VAULT_TOKEN                                | Vault service auth token.                                                                                                                                    | String                | Null                                          |
| VAULT_USE_SEMANTIC_PATH                    | Enable full path convention for vault secret path.                                                                                                           | Boolean               | true                                          |
| WEBHOOK_PARALLELISM                        | Maximum number of events that will be retrieved in a single iteration from the event queue by the webhook publisher.                                         | Int                   | Null                                          |

## Hocon duration format

Hocon [duration format](https://github.com/lightbend/config/blob/main/HOCON.md#duration-format) is a string that
represents a duration of time.
It is used in the configuration file to specify the duration of time in seconds, minutes, hours, etc.

## `Null` default value

`Null` default value means that the variable is not set by default and must be set by the user based on the environment
configuration.
</file>

<file path="documentation/developers/cloud-agent/secrets-storage.md">
# Secrets Storage

## Introduction

To ensure maximum security, storing sensitive information, including secrets, actively and securely in the [Secrets Storage](</home/concepts/glossary#secrets-storage>). The Cloud Agent manages the following types of secrets:

- **seed**: a secret used to derive cryptographic keys
- **private key**: a secret used to sign and decrypt data
- any other sensitive data from entities' (for instance, credential definition and the link-secret used by the AnonCreds)

The default secret storage for the Cloud Agent is the [HashiCorp Vault Service](</home/concepts/glossary#vault-service>). Other implementations of secret storage can be implemented based on the needs.

## Technical Overview

### Secrets Engine Configuration

The Vault service uses a secrets engine to store secrets. KV2 secrets engine is used to keep secrets in the Vault service and provides the following features:

- encrypted at rest
- encrypted in transit
- versioned
- can be deleted, restored, and rolled back to a previous version
- available via REST API, WEB UI, and command client

### Secrets Storage Sequence Diagram

```mermaid
sequenceDiagram
    Application->>+Agent: REST API
    Agent->>+Wallet: route the request
    Wallet->>+Secrets Storage: get secrets
    Secrets Storage->>+Wallet: secrets
    Wallet->>+Wallet: business logic
    Wallet->>+Agent: route the response
    Agent->>+Application: REST API
```

### Naming Convention for the Vault Assets

Each asset is assigned a unique name to store the assets in the Vault service. The Vault is a key/value store with metadata attached to the key and versioning.

The naming convention for the Vault assets is a matter of implementation. For a multi-tenant configuration, the Wallet requires all the asset storage under the path containing the `wallet-id'.

As an example, the `seed` could be stored in the path below:

```
<wallet-id>/seed value=<base64-encoded-value> <metadata>
```

The following path can store the private keys for the DID:

```
<wallet-id>/dids/prism/<did-ref>/keys/<key-purpose>/<key-index>/<operation-hash> value=<base64-encoded-value> <metadata>
```

where:

- `wallet-id` is the unique identity of the Wallet
- `did-ref` is the DID ref
- `key-purpose` is the key purpose according to the PRISM DID Method specification
- `key-index` is the key index. Starting from 0 and increasing incrementally after each key rotation
- `operation-hash` is the reference to the updated DID document operation
- `base64-encoded-value` is the base64-encoded value of the key
- `metadata` is the key/value metadata attached to the key used to store additional information about the key, such as `seed` or `key-derivation-path`

The following path can store the key material of the DID peer:

```
<wallet-id>/dids/peer/<did-ref>/keys/<key-purpose> value=<base64-encoded-value> <metadata>
```

## Links:

- [HashiCorm Vault](https://www.vaultproject.io/)
- [Vault KV Secrets Engine](https://www.vaultproject.io/docs/secrets/kv/kv-v2)
</file>

<file path="documentation/developers/cloud-agent/troubleshooting&considerations.md">
# Troubleshooting & Considerations

## Docker Logging Management Considerations
When setting up a long-running environment using Docker Compose, it’s important to consider several factors to avoid issues such as excessive log file sizes leading to out-of-memory errors.

### Configuring Docker Compose for Effective Log Management
To ensure your Docker containers run smoothly and avoid problems related to excessive log file growth, configure log rotation in your docker-compose.yml file. This will help manage log file sizes and prevent out-of-memory errors caused by uncontrolled log growth.

### Log Rotation Example

Here’s a example in the mediator how to set up log rotation in your Docker Compose configuration:

1. Open your docker-compose.yml file.
2. Add or update the logging configuration under your service definition. For example:

```yaml
version: '3.8'

services:
  identus-mediator:
    image: ghcr.io/input-output-hk/atala-prism-mediator:0.14.2
    logging:
      driver: json-file
      options:
        max-size: "10m"
        max-file: "3"
```

- `driver`: Specifies the logging driver to use. The json-file driver is the default and supports log rotation options.
- `max-size`: Sets the maximum size of the log file before it gets rotated. In the example above, the log file will be rotated when it reaches 10 MB.
- `max-file`: Determines the number of rotated log files to keep. In this example, up to 3 log files will be kept before old files are deleted.

```shell
docker-compose up -d
```

3. Save the changes to your `docker-compose.yml` file and **Restart** your Docker containers to apply the new logging configuration.

### Configuring Docker Daemon for Effective Log Management

We should consider configuring the logging Options in the **Docker Daemon**. For a global logging configuration applicable to all Docker containers. We can modify the Docker daemon settings. This approach ensures consistent log management across all containers.

1. Edit the Docker daemon configuration file (usually located at /etc/docker/daemon.json). If the file doesn’t exist, you can create it.
2. Add or update the logging options in the `daemon.json` file:

```json
{
  "log-driver": "json-file",
  "log-opts": {
    "max-size": "10m",
    "max-file": "3"
  }
}
```
3. Restart the Docker daemon to apply the new settings:

```shell
sudo systemctl restart docker
```

#### Docker logging drivers

For more information see https://docs.docker.com/engine/logging/configure/#supported-logging-drivers
</file>

<file path="documentation/developers/cloud-agent/vdr.md">
# VDR

VDR is an interface for performing CRUD and verification operations on a generic data storage driver.
The specification is available [here](https://github.com/hyperledger-identus/vdr),
along with a reference implementation provided as a library.
The interface defines simple create, read, update, delete, and verify operations, delegating
their execution to an underlying driver. Driver is an actual implementation of a data storage mechanism.

## HTTP binding

Although the VDR implementation is available as a library, it is also integrated intothe Cloud Agent
to expose its functionality via HTTP, supporting use cases where direct library integration is not feasible.

The Cloud Agent exposes the VDR functionality through a RESTful stlye API,
providing an interface analogous to direct method calls.

__Example__

| Operation | HTTP Endpoint |
|-|-|
| `create(data, options)`      | `POST /vdr/entries?drid=...`  |
| `read(url, options)`         | `GET /vdr/entries?url=...`    |
| `update(url, data, options)` | `PUT /vdr/entries?url=...`    |
| `delete(url, options)`       | `DELETE /vdr/entries?url=...` |

## Selecting VDR drivers

The driver is a key component of VDR, providing the actual implementation for the storage backend.
The cloud agent acts as a proxy, supporting multiple drivers and allowing users to choose the one that best fits their needs.
To select the appropriate driver, specify the following parameters when creating a VDR entry.

| Parameters | Description |
|-|-|
| `drid` | Driver ID      |
| `drf`  | Driver famely  |
| `drv`  | Driver version |

Currently, the Cloud Agent supports the following drivers

| Driver | ID | Family | Version | Description |
|-|-|-|-|-|
| In-memory | `memory`   | `memory` | `0.1.0` | Driver storing data in-memory for testing |
| Database  | `database` | `database` | `0.1.0` | Driver storing data in local database testing purposes |

For a full range of parameters and driver options, please refer the the [VDR specification](https://github.com/hyperledger-identus/vdr).
</file>

<file path="documentation/developers/quick-start.md">
---
sidebar_position: 1
---

import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';

# Quick Start Guide
## Introduction into Self Sovereign Identity (SSI)
[Self-sovereign identity (SSI)](/home/concepts/glossary/#self-sovereign-identity) is complex. This Quick Start Guide explains the fundamental concepts to get up and running with Hyperledger Identus. This guide will familiarize you with the general concepts and how to create [decentralized identifiers (DIDs)](/home/concepts/glossary/#decentralized-identifier), issue credentials, make connections, and verify credentials with [verifiable presentations](/home/concepts/glossary/#verifiable-presentation). Refer to the Concepts and Components sections for a more in-depth explanation.

The trust triangle is the most basic process for conveying trust in the digital world. There are three roles in an SSI ecosystem: [Holders](/home/concepts/glossary/#holder), [Issuers](/home/concepts/glossary/#issuer), and [Verifiers](/home/concepts/glossary/#verifier).

![Component Diagram](/img/trust-triangle.png)

Holders can be any entity, such as individuals, organizations, and digital or physical things. They will hold [verifiable credentials (VCs)](/home/concepts/glossary/#verifiable-credential) and use a verifiable presentation to share their VCs.

Issuers can also be any entity that makes [claims](/home/concepts/glossary/#claim) about an [entity](/home/concepts/glossary/#entity). These claims are attestations, or evidence of something, about the Holder. As an example, an insurance company would provide proof of valid insurance.

Verifiers are the [relying party](/home/concepts/glossary/#relying-party) in the triangle. They will request information from the Holder, such as proof of insurance, and the Holder will use a verifiable presentation to share the appropriate VCs with the Verifier. The Holder's digital signature, the issuer DID get verified, and the contents therein to ensure nothing has been tampered with.



## Hyperledger Identus flow
The diagram details how the concepts fit alongside the Identus components in a typical SSI interaction.

![Component Diagram](/img/component-diagram.png)



## An overview of Hyperledger Identus components
Identus consists of core libraries that facilitate typical SSI interactions between [Issuers](/home/concepts/glossary/#issuer), [Holders](/home/concepts/glossary/#holder), and [Verifiers](/home/concepts/glossary/#verifier).


### A Cloud Agent
A Cloud Agent can issue, hold, and verify [verifiable credentials (VCs)](/home/concepts/glossary/#verifiable-credential) for any entity and manage [decentralized identifiers (DIDs)](/home/concepts/glossary/#decentralized-identifier) and DID-based connections. The Cloud Agent has an easy-to-use REST API to enable easy integration into any solution and uses [DIDComm V2](/home/concepts/glossary/#didcomm) as a messaging protocol for Cloud Agent-to-Cloud Agent communication.

It is maintained as an open source through the [Hyperledger Identus](https://www.hyperledger.org/projects/identus).

More in depth documentation about [Cloud Agent](/home/concepts/glossary/#cloud-agent) can be found [here](/home/identus/cloud-agent/overview).



### Wallet SDKs
[Wallet SDKs](/home/concepts/glossary/#wallet-sdk) for web and mobile (iOS, Android, TypeScript) enable identity holders to store credentials and respond to proof requests. They are typically used in applications that allow identity holders to interact with issuers and verifiers.

More in-depth documentation about the different Wallet SDKs can be found here ([TypeScript](https://hyperledger-identus.github.io/docs/sdk-ts/sdk/), [Swift](https://hyperledger-identus.github.io/sdk-swift/documentation/edgeagentsdk/), [KMP](https://hyperledger-identus.github.io/sdk-kmp/))



### Mediator
[Mediators](/home/concepts/glossary/#mediator) are for storing and relaying messages between Cloud Agents and Wallet SDKs. They act as a proxy that remains connected to the network and receives any message, credential, or proof request on behalf of the Wallet SDKs (which can be offline occasionally).

More in-depth documentation about Mediator can be found [here](/home/identus/mediator).

#### Node for a Verifiable Data Registry (VDR)
To issue and verify VCs to and from DIDs, we need a [Verifiable Data Registry (VDR)](/home/concepts/glossary/#verifiable-data-registry) that is globally resolvable and always on. In Identus's case, it is `prism-node`, [anchoring](/home/concepts/glossary/#anchoring) key information required for issuance and verification on the Distributed Ledger.



## Pre-Requisites


### Agent Deployment
This guide will demonstrate a single-tenant deployment with API Key authentication disabled and an in-memory ledger for published DID storage, which is the simplest configuration to get started as a developer. More advanced configuration options can be found in [Multi-Tenancy Management](/tutorials/multitenancy/tenant-onboarding) and associated [Environment Variables](/home/identus/cloud-agent/environment-variables) configuration options.

We develop on modern machines equipped with either Intel based x64 processors or Apple ARM processors with a minimum of four cores, 16 GB of memory and 128GB+ of SSD-type storage.

1. To spin up an Cloud Agent you must:
  * Have Git installed.
  * Have Docker installed.
  * Clone the [Identus Cloud Agent repository](https://github.com/hyperledger/identus-cloud-agent).

```bash
git clone https://github.com/hyperledger/identus-cloud-agent
```


2. Once cloned, create a new file named  __./identus-cloud-agent/infrastructure/local/.env-issuer__ to define the Issuer Agent environment variable configuration with the following content:


```
API_KEY_ENABLED=false
AGENT_VERSION=2.0.0
PRISM_NODE_VERSION=2.6.0
PORT=8000
NETWORK=identus
VAULT_DEV_ROOT_TOKEN_ID=root
PG_PORT=5432
```

3. Create a new file named  __./identus-cloud-agent/infrastructure/local/.env-verifier__ to define the Verifier Agent environment variable configuration with the following content:



```
API_KEY_ENABLED=false
AGENT_VERSION=2.0.0
PRISM_NODE_VERSION=2.6.0
PORT=9000
NETWORK=identus
VAULT_DEV_ROOT_TOKEN_ID=root
PG_PORT=5433
```

4. Setting the `API_KEY_ENABLED` to `false` disables the requirement of using API Keys.

:::caution

API_KEY_ENABLED disables API Key authentication. This should **not** be used beyond Development purposes.

:::

5. Start the `issuer` and `verifier` Cloud Agents by running the below commands in the terminal.



  * Issuer Cloud Agent: 
    
  Mac OSX  terminal shell
```bash
 ./infrastructure/local/run.sh -n issuer -b -e ./infrastructure/local/.env-issuer -p 8000 -d "$(ipconfig getifaddr $(route get default | grep interface | awk '{print $2}'))"
```
 Linux terminal shell
```bash
 ./infrastructure/local/run.sh -n issuer -b -e ./infrastructure/local/.env-issuer -p 8000 -d "$(ip addr show $(ip route show default | awk '/default/ {print $5}') | grep 'inet ' | awk '{print $2}' | cut -d/ -f1)"
```

  * The Issuer [API endpoint](http://localhost:8000/cloud-agent/) will be accessible on port 8000 `http://localhost:8000/cloud-agent/` with a [Swagger Interface](http://localhost:8000/cloud-agent/redoc) available at `http://localhost:8000/cloud-agent/redoc`.


  * Verifier Cloud Agent:

 For Mac OSX  terminal shell
```bash
 ./infrastructure/local/run.sh -n verifier -b -e ./infrastructure/local/.env-verifier -p 9000 -d "$(ipconfig getifaddr $(route get default | grep interface | awk '{print $2}'))"
```
 For Linux terminal shell
```bash
 ./infrastructure/local/run.sh -n verifier -b -e ./infrastructure/local/.env-verifier -p 9000 -d "$(ip addr show $(ip route show default | awk '/default/ {print $5}') | grep 'inet ' | awk '{print $2}' | cut -d/ -f1)"
```


  * The Verifier [API endpoint](http://localhost:9000/cloud-agent/) will be accessible on port 9000 `http://localhost:9000/cloud-agent/` with a [Swagger Interface](http://localhost:9000/cloud-agent/redoc) available at `http://localhost:9000/cloud-agent/redoc`.



### Agent configuration

#### Creating LongForm PrismDID
1. Run the following API request against your Issuer API to create a PRISM DID:

:::note

📌 **Note:** [To create DIDs with various supported curves](/tutorials/dids/create#2-create-the-cloud-agent-managed-did-using-did-registrar-endpoint).

:::

```bash
curl --location \
--request POST 'http://localhost:8000/cloud-agent/did-registrar/dids' \
--header 'Accept: application/json' \
--data-raw '{
    "documentTemplate": {
        "publicKeys": [
        {
          "id": "auth-1",
          "purpose": "authentication"
        },
        {
          "id": "issue-1",
              "purpose": "assertionMethod"
        }
      ],
      "services": []
    }
}'
```

2. Publish the DID by replacing `{didRef}` with the `longFormDid` output value from the previous step.

```bash
curl --location \
--request POST 'http://localhost:8000/cloud-agent/did-registrar/dids/{didRef}/publications' \
--header 'Accept: application/json'
```

3. The short version of the DID is the publishedPrismDID.

:::info

📖Learn more about PRISM DIDs and why it is necessary to publish specific DIDs [here](https://staging-docs.atalaprism.io/tutorials/dids/publish).

:::


#### Create a credential schema (JWT W3C Credential)

1. To create a [credential schema](/home/concepts/glossary/#credential-schema) on the Issuer API instance, run the following request:

:::info

Replace the `[[publishedPrismDID]]` in the example request with the `did` value from the previous step.

:::

2. We need to capture the schema's guid as its used in further steps.

```bash
curl -X 'POST' \
  'http://localhost:8000/cloud-agent/schema-registry/schemas' \
  -H 'accept: application/json' \
  -H 'Content-Type: application/json' \
  -d '{
  "name": "driving-license",
  "version": "1.0.0",
  "description": "Driving License Schema",
  "type": "https://w3c-ccg.github.io/vc-json-schemas/schema/2.0/schema.json",
  "author": [[publishedPrismDID]],
  "tags": [
    "driving",
    "license"
  ],
  "schema": {
    "$id": "https://example.com/driving-license-1.0.0",
    "$schema": "https://json-schema.org/draft/2020-12/schema",
    "description": "Driving License",
    "type": "object",
    "properties": {
      "emailAddress": {
        "type": "string",
        "format": "email"
      },
      "givenName": {
        "type": "string"
      },
      "familyName": {
        "type": "string"
      },
      "dateOfIssuance": {
        "type": "string",
        "format": "date-time"
      },
      "drivingLicenseID": {
        "type": "string"
      },
      "drivingClass": {
        "type": "integer"
      }
    },
    "required": [
      "emailAddress",
      "familyName",
      "dateOfIssuance",
      "drivingLicenseID",
      "drivingClass"
    ],
    "additionalProperties": true
  }
}'
```


### Starting Sample App
All wallet SDK's come bundled with a sample application, that cover all the Identus flows, including establishing connections, issuance, and verification flows.

<Tabs>
<TabItem value="js" label="Typescript Sample APP">

1. Clone the [TypeScript SDK](https://github.com/hyperledger-identus/sdk-ts) repository.

```bash
git clone -b v6.6.0 https://github.com/hyperledger-identus/sdk-ts
```

2. Ensure you have all applications installed for building the SDK and their dependencies

[rust](https://www.rust-lang.org/tools/install) and [wasm-pack](https://rustwasm.github.io/wasm-pack/installer/) are leveraged to build and use the AnonCreds and DIDComm Rust libraries within TypeScript. To build the SDK locally or run demonstration applications, you must have these applications installed.

The following should work Linux and MacOS. If you experience any issues, refer to the latest installation instructions for your platform.

```
curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh
curl https://rustwasm.github.io/wasm-pack/installer/init.sh -sSf | sh
```

3. Run the following:
  * Build the source SDK:

```bash
cd sdk-ts
git submodule update --init --recursive
npm i
npm run build
```

  * Start the React demo:
```bash
cd demos/next
npm i
npm run build
npm run start
```

  * This will start the React Wallet SDK TypeScript Demonstration at [http://localhost:3000](http://localhost:3000).

</TabItem>
<TabItem value="swift" label="Swift Sample APP">

1. Clone the [Swift SDK](https://github.com/hyperledger/identus-edge-agent-sdk-swift) repository.

```bash
git clone https://github.com/hyperledger/identus-edge-agent-sdk-swift
```

2. Open the XCode project on __./Sample/AtalaPrismWalletDemo/AtalaPrismWalletDemo.xcodeproj__
3. On the top left of the XCode window you will see a Play/Run button, click it.
4. The app will start.
5. Click Wallet Demo 2.0
  * You will be able to run the rest of the guide here.

</TabItem>
<TabItem value="android" label="Android Sample APP">

1. Clone the [KMM SDK](https://github.com/hyperledger/identus-edge-agent-sdk-kmm) repository.

```bash
git clone https://github.com/hyperledger/identus-edge-agent-sdk-kmm
```

2. Open the Wallet SDK project on IntelliJ or Android Studio.
3. In the `Run configuration` dropdown, select SampleApp.
4. Select the device or emulator you want to use.
5. Click "Run".
  * The SampleApp will launch on the applicable device or emulator.

</TabItem>
</Tabs>

### Deploy & Establish Mediation
Mediation is the process that ensures messages get routed and stored correctly between Issuers, Verifiers and Holders, even if they are offline. The mediator offers a service that is always running and can securely store messages and deliver them to the associated DIDs using DIDComm. This enables use-cases where connectivity to a (mobile) wallet cannot be guaranteed.

#### Preparation
1. To get the mediator deployed locally for the demo, clone the [Mediator repository](https://github.com/hyperledger/identus-mediator).

```bash
git clone https://github.com/hyperledger/identus-mediator
```

2. With a Docker service running, open a new terminal and run:

:::info

The latest mediator version can be found at [Mediator releases](https://github.com/hyperledger/identus-mediator/releases). Change the version in the example if you want to use the latest version.

:::

  Mac OSX  terminal shell
```bash
MEDIATOR_VERSION=1.1.0 SERVICE_ENDPOINTS="http://$(ipconfig getifaddr $(route get default | grep interface | awk '{print $2}')):8080;ws://$(ipconfig getifaddr $(route get default | grep interface | awk '{print $2}')):8080/ws" docker compose up
```
 Linux terminal shell
```bash
MEDIATOR_VERSION=1.1.0 SERVICE_ENDPOINTS="http://$(ip addr show $(ip route show default | awk '/default/ {print $5}') | grep 'inet ' | awk '{print $2}' | cut -d/ -f1):8080;ws://$(ip addr show $(ip route show default | awk '/default/ {print $5}') | grep 'inet ' | awk '{print $2}' | cut -d/ -f1):8080/ws" docker compose up
```

`MEDIATOR_ENDPOINTS` is then set to your local IP address:8080.

3. More advanced documentation and configuration options can be found [here](https://github.com/hyperledger/identus-mediator).



4. Now you need to capture the Mediator's [Peer DID](/home/concepts/glossary/#peer-did) in order to start DIDCOMM V2 Mediation protocol, you can do so by opening you browser at the mediators [endpoint](/home/concepts/glossary/#endpoints).

#### Demo application

:::info

In order to complete this step you'll need to connect to the mediators Peer DID which you can fetch by making the following API request.

```bash
curl --location \
--request GET 'localhost:8080/invitation' \
--header 'Content-Type: application/json'
```

:::

Follow the steps in your desired platform as stated below:

<Tabs>
<TabItem value="js" label="Typescript Sample APP">

1. Open http://localhost:3000/debug in your browser, 
1. paste the mediator peer DID (obtained from the `from` attribute after fetching from the mediator's invitation endpoint),
1. click **Edge Agent** tab in the bottom left,
1. click **Connect** button,
1. click **Start** button.

</TabItem>

<TabItem value="swift" label="Swift Sample APP">

1. In the app, go to Wallet Demo, and on the Mediator tab, insert the mediator DID.

</TabItem>
<TabItem value="android" label="Android  Sample APP">

1. Go back to the Sample app. In the main screen, you can provide the mediator DID of your choice or use what is there already. Proceed and click **Start** after.

</TabItem>
</Tabs>



2. If you are running the SampleApp, click the **Start Agent** button.

The below code examples show how to establish mediation when building your own application.

<summary>Code examples</summary>
3. The following code examples represent establishing mediation and instantiating the Cloud Agent.

<Tabs>
<TabItem value="js" label="Typescript">


```typescript
  const mediatorDID = SDK.Domain.DID.fromString(
    [[MEDIATOR DID PEER]]
  );
  const api = new SDK.ApiImpl();
  const apollo = new SDK.Apollo();
  const castor = new SDK.Castor(apollo);
  const didcomm = new SDK.DIDCommWrapper(apollo, castor, pluto);
  const mercury = new SDK.Mercury(castor, didcomm, api);
  const store = new SDK.PublicMediatorStore(pluto);
  const handler = new SDK.BasicMediatorHandler(mediatorDID, mercury, store);
  const manager = new SDK.ConnectionsManager(castor, mercury, pluto, handler);
  const seed = apollo.createRandomSeed()
  const agent = new SDK.Agent(
    apollo,
    castor,
    pluto,
    mercury,
    handler,
    manager,
    seed.seed
  );
   /**
   * This internally will attempt to load an existing mediator from the
   * database. If it does not exist it will try to achieve mediation
   * automatically, by creating a PeerDID and sending a MediationRequest.
   * After this step the mediator starts capturing messages for the PeerDID we specied.
   */
  await agent.start()
```

</TabItem>

<TabItem value="swift" label="Swift">


```swift
  let agent = CloudAgent(mediatorDID: did)
  try await agent.start()
  agent.startFetchingMessages()
```

</TabItem>
<TabItem value="android" label="Android">


```kotlin
val apollo = ApolloImpl()
val castor = CastorImpl(apollo)
val pluto = Pluto(DbConnection())
(pluto as PlutoImpl).start(context)
val mercury = mercury = MercuryImpl(
    castor,
    DIDCommWrapper(castor, pluto, apollo),
    ApiImpl(httpClient())
)
val pollux = PolluxImpl(castor)
val seed = apollo.createRandomSeed()
val handler = BasicMediatorHandler(
  mediatorDID = DID(<DID_STRING>),
  mercury = mercury,
  store = BasicMediatorHandler.PlutoMediatorRepositoryImpl(pluto)
)
agent = CloudAgent(
    apollo = apollo,
    castor = castor,
    pluto = pluto,
    mercury = mercury,
    pollux = pollux,
    seed = seed,
    mediatorHandler = handler
)
agent.start()
agent.startFetchingMessages()

```

</TabItem>
</Tabs>

## Establish Holder connections
To connect the Holder to both Cloud Agent instances, you must run this in both Issuer and Verifier endpoints.

### Establish a connection - Agent side
A connection must be established between the Holder and Cloud Agents to correctly deliver the Issuance + Verification Messages to the Holder.

#### Establish connection on the Issuer Cloud Agent
```bash
curl --location \
--request POST 'http://localhost:8000/cloud-agent/connections' \
--header 'Content-Type: application/json' \
--data-raw '{
    "label": "Cloud Agent demo connection with holder"
}'
```

1. This request will return a JSON response with an invitation and its URL. The Issuer Cloud Agent would share this URL as a QR code, and the holder would scan it with the wallet app.

  * Copy the `invitationUrl` and the `connectionId`.

#### Establish connection on the Verifier Cloud Agent
```bash
curl --location \
--request POST 'http://localhost:9000/cloud-agent/connections' \
--header 'Content-Type: application/json' \
--data-raw '{
    "label": "Cloud Agent demo connection with holder"
}'
```

2. This request will return a JSON response with an invitation and its URL. The Verifier Cloud Agent would share this URL as a QR code, and the holder would scan it with the wallet app.

  * Copy the `invitationUrl` and the `connectionId`.

### Establish a connection - Holder side
3. Now that you have the invitation, it's time for the Holder to accept it.

#### Demo application
<Tabs>
<TabItem value="js" label="Typescript Sample APP">

4. Open a browser at localhost:3000.
5. Start the Edge Agent by clicking the button.
6. Paste the invitation URL generated in the previous step into the `CloudAgent` connection section and click on Create Connection.
  * The application will react when the connection gets established correctly and show a new connection.


</TabItem>
<TabItem value="swift" label="Swift Sample APP">

4. On the Out of Bounds (OOB) dialog, paste the invitation URL we generated into the `CloudAgent` connection section and click **Validate**.
  * The application will respond once the connection gets established correctly and show a message under messages.

</TabItem>
<TabItem value="android" label="Android Sample APP">

4. Go back to the Application:
5. Click the floating button at the bottom right corner of the Contacts tab.
6. On the dialog, paste the invitation URL we generated into the `CloudAgent` connection section and click **Validate**.
  * The application will react once the connection gets established correctly and show a message under messages.


</TabItem>
</Tabs>

<summary>Code examples</summary>
<Tabs>
<TabItem value="js" label="Typescript">

```js
const parsed = await props.agent.parseOOBInvitation(new URL([[OOB URL]]));
await props.agent.acceptDIDCommInvitation(parsed);
```

</TabItem>
<TabItem value="swift" label="Swift">

```swift
  let message = try agent.parseOOBInvitation(url: oobUrl)
  try await agent.acceptDIDCommInvitation(invitation: message)
```

</TabItem>
<TabItem value="android" label="Android">

```kotlin
val invitation = agent.parseInvitation(oobUrl)
agent.acceptOutOfBandInvitation(invitation)
```

</TabItem>
</Tabs>

## Issue a Credential from the Issuer to the holder

The credential issuance flow consists of multiple steps, detailed in this section. It starts with the Issuer sending a [Credential Offer](/home/concepts/glossary/#credential-offer) to the Holder, which would accept or reject this invitation and create a `credentialRequest` from it. The [credential request](/home/concepts/glossary/#credential-request) gets sent through DIDComm to the Issuer, issuing and sending the credential back to the Holder.

The Issuer can create a credential offer in two ways:
1. As a direct credential offer DIDComm message for a holder with an existing connection
2. As an credential offer as attachment in an OOB invitation message for connectionless issuance

<Tabs>
<TabItem value="existing" label="With Existing Connection">
### Create a Credential Offer with an existing connection **Issuer Agent**

1. To trigger the creation of a credential-offer, we call the credential-offers-endpoint, as follows:

:::info

Please replace the following variables in the example request before sending:

- `connectionId`: The ID of the connection previously established between agent and holder. This is part of the response of the POST message from the agent when calling the `cloud-agent/connections` endpoint. It is returned in the `connectionId` attribute. There is a unique connection ID for the relationship between issuer and holder and verifier and holder. In this example, please use the `connectionId` returned when creating the connection between issuer and holder
- `publishedPrismDID`: The short form of the PRISM DID created when setting up the Issuer agent

The `connectionId` is just the ID of the connection we previously established with the issuer.

The Issuing DID is the published PRISM DID in its short version which was also used to create and publish the credential schema.

- ``

:::

```bash
curl --location --request POST 'http://localhost:8000/cloud-agent/issue-credentials/credential-offers' \
--header 'Content-Type: application/json' \
--data-raw '{
    "claims": {
      "emailAddress":"sampleEmail",
      "familyName":"Alice",
      "dateOfIssuance":"2023-01-01T02:02:02Z",
      "drivingLicenseID":"42",
      "drivingClass":1
    },
    "connectionId": [[connectionId]],
    "issuingDID": [[publishedPrismDID]],
    "schemaId": [[schemaId]],
    "automaticIssuance": true
}'
```
</TabItem>
<TabItem value="connectionless" label="Connectionless Issuance">
### Create a Credential Offer as Invitation for connectionless issuance **Issuer Agent**

1. To trigger the creation of a credential-offer, we call the credential-offers-invitation-endpoint, as follows:

:::info

Please replace the following variables in the example request before sending:

- `goalCode`: OPTIONAL A self-attested code the receiver may want to display to the user or use in automatically deciding what to do with the out-of-band message,
- `goal`: OPTIONAL. A self-attested string that the receiver may want to display to the user about the context-specific goal of the out-of-band message.
- `publishedPrismDID`: The short form of the PRISM DID created when setting up the Issuer agent

The Issuing DID is the published PRISM DID in its short version, which was also used to create and publish the credential schema.

:::

```bash
curl --location --request POST 'http://localhost:8000/cloud-agent/issue-credentials/credential-offers/invitation' \
--header 'Content-Type: application/json' \
--data-raw '{
    "claims": {
      "emailAddress":"sampleEmail",
      "familyName":"Alice",
      "dateOfIssuance":"2023-01-01T02:02:02Z",
      "drivingLicenseID":"42",
      "drivingClass":1
    },
    "goalCode": [[goalCode]],
    "goal": [[goal]],
    "credentialFormat": "JWT",
    "issuingDID": [[publishedPrismDID]],
    "schemaId": [[schemaId]],
    "automaticIssuance": true
}'
```


### Accept Credential Offer Invitation for connectionless issuance **Holder**

For connectionless issuance, the Holder needs to accept the invitation containing the credential offer. This step is necessary before creating the Credential Request.
#### Demo application
<Tabs>
<TabItem value="js" label="Typescript Sample APP">

1. In the browser at localhost:3000, navigate to the "Credential Offer" section.
2. Paste the invitation URL received from the Issuer into the provided input field.
3. Click on "Accept Invitation" to process the credential offer.

</TabItem>
<TabItem value="swift" label="Swift Sample APP">

1. In the Swift mobile app, go to the "Credential Offer" section.
2. Enter the invitation URL received from the Issuer.
3. Tap on "Accept Invitation" to process the credential offer.

</TabItem>
<TabItem value="android" label="Android Sample APP">

1. In the Android mobile app, navigate to the "Credential Offer" section.
2. Input the invitation URL provided by the Issuer.
3. Tap "Accept Invitation" to process the credential offer.

</TabItem>
</Tabs>

<summary>Code examples</summary>
<Tabs>
<TabItem value="js" label="Typescript">

```js
const parsed = await props.agent.parseOOBInvitation(new URL([[OOB URL]]));
await props.agent.acceptDIDCommInvitation(parsed);
```

</TabItem>
<TabItem value="swift" label="Swift">

```swift
  let message = try agent.parseOOBInvitation(url: oobUrl)
  try await agent.acceptDIDCommInvitation(invitation: message)
```

</TabItem>
<TabItem value="android" label="Android">

```kotlin
val invitation = agent.parseInvitation(oobUrl)
agent.acceptOutOfBandInvitation(invitation)
```

</TabItem>
</Tabs>

</TabItem>
</Tabs>
### Create CredentialRequest from CredentialOffer **Holder**

2. Because this credential Offer was created with the `automaticIssuance` true, as soon as the `CloudAgent` receives this `credentialRequest` it will respond with the `IssuedCredential` message and send this back to the holder.

:::info

automaticIssuance is optional. It can also be manually triggered and confirmed by the Holder.```

:::


#### Demo application
3. The holder will at some point receive a `CredentialOffer`, which the holder must accept, and then, a `CredentialRequest` is created and sent back to the Issuer through DIDComm V2 protocols.

<Tabs>
<TabItem value="js" label="Typescript Sample APP">
4. The <code>CredentialOffer</code> message will be automatically accepted as soon as it reaches the browser. In exchange, a <code>CredentialRequest</code> message will get sent back to the `CloudAgent.`

</TabItem>
<TabItem value="swift" label="Swift Sample APP">
4. As soon as the <code>CredentialOffer</code> message reaches the Swift mobile app, it will display to the user to accept or reject, and in exchange, a <code>CredentialRequest</code> message will get sent back to the <code>CloudAgent</code>.

</TabItem>
<TabItem value="android" label="Android Sample APP">
4. As soon as the <code>CredentialOffer</code> message reaches the Android mobile app, it will be automatically accepted, and in exchange, a <code>CredentialRequest</code> message will get sent back to the <code>CloudAgent</code>.

</TabItem>
</Tabs>


<summary>Code examples</summary>

5. The exchange between CredentialOffer and CredentialRequest is demonstrated through more advanced code samples below, showcasing how different platforms handle it.

<Tabs>
<TabItem value="js" label="Typescript">

```typescript
props.agent.addListener(ListenerKey.MESSAGE,async (newMessages:SDK.Domain.Message[]) => {
    //newMessages can contain any didcomm message that is received, including
    //Credential Offers, Issued credentials and Request Presentation Messages
    const credentialOffers = newMessages.filter((message) => message.piuri === "https://didcomm.org/issue-credential/2.0/offer-credential");

    if (credentialOffers.length) {
        for(const credentialOfferMessage of credentialOffers) {
        const credentialOffer = OfferCredential.fromMessage(credentialOfferMessage);
        const requestCredential = await props.agent.prepareRequestCredentialWithIssuer(credentialOffer);
        try {
            await props.agent.sendMessage(requestCredential.makeMessage())
        } catch (err) {
            console.log("continue after err", err)
        }
        }
    }
})
```

</TabItem>
<TabItem value="swift" label="Swift">

```swift
agent
  .handleMessagesEvents()
  .sink(receiveCompletion: { _ in }, receiveValue: { [weak self] in
    guard
        let message,
        message.direction == .received,
        let msgType = ProtocolTypes(rawValue: message.piuri)
      else { return }

      Task.detached { [weak self] in
          do {
            switch msgType {
             case .didcommOfferCredential:
                let newPrismDID = try await agent.createNewPrismDID()
                guard let requestCredential = try await agent.prepareRequestCredentialWithIssuer(
                    did: newPrismDID,
                    offer: try OfferCredential(fromMessage: message)
                ) else { throw UnknownError.somethingWentWrongError() }

                _ = try await agent.sendMessage(message: try requestCredential.makeMessage())
          } catch {}
      }
  })

```

</TabItem>
<TabItem value="android" label="Android">

```kotlin
agent.handleReceivedMessagesEvents().collect { list ->
    list.forEach { message ->
        if (message.piuri == ProtocolType.DidcommOfferCredential.value) {
            val credentials = pluto.getAllCredentials().first()
            if (credentials.isEmpty()) {
                val offer = OfferCredential.fromMessage(message)
                val subjectDID = agent.createNewPrismDID()
                val request =
                agent.prepareRequestCredentialWithIssuer(
                    subjectDID,
                    offer
                )
                mercury.sendMessage(request.makeMessage())
            }
        }
    }
}
```

</TabItem>
</Tabs>

### Store the Issued Credential [Holder]
:::caution

The sample application are using an insecure storage solution which should only be used for testing purposes and not production environments!

:::

<summary>Code examples</summary>
6. Once the Holder receives a credential from the Cloud Agent, it needs to store the credential somewhere:

<Tabs>
<TabItem value="js" label="Typescript">



```typescript
props.agent.addListener(ListenerKey.MESSAGE,async (newMessages:SDK.Domain.Message[]) => {
    //newMessages can contain any didcomm message that is received, including
    //Credential Offers, Issued credentials and Request Presentation Messages
    const issuedCredentials = newMessages.filter((message) => message.piuri === "https://didcomm.org/issue-credential/2.0/issue-credential");
    if (issuedCredentials.length) {
        for(const issuedCredential of issuedCredentials) {
            const issueCredential = IssueCredential.fromMessage(issuedCredential);
            await props.agent.processIssuedCredentialMessage(issueCredential);
        }
    }
})
```

</TabItem>
<TabItem value="swift" label="Swift">

```swift
agent
  .handleMessagesEvents()
  .sink(receiveCompletion: { _ in }, receiveValue: { [weak self] in
    guard
        let message = $0
        message.direction == .received,
        let msgType = ProtocolTypes(rawValue: message.piuri)
      else { return }

      Task.detached { [weak self] in
          do {
            switch msgType {
              case .didcommIssueCredential:
                  let issueCredential = try IssueCredential(fromMessage: message)
                  _ = try await agent.processIssuedCredentialMessage(message: issueCredential)
            }
          } catch {}
      }
  })
```

</TabItem>
<TabItem value="android" label="Android">


```kotlin
agent.handleReceivedMessagesEvents().collect { list ->
    list.forEach { message ->
        if (message.piuri == ProtocolType.DidcommIssueCredential.value) {
                agent.processIssuedCredentialMessage(
                    IssueCredential.fromMessage(
                    message
                )
            )
        }
    }
}
```

</TabItem>
</Tabs>

## Request a verification from the Verifier Cloud Agent to the Holder (JWT W3C Credential)
Now that the Holder has received a credential, it can be used in a verification workflow between a Holder and a Verifier. This requires the following steps:

1. Verifier creates a proof request
2. Holder receives the proof request
3. Holder creates a proof presentation and shares this with the verifier
4. Verifier verifies the proof presentation

:::info

In the example, we demonstrate two verification flows:

1. Verification with an established connection between the Holder and the Verifier.
2. Connectionless verification in which the Holder and Verifier do not have a pre-established connection.
:::


### Verifier Agent
<Tabs>
<TabItem value="existing" label="With Existing Connection">

5. To run this section, we will use [the connection](/home/quick-start#establish-connection-on-the-verifier-cloud-agent) we created between the Holder and the Verifier.

```bash
curl --location \
--request POST 'http://localhost:9000/cloud-agent/present-proof/presentations' \
--header 'Content-Type: application/json' \
--data-raw '{
    "connectionId": [[connectionId]],
    "proofs": [
        {
            "schemaId": [[schemaId]],
            "trustIssuers": [
                [[PUBLISHED PRISM DID FROM THE ISSUER]]
            ]
        }
    ],
    "options": {
        "challenge": "A challenge for the holder to sign",
        "domain": "domain.com"
    }
}'
```

  * This API request will return a `presentationRequestId,` which the verifier can use later to check the current status  of the request.

</TabItem>
<TabItem value="connectionless" label="Connectionless Request Presentation"> 

5. To run this section, we'll use the presentation invitation endpoint to create a request presentation invitation, which the holder can scan to receive the invitation or the verifier can share directly.

```bash
curl --location \
--request POST 'http://localhost:9000/cloud-agent/present-proof/presentations/invitation' \
--header 'Content-Type: application/json' \
--data-raw '{
    "goalCode": [[goalCode]],
    "goal": [[goal]],
    "credentialFormat": "JWT",
    "proofs": [
        {
            "schemaId": [[schemaId]],
            "trustIssuers": [
                [[PUBLISHED PRISM DID FROM THE ISSUER]]
            ]
        }
    ],
    "options": {
        "challenge": "A challenge for the holder to sign",
        "domain": "domain.com"
    }
}'
```

  * This API request will return an `invitationId` along with an Out-Of-Band (OOB) message. The OOB message includes a Request Presentation in JSON format as an attachment and is encoded as a base64 URL-encoded string, which can be shared with the holder.

### Accept Request Presentation invitation for connectionless verification **Holder**

For connectionless verification, the Holder needs to accept the invitation containing the Request Presentation.
#### Demo application
<Tabs>
<TabItem value="js" label="Typescript Sample APP">

1. In the browser at localhost:3000, navigate to the "Request Presentation" section.
2. Paste the invitation URL received from the Issuer into the provided input field.
3. Click on "Accept Invitation" to process the request presentation.

</TabItem>
<TabItem value="swift" label="Swift Sample APP">

1. In the Swift mobile app, go to the "Request Presentation" section.
2. Enter the invitation URL received from the Issuer.
3. Tap on "Accept Invitation" to process the request presentation.

</TabItem>
<TabItem value="android" label="Android Sample APP">

1. In the Android mobile app, navigate to the "Request Presentation" section.
2. Input the invitation URL provided by the Issuer.
3. Tap "Accept Invitation" to process the request presentation.

</TabItem>
</Tabs>

<summary>Code examples</summary>
<Tabs>
<TabItem value="js" label="Typescript">

```js
const parsed = await props.agent.parseOOBInvitation(new URL([[OOB URL]]));
await props.agent.acceptDIDCommInvitation(parsed);
```

</TabItem>
<TabItem value="swift" label="Swift">

```swift
  let message = try agent.parseOOBInvitation(url: oobUrl)
  try await agent.acceptDIDCommInvitation(invitation: message)
```

</TabItem>
<TabItem value="android" label="Android">

```kotlin
val invitation = agent.parseInvitation(oobUrl)
agent.acceptOutOfBandInvitation(invitation)
```

</TabItem>
</Tabs>

</TabItem>
</Tabs>



### Holder: Receives the Presentation proof request

6. The Holder needs an Edge Agent running with the message listener active. It will receive the presentation proof request from the Verifier Cloud Agent for the correct type of messages as detailed below:

<summary>Code examples</summary>

<Tabs>
<TabItem value="js" label="Typescript">

```typescript
props.agent.addListener(ListenerKey.MESSAGE,async (newMessages:SDK.Domain.Message[]) => {
    //newMessages can contain any didcomm message that is received, including
    //Credential Offers, Issued credentials and Request Presentation Messages
    const requestPresentations = newMessages.filter((message) => message.piuri === "https://didcomm.atalaprism.io/present-proof/3.0/request-presentation");

    if (requestPresentations.length) {
        for(const requestPresentation of requestPresentations) {
            const lastCredentials = await props.pluto.getAllCredentials();
            const lastCredential = lastCredentials.at(-1);
            const requestPresentationMessage = RequestPresentation.fromMessage(requestPresentation);
            try {
            if (lastCredential === undefined) throw new Error("last credential not found");

            const presentation = await props.agent.createPresentationForRequestProof(requestPresentationMessage, lastCredential)
            await props.agent.sendMessage(presentation.makeMessage())
            } catch (err) {
            console.log("continue after err", err)
            }
        }
    }
})
```

</TabItem>
<TabItem value="swift" label="Swift">

```swift
agent
  .handleMessagesEvents()
  .sink(receiveCompletion: { _ in }, receiveValue: { [weak self] in
    guard
          let message,
          message.direction == .received,
          let msgType = ProtocolTypes(rawValue: message.piuri)
      else { return }

      Task.detached { [weak self] in
        do {
          switch msgType {
          case .didcommRequestPresentation:
              let credential = try await agent.verifiableCredentials().map { $0.first }.first().await()
              guard let credential else {
                  throw UnknownError.somethingWentWrongError()
              }
              let presentation = try await agent.createPresentationForRequestProof(
                  request: try RequestPresentation(fromMessage: message),
                  credential: credential
              )
              _ = try await agent.sendMessage(message: try presentation.makeMessage())
          }
        } catch {}
      }
  })
```

</TabItem>

<TabItem value="android" label="Android">

```kotlin
agent.handleReceivedMessagesEvents().collect { list ->
    list.forEach { message ->
        if (message.piuri == ProtocolType.DidcommRequestPresentation.value && !presentationDone) {
            viewModelScope.launch {
                presentationDone = true
                agent.getAllCredentials().collect {
                    val credential = it.first()
                    val presentation = agent.preparePresentationForRequestProof(
                        RequestPresentation.fromMessage(message),
                        credential
                    )
                    mercury.sendMessage(presentation.makeMessage())
                }
            }
        }
    }
}
```

</TabItem>

</Tabs>

### Verifier: Will then check on the API if the Presentation Request has been completed or not.

```bash
curl --location \
--request GET 'http://localhost:9000/cloud-agent/present-proof/presentations/[[presentationRequestId]]' \
--header 'Accept: application/json'
```

7. The response body establishes the completion of the request and can be verified for correctness.
</file>

<file path="documentation/learn/components/cloud-agent/_category_.json">
{
    "label": "Cloud Agent",
    "collapsed": true,
    "link": {
      "type": "doc",
      "id": "README"
    }
  }
</file>

<file path="documentation/learn/components/cloud-agent/multi-tenancy.md">
# Multi-Tenancy

## Introduction

### Purpose

Multi-tenancy is a fundamental capability of the Identus toolset designed to provide secure and efficient identity and credential management for diverse users, identities, and organizations. It enables the platform to serve multiple tenants while ensuring the logical isolation of their assets.

### Key Benefits

- **Scalability**: Identus' multi-tenancy support allows the platform to scale effortlessly to accommodate growing users, entities, and organizations.
- **Resource Efficiency**: It optimizes resource utilization by enabling the sharing of infrastructure components, enhancing cost-effectiveness.
- **Data Isolation**: Multi-tenancy ensures that the data, such as DIDs (Decentralized Identifiers), connections, VCs (Verifiable Credentials), keys, and more, of one tenant is completely isolated from others, preserving data privacy and security.
- **Enhanced Collaboration**: Shared wallets enable collaboration between entities while maintaining data segregation.
- **Granular Control**: Administrators control the resources and permissions assigned to each entity.

## Roles in multi-tenancy

Multi-tenancy permits multiple users or systems to use the Cloud Agent within a shared instance, enhancing resource utilization while preserving each tenant's and their resources' separation. Understanding different roles in multi-tenancy is essential for the secure and efficient operation of the Cloud Agent.

### Administrator

The administrator's primary responsibilities include setting up, allocating, and managing tenant resources. Administrators have elevated privileges within the system, allowing them to create, modify, or delete multi-tenant resources such as entities, wallets, and authentication methods. They play a critical role in tenant management but typically don't participate in everyday interactions.

### Tenant

Tenants are the users or systems engaged in everyday interactions. They have representation via a resource called an entity, and this entity uses a wallet as a container for the assets they own. Tenants can only view the assets within the wallet assigned to their entity. To access the system, they need an authentication method provided by the administrator.

## Concepts

### Wallet

A wallet is a central component of Identus' multi-tenancy model. It serves as the container for storing and managing a tenant's digital assets, including DIDs, connections, VCs, keys, and more. Wallets are logically isolated to ensure that the data associated with one entity remains separate and secure from others.

### Entity

An entity represents a user or any other identity within the Identus platform. Each entity owns a Wallet, making it the primary entity for managing and interacting with its digital credentials and assets. Entities can also share wallets, enabling collaborative use of resources.

### Authentication Method

Each entity is associated with an Authentication Method, which serves as a secure means of verifying the identity and access rights of the entity. This method ensures the entity's identity is authenticated during interactions with the platform, enhancing security.

### Logical Isolation
Logical Isolation is a core principle of Identus' multi-tenancy model. It ensures that one entity's digital assets, transactions, and data are logically separated from others, maintaining the highest data privacy and security level.

### Shared Wallets
Identus' multi-tenancy capabilities allow for the sharing of wallets among multiple entities. This feature facilitates collaborative work and resource sharing while preserving data isolation within the shared wallet. The entity can own only one wallet. Sharing it with other entities is possible, but multiple entities cannot own it.

### Tenant Management
Tenant Management is the process of onboarding, provisioning, and managing entities and wallets within the Identus platform. Administrators can configure each entity's permissions, resources, and access control, ensuring efficient and secure multi-tenancy operations.

### DIDComm Connections
DIDComm Connections are the secure communication channels between peers within the SSI ecosystem. Identus' multi-tenancy model ensures that the connections of one entity are logically isolated from those of other entities, preserving data privacy and security. Based on the DID-Peer of the message recipient, the corresponding wallet processes the message.

### Webhook Notifications
Webhook notifications enable users to receive alerts for specific events in the system. There are two types of webhook notifications:
- Global webhooks: monitor all events across all wallets at the Cloud Agent level
- Wallet webhooks: isolated to individual wallets and do not have visibility into assets in other wallets.

## Relation Diagram

The following diagram illustrates the relationship between the key components of Identus' multi-tenancy model.


```mermaid
graph TB;
    Tenant(Tenant) --> |Represented by| Entity
    Tenant --> |Authenticates by| AuthMethod(Authentication Method)

    subgraph Identus Platform
            Wallet(Wallet) -->|Contains| DIDs(DIDs )
            Wallet(Wallet) -->|Contains| VCs(Verifiable Credentials)
            Wallet(Wallet) -->|Contains| VSSchemas(VC Schemas)
            Wallet(Wallet) -->|Contains| Assets(Other Assets...)
            Entity(Entity) -->|Owns| Wallet(Wallet)
            Entity(Entity) -->|Uses| AuthMethod(Authentication Method)
    end
```
</file>

<file path="documentation/learn/components/cloud-agent/README.md">
# Overview

The [Cloud Agent](/home/concepts/glossary#cloud-agent) is a scaleable, easy-to-use, robust, and W3C standards-based agent that provides [self-sovereign identity (SSI)](/home/concepts/glossary#self-sovereign-identity) services to build products and solutions based on it. The Cloud Agent exposes REST API for integration with any programming language.

The Cloud Agent provides all the required capabilities to leverage the power of decentralized identity through the support of W3C standards, [DIDComm](/home/concepts/glossary#didcomm), and the Hyperledger Aries protocols, solutions based on the Cloud Agent are interoperable with the SSI ecosystem. The Cloud Agent includes the following high-level features:

- Use the Cloud Agent API to develop [controllers](/home/concepts/glossary#controller) that implement specific business logic according to use case requirements.
- Develop controllers in any programming language and remove the requirement for developers to interact or learn the nuts and bolts of the various standards.
- The Cloud Agent is written in Scala, supports the `did:prism` method, interacts with the PRISM Node over gRPC protocol, and uses it as the [Verifiable Data Registry](/home/concepts/glossary#verifiable-data-registry). 
- The PRISM Node interacts with a distributed ledger, providing high security, scalability, and availability.
- The Cloud Agent is a cloud agent accessible from anywhere.
- Highly scalable and flexible, making it well-suited for a wide range of use cases and applications.


## Cloud Agent Features

This document provides an overview of the Cloud Agent feature set. This document is manually updated; as such, it may not be up to date with the most recent release of Cloud Agent.

**Last Update**: 2023-03-06, Release 0.48.3

## Platform Support

| Platform | Supported | Notes             |
| -------- | :-------: |  ------- |
| Server   | :white_check_mark: |    |
| Kubernetes | :white_check_mark: |  |
| Docker   | :white_check_mark: | IOG will publish docker images on Docker Hub in future. A link to the IOG Docker Hub for Cloud Agent will be provided once published. |
| Desktop  | :warning:         | Could be run as a local service on the computer |
| iOS      | :x:        |    |
| Android  | :x:        |    |
| Browser  | :x:        |    |

## Agent Types

| Role     | Supported | Notes      |
| -------- | :-------: |  --------- |
| Issuer   | :white_check_mark:        |            |
| Holder   | :white_check_mark:        |            |
| Verifier | :white_check_mark:        |            |
| Mediator Service | :white_check_mark:|            |

## Credential Types

| Credential Type | Supported | Notes |
| --- | :--: | -- |
| [W3C Standard Verifiable Credentials](https://www.w3.org/TR/vc-data-model/) | :white_check_mark: | Supports JSON and JSON-LD Credentials encoded as JWT using the `JsonWebSignature2020` with `ES256K` (secp256k1) signature suite.<br /><br />Supports the [DIF Presentation Exchange](https://identity.foundation/presentation-exchange/) data format for presentation requests and presentation submissions. |

## DID Methods

| Method | Supported | Notes                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |
| --- | :--: |------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| `did:prism` | :white_check_mark: | W3C [Decentralized Identifiers (DIDs) v1.0](https://www.w3.org/TR/did-core/) compliant [PRISM DID Method](https://github.com/input-output-hk/PRISM-did-method-spec/blob/main/w3c-spec/PRISM-method.md) specification. A distributed ledger is the default [Verifiable Data Registry (VDR)](/home/concepts/glossary#verifiable-data-registry) to anchor DIDs. PRISM DID Method is used for [Verifiable Credentials](/home/concepts/glossary#verifiable-credential) and [Verifiable Presentations](/home/concepts/glossary#verifiable-presentation). |
| `did:peer` | :white_check_mark:| Used for P2P connections                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |
| Universal Resolver | :construction: |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |

## DIDComm

| Version | Supported |
| --- | :--: |
| [`V1`](https://github.com/hyperledger/aries-rfcs/blob/main/concepts/0005-didcomm/README.md) | :x: |
| [`V2`](https://identity.foundation/didcomm-messaging/spec/) | :white_check_mark: |

## DIDComm Protocol Support

| Protocol | Supported |
| --- | :--: |
| [Mediator Coordinator](https://didcomm.org/mediator-coordination/2.0/) | :white_check_mark: |
| [DIDComm V2 Messaging](https://identity.foundation/didcomm-messaging/spec) | :white_check_mark: |
| [DIDComm V2 Issue Credential](https://github.com/decentralized-identity/waci-didcomm/tree/main/issue_credential) | :white_check_mark: |
| [DIDComm V2 Present Proof](https://github.com/decentralized-identity/waci-didcomm/blob/main/present_proof/present-proof-v3.md) | :white_check_mark: |
| [DIDComm V2 Report Problem](https://identity.foundation/didcomm-messaging/spec/#problem-reports) | :white_check_mark: |
| [DIDComm V2 Routing Protocol](https://identity.foundation/didcomm-messaging/spec/#routing-protocol-20) | :white_check_mark: |

## Supported Aries RFCs

| RFC | Supported |
 --- | :--: |
| [0023-did-exchange](https://github.com/hyperledger/aries-rfcs/tree/main/features/0023-did-exchange)      | :white_check_mark:        |
| [0434-out-of-band-protocol](https://github.com/hyperledger/aries-rfcs/blob/main/features/0434-outofband/README.md)      | :white_check_mark:        |
| [0453-issue-credential-protocol](https://github.com/hyperledger/aries-rfcs/tree/main/features/0453-issue-credential-v2)      | :white_check_mark:        |
| [0453-present-proof-protocol](https://github.com/hyperledger/aries-rfcs/tree/main/features/0454-present-proof-v2)      | :white_check_mark:        |
</file>

<file path="documentation/learn/components/did-prism/_category_.json">
{
    "label": "DID Prism",
    "collapsed": true,
    "link": {
      "type": "doc",
      "id": "README"
    }
  }
</file>

<file path="documentation/learn/components/did-prism/did-prism-resolver-sdk.md">
---
title: DID Resolver
---
# DID PRISM Resolver in SDKs

In order to resolve the DID PRISM in the SDKs, the corresponding DIDResolver must be configured in the application.
The DID resolver URL should be set in the SDK configuration to point to the appropriate resolver endpoint.

## SDK-TS

The SDK-TS provides a way to resolve PRISM DIDs and retrieve their associated DID Documents. 
The resolver that can be configured with the corresponding URL is the [PrismShortFormDIDResolver](https://github.com/hyperledger-identus/sdk-ts/blob/main/integration-tests/e2e-tests/src/resolvers/PrismShortFormDIDResolver.ts).

## SDK-Swift
The SDK-Swift provides a way to resolve PRISM DIDs and retrieve their associated DID Documents.
The resolver that can be configured with the corresponding URL is the [PrismShortFormResolver](https://github.com/hyperledger-identus/sdk-swift/blob/main/E2E/Tests/Source/Resolvers/PrismShortFormResolver.swift)

## SDK-KMP

SDK-KMP cannot resolve the short form DIDs, but it can resolve the long form DIDs.
</file>

<file path="documentation/learn/components/did-prism/README.md">
# DID PRISM Resolver

The did:prism method has multiple resolver implementations in different programming languages, that are totally independent.

Officially according to the specs the `did:prism` method only existed in the Cardano `mainnet` network.
But for testing purposes, many of the users are using the Cardano `preprod` network.

## Cloud Agent and Prism Node
The Cloud Agent together with the Prism Node provides a comprehensive solution for managing and resolving DIDs. The Prism Node is a microservice that exposes gRPC endpoints and implements `did:prism` events, allowing users to create, update, deactivate, and resolve DIDs in a secure and privacy-preserving manner.
This solution is designed for the enterprise use case and requires setting up the full Cardano stack together with the Prism Node.

## Universal Resolver
The Universal Resolver is a service that provides a unified interface for resolving DIDs across different DID methods. 
It allows users to resolve DIDs from various DID methods, including PRISM, and retrieve their associated DID Documents.
The `did:prism` has been integrated into the Universal Resolver, enabling users to resolve PRISM DIDs using the same interface as other DID methods.

According to the `did:prism` specification, the Universal Resolver resolves PRISM DIDs published on the Cardano mainnet.

The Universal Resolver endpoint for PRISM DIDs is:
https://dev.uniresolver.io/

## SDKs
Each SDK provides a way to resolve DIDs and retrieve their associated DID Documents. The SDKs are designed to be easy to use and integrate into existing applications, allowing developers to quickly add DID resolution capabilities to their projects.
Each SDK implements the `DIDResolver` interface via a URL. In order to configure the DIDResolver the corresponding endpoint should be set in the SDK configuration.

## Identus Community Project

### Blocktrust Node
The Blocktrust Node is a community alternative to the Prism Node, providing similar functionality for resolving DIDs and managing DID Documents.
The publicly available Blocktrust Node can be used to resolve PRISM DIDs and retrieve their associated DID Documents.

The list of the Blocktrust endpoints for the PRISM DIDs is:
[https://statistics.blocktrust.dev/resolve](https://statistics.blocktrust.dev/resolve)

### Neoprism
The [Neoprism](https://github.com/patextreme/neoprism) is a community alternative to the Prism Node, providing similar functionality for resolving DIDs and managing DID Documents.
The publicly available Neoprism can be used to resolve PRISM DIDs and retrieve their associated DID Documents.

The instance of the Neoprism for the PRISM DIDs published in the Cardano mainnet is:
[https://neoprism.patlo.dev/resolver](https://neoprism.patlo.dev/resolver)

### PRISM Indexer

The PRISM Indexer is a community project that uses the Blockfrost API to fetch all relative transactions and metadata from the Cardano Blockchain relative to `did:prism` protocol.
It them passes the method data into PRISM events and indexes them by DID.

After having the sequence of events relative to each `did:prism` the indexer is capable of validate the PRISM Events and build the corresponding DID Document.
This validation can also be performed locally on the end-user's device. Requiring zero trust on Indexer since from the PRISM events you can verify the authenticity from the SSI itself.

The project is distributed as a Docker image and available in [fmgp's Docker Hub](https://hub.docker.com/repository/docker/fmgp/prism-indexer/general).
This image store the status off all DIDs in the file system. But the image is light enough that is capable of running constantly in GitHub actions.

The code is also distributed as a library in [Maven Central repository](https://mvnrepository.com/repos/central) and it's capable to run on the JVM and JS Environments.

#### Prism-VDR
The [prism-vdr](https://github.com/FabioPinheiro/prism-vdr) is alternative PRISM DID indexer that stores all the PRISM DIDs events and their associated DID Documents in the GitHub repository.
The PRISM-VDR supports the following Cardano networks:
- [`mainnet` network](https://github.com/FabioPinheiro/prism-vdr/tree/main/mainnet/diddoc)
- [`preprod` network](https://github.com/FabioPinheiro/prism-vdr/tree/main/preprod/diddoc)

#### Blockfrost & PRISM 

Blockfrost is also constantly running the PRISM Indexer for all the Cardano networks and uploading the status to a [Blockfrost's GitHub repository](https://github.com/blockfrost/prism-vdr):
- [`mainnet` network](https://github.com/blockfrost/prism-vdr/tree/main/mainnet/diddoc)
- [`preprod` network](https://github.com/blockfrost/prism-vdr/tree/main/preprod/diddoc)
- [`preview` network](https://github.com/blockfrost/prism-vdr/tree/main/preview/diddoc)

Blockfrost is also developing a experimental API for resolving and managing the PRISM DIDs.

#### scala-did

Its a tool using the same library as PRISM Indexer but running the in JS Environment.
Its fetchs the all PRISM events from one DID from the Prism-VDR Github repos and resolves into a DID Document.

A instance of this tool can be found in [did.fmgp.app](https://did.fmgp.app/#/resolver/did:prism:00592a141a4c2bcb7a6aa691750511e2e9b048231820125e15ab70b12a210aae).
Its capable of resolving some DID methods including the `did:prism`.
</file>

<file path="documentation/learn/components/prism-node/_category_.json">
{
    "label": "Prism Node",
    "collapsed": true,
    "link": {
      "type": "doc",
      "id": "README"
    }
  }
</file>

<file path="documentation/learn/components/prism-node/README.md">
---
id: README
sidebar_position: 1
---

# PRISM Node
The PRISM Node acts as a second-layer node for the Distributed Ledger. It functions as a [verifiable data registry](/home/concepts/glossary#verifiable-data-registry), providing a secure and reliable way to store and manage data.

PRISM Node's primary purpose is to provide a secure and trustworthy platform for storing and managing data on the Distributed Ledger. By leveraging the blockchain's security and decentralization, the PRISM Node stores and retrieves data in a secure and immutable manner. All operations are independently verified and authenticated using cryptographic signatures and other security measures, so all data is accurate and trustworthy.

The PRISM Node runs alongside a blockchain. It keeps the internal state synchronized with the underlying blockchain and indexed for efficient lookup operations. Furthermore, it implements `did:prism` method in an efficient and scalable way by publishing transactions to the Distributed Ledger. These transactions support the PRISM Node's protocol and perform the creation, update, resolution, and deactivation of [decentralized identifiers (DIDs)](/home/concepts/glossary#decentralized-identifiers).

The PRISM Node generates a transaction with information about the DID operation and verifies and validates the DID operation before publishing it to the blockchain. Once the transaction gets confirmed on the blockchain, the PRISM Node updates its internal state to reflect the changes.

The second-layer PRISM Node also plays a critical role in resolving DIDs. Because the PRISM Node keeps the internal state indexed, it allows retrieving information about a particular DID quickly and efficiently without requiring access to the underlying blockchain.

Overall, second-layer nodes are essential for the PRISM Node protocol, providing the necessary off-chain processing and data storage capabilities to make DIDs scalable and efficient.

At its core, PRISM depends on protocols defining how to manage decentralized identifiers and [Verifiable Credentials (VCs)](/home/concepts/glossary#verifiable-credential). The PRISM Node is the main component that implements these protocols. First, we should explain how the protocol works to understand what the PRISM Node does.

- Any user can run a PRISM Node to self-validate information or rely on a set of actors that run nodes on his behalf. The level of delegation of trust is a decision made by each user.
- Any user willing to create a DID can do so without any need to interact with any PRISM Node. Creating a DID can be optionally announced publicly by publishing a creation operation on-chain. The action of posting an operation on-chain does require interaction with a PRISM Node.
- Users can update the DID documents associated with their DIDs. To do this, they must publish respective update operations on-chain by interacting with a PRISM Node.
- Deactivating a DID can be performed along the same lines as an update by publishing a deactivation operation.
- PRISM Nodes read the operations published on-chain (by possibly other nodes) and internally maintain the map of DIDs to the history of changes of their associated DID documents.
- Clients can query any PRISM Node and obtain a DID's historical change log information.
- DID resolvers can take the output of PRISM Nodes and construct the current DID document associated with a DID.
- An additional consideration is that operations can be posted on-chain in blocks, helping on the scalability side and general reduction of fees.


Additionally, the node provides an interface to track the status of operations submitted to a PRISM Node.
</file>

<file path="documentation/learn/components/prism-node/running-node.md">
# Running Node

## Overview

The Node is a level 2 proxy on top of the Cardano blockchain responsible for publishing, resolving, updating, and deactivating DIDs. It exposes a gRPC API to perform all the operations above. 

:::note
This documentation will not describe the gRPC API itself.
:::

The Identus cloud Agent communicates with the Node, with all operations conducted through it. The following documentation will provide a high-level overview of how the Node functions and explain its usage, including the environment variables required and how to deploy it to the Cardano mainnet and testnet.

## Node components

The Node consists of four separate executables:

1. Node gRPC server
2. Node PosgresQL database
3. Cardano wallet backend
4. DB sync

#### Node gRPC server

The Node gRPC is a service responsible for submitting transactions to the Cardano network with a configurable frequency, retrieving blocks, and processing the data for storage in a database.


#### Node PosgresQL database

The database used by the Node to store processed data, namely DID documents and their respective states.


#### Cardano wallet backend

An interface enables the Node to submit transactions to the Cardano node.

#### DB sync.

The Node uses an indexed version of the Cardano blockchain to access and parse minted blocks.


```mermaid
graph TB
    GRPC["Node gRPC Server"]
    DB["Node PostgreSQL Database"]
    CWB["Cardano Wallet Backend"]
    DBS["DB Sync"]
    CN["Cardano node"]
    
    GRPC -->|"Submits transactions via"| CWB
    GRPC -->|"Processes data for storage"| DB
    DBS --> |"Indexes the whole blockchain"| CN
    CWB -->|"Submits trasactions"| CN
    GRPC --> |"retrieves blocks and their transactions"| DBS

```

### Node enviroment variables


| Environment Variable                    | Description                                                                           | Default Value   |  Data Type                       |
|-----------------------------------------|---------------------------------------------------------------------------------------|-----------------|----------------------------------|
| NODE_PSQL_HOST                          | Host and port of Node PosgresQL database                                              | localhost:5432  | String                           |
| NODE_PSQL_DATABASE                      | Name of the database to connect to                                                    | node_db         | String                           | 
| NODE_PSQL_USERNAME                      | Username for database authentication                                                  | postgres        | String                           |
| NODE_PSQL_PASSWORD                      | Password for database authentication                                                  | postgres        | String                           |  
| NODE_PSQL_AWAIT_CONNECTION_THREADS      | Maximum amount of database connections                                                | 8               | Int                              |
| NODE_LEDGER                             | Ledger which will be used for txs and blocks                                          | in-memory       | Enum(in-memory, cardano)         |
| NODE_REFRESH_AND_SUBMIT_PERIOD          | Time period between refreshing transaction statuses and submitting pending operations | 20s             | String                           |
| NODE_MOVE_SCHEDULED_TO_PENDING_PERIOD   | Time period between making scheduled operations ready for submissions                 | 15s             | String                           |
| NODE_WALLET_MAX_TPS                     | Maximum number of transactions cardano-wallet can work with                           | 10              | Int                              |
| NODE_DID_PUBLIC_KEYS_LIMIT              | Maximum number of public keys Node API can create/update/get per request to a DID     | 50              | Int                              |
| NODE_DID_SERVICES_LIMIT                 | Maximum number of services Node API can create/update/get per request to a DID        | 50              | Int                              |
| NODE_SERVICE_ENDPOINT_CHAR_LIMIT        | Maximum number of characters every DID service endpoint can have                      | 300             | Int                              |
| NODE_SERVICE_TYPE_CHAR_LIMIT            | Maximum number of characters every DID type can have                                  | 100             | Int                              |
| NODE_CONTEXT_STRING_CHAR_LIMIT          | Maximum number of characters every context string of a DID can have                   | 100             | Int                              |
| NODE_ID_CHAR_LIMIT                      | Maximum number of characters id field of pk and service can have                      | 50              | Int                              |
| NODE_CARDANO_NETWORK                    | Cardano network node should operate on                                                | testnet         | Enum(testnet, mainnet)           |
| NODE_CARDANO_WALLET_ID                  | ID (hex encoded) of the wallet to use for payments                                    |                 | String                           |
| NODE_CARDANO_WALLET_PASSPHRASE          | Spending passphrase of NODE_CARDANO_WALLET_ID                                         |                 | String                           | 
| NODE_CARDANO_PAYMENT_ADDRESS            | Address (hex encoded) to make payments to, can be NODE_CARDANO_WALLET_ID itself       |                 | String                           |
| NODE_CARDANO_WALLET_API_HOST            | Cardano wallet backend API host                                                       | localhost       | String                           |
| NODE_CARDANO_WALLET_API_PORT            | Cardano wallet backend API port                                                       | 8090            | Int                              |
| NODE_CARDANO_WALLET_ROUTING_HEADER_NAME | Cardano wallet backend routing header name                                            |                 | String                           |
| NODE_CARDANO_PRISM_GENESIS_BLOCK        | Index of the first block from which node should start syncing from                    | 1868381         | Int                              |
| NODE_CARDANO_CONFIRMATION_BLOCKS        | Number of blocks to wait before transaction is considered to be confirmed             | 112             | Int                              |
| NODE_CARDANO_DB_SYNC_HOST               | Db sync database host and port                                                        | localhost:5433  | String                           |
| NODE_CARDANO_DB_SYNC_DATABASE           | databse name in DB sync PosgresQL database                                            | cexplorer       | String                           |
| NODE_CARDANO_DB_SYNC_USERNAME           | Username for db sync database authentication                                          | postgres        | String                           |
| NODE_CARDANO_DB_SYNC_PASSWORD           | Password for db sync database authentication                                          | password        | String                           |


#### Running node

Node docker image is available on GitHub, accessible here:

```bash
docker pull ghcr.io/input-output-hk/prism-node:2.3.0
```

By default, Node will run with an `in-memory` ledger, ideal for development purposes. To run it on Cardano, you must set the `NODE_LEDGER` environment variable to `cardano`. If you do this, Node will utilize the Cardano wallet backend and DB-sync to query for blocks and submit transactions. It is crucial to have the Cardano wallet backend and DB-sync running before running the Node with `NODE_LEDGER` set to `cardano`.

Most up-to-date instructions on how to run the Cardano wallet backend and DB-sync are available in their respective repositories:

* [Cardano wallet backend](https://github.com/cardano-foundation/cardano-wallet)
* [DB-sync](https://github.com/IntersectMBO/cardano-db-sync)

Once you have these services up and running, specify their respective URLs in the environment variables of the Node:

* Cardano wallet backend
    - `NODE_CARDANO_WALLET_API_HOST` for wallet server host
    - `NODE_CARDANO_WALLET_API_PORT` for wallet server port
    - `NODE_CARDANO_WALLET_ROUTING_HEADER_NAME` for wallet routing header name
* DB-sync
    - `NODE_CARDANO_DB_SYNC_HOST` for DB-sync host and port in a format `host:port`
    - `NODE_CARDANO_DB_SYNC_DATABASE` the databse name in DB-sync postgres database
    - `NODE_CARDANO_DB_SYNC_USERNAME` DB-sync Database username 
    - `NODE_CARDANO_DB_SYNC_PASSWORD` DB-sync Database password

When running the Node with Cardano ledger, you must specify which network to use, either `mainnet` or `testnet`, using the `NODE_CARDANO_NETWORK` environment variable. While this environment variable is essential for the correct operation of the Node, it does not define the usable network. As mentioned earlier, the interface communicates with the Cardano node; subsequently, the network is DB-sync and Cardano wallet backend. Therefore, when configuring those services, you must specify the network used in their respective configurations. It is possible to run DB-sync on testnet, and Cardano wallet backend on mainnet, and select either one via `NODE_CARDANO_NETWORK`. The Node won't report any errors, but this configuration would be incorrect and won't work correctly. You are responsible for syncing these three components. If you intend to use the testnet, set `NODE_CARDANO_NETWORK` to the testnet, but also run Cardano wallet backend connected to the testnet and start DB-sync to sync from the Node that is also running on the testnet as well. The same goes with mainnet.

Apart from that, you must also provide the Wallet ID and its spending password as environment variables as well:

* `NODE_CARDANO_WALLET_ID` - The wallet ID must be in hex-encoded format, and the wallet must belong to the network you are running the Node on, which can be either testnet or mainnet     
* `NODE_CARDANO_WALLET_PASSPHRAS` - Spending password or wallet above

The Node utilizes Cardano as a decentralized open database, and its implementation is similar to the DIF Sidetree Protocol. In short, the Node stores all relevant information in a Cardano transaction metadata and sends 1 ADA (minimum allowed amount) to another address, which you must provide via the `NODE_CARDANO_PAYMENT_ADDRESS` environment variable, which will store arbitrary information on the blockchain. In most cases, you don't need to specify a particular address for sending transactions as long as the transaction gets recorded. In this case, you should set NODE_CARDANO_PAYMENT_ADDRESS to the same address you are sending transactions from, `NODE_CARDANO_WALLET_ID`. In this configuration, you are not spending any ADA other than the transaction fee for every transaction. Suppose your wallet does not have enough ADA to cover the transaction fee (plus 1 ADA to send to yourself). In that case, the transaction won't get recorded, and your operation, which includes any DID-related action, won't be submitted.

The last component is the database that the Node uses internally to index Identus cloud agent related Cardano transactions and maintain the state of DID documents. It is a simple PostgreSQL version 13 database, and you can use the Docker image `postgres:13` to run it.

When running the Node, you must specify the host, database name, username, and password of this database via environment variables.

* `NODE_PSQL_HOST` - host with a port, in a format `host:port`
* `NODE_PSQL_DATABASE` - database name
* `NODE_PSQL_USERNAME` - username
* `NODE_PSQL_PASSWORD` - password


The Node gRPC server has three dependencies: Node DB, Cardano wallet, and DB-sync. You need to run these three services before starting the Node.

Node DB is a simple PostgreSQL database. 

Cardano wallet is an application that communicates with the Cardano network; it functions as a server that you can start and connect to either the mainnet or testnet. You must provide the Node runnable's host and port as environment variables.

DB-sync is an application responsible for syncing the Cardano blockchain with a PostgreSQL database. It would help if you used it to sync with either the mainnet or testnet and must provide the database host with port, database name, and credentials as environment variables to the Node runnable.
</file>

<file path="documentation/learn/components/_category_.json">
{
    "label": "Components",
    "position": 3,
    "collapsed": false,
    "collapsible": false,
    "link": {
      "type": "generated-index",
      "title": "Components",
      "description": "Technical reference documentation including ADRs, specifications, and API details."
    }
  }
</file>

<file path="documentation/learn/components/mediator.md">
# Mediator

## Introduction

[Mediators](/home/concepts/glossary#mediator) are indispensable in identity wallets, especially when dealing with mobile devices or edge agents. These mobile devices usually lack static IP addresses. They may only sometimes be online, posing a challenge in establishing reliable, direct connections between various parties in a decentralized ecosystem and guaranteeing delivery of [DIDComm](/home/concepts/glossary#didcomm) messages. The Mediator functions similarly to an email inbox for the recipient, storing "Forward" messages and their final encrypted content until retrieved by the intended party.

## Mediator's Role in Decentralized Identity Transactions

Within a decentralized identity framework, the Mediator bridges the [Holder's](/home/concepts/glossary#holder) device and other key players, such as [Issuers](/home/concepts/glossary#issuer) and [Verifiers](/home/concepts/glossary#verifier). For instance, if a Holder wishes to obtain a credential from an Issuer, the Mediator can [relay](/home/concepts/glossary#relay) or mediate this request, ensuring smooth interaction between both parties. Similarly, when a holder must present a credential to a verifier, the Mediator steps in again to mediate or relay the transaction. By leveraging a Mediator, identity wallets can offer secure and reliable communication between all involved entities. This approach also empowers holders to maintain control over their data and identities.

## Security, Scalability, and Efficiency

The Mediator is an integral part of Identus, facilitating secure communication without the need for a centralized authority. It seamlessly connects various entities in the Identus ecosystem, including Holders, Issuers, and Verifiers, ensuring the system remains secure, scalable, and efficient.

## Technical Specifications

The Mediator is an open-source initiative. For more details, you can refer to the [atala-prism-mediator Github repository](https://github.com/input-output-hk/atala-prism-mediator). The project aims to address the inherent limitations of edge agents, which are not always online, by assuming that the Mediator is perpetually online. Employing DIDComm v2 protocols and cryptographic methods, the Mediator guarantees all exchanged messages' authenticity, integrity, and confidentiality.

## DIDComm V2 Mediator Test Suite

### Overview
We have rigorously evaluated our Mediator protocols using the [DIDComm V2 Mediator Test Suite](https://github.com/input-output-hk/didcomm-v2-mediator-test-suite/). This test suite scrutinizes the compatibility of mediators with DIDComm V2 protocols, serving as a benchmark for quality and reliability.

### Protocols Tested
The suite tests a variety of vital protocols, including:

- [Trust Ping 2.0](https://didcomm.org/trust-ping/2.0/)
- [Mediator Coordination 2.0](https://didcomm.org/mediator-coordination/2.0/)
- [Pickup 3.0](https://didcomm.org/pickup/3.0/)

You can consult the [features folder](https://github.com/input-output-hk/didcomm-v2-mediator-test-suite/blob/main/src/test/resources/features) in the test suite repository for an exhaustive list of test scenarios and detailed descriptions.

### Future Enhancements in Testing

As part of our ongoing commitment to improving the Mediator, we plan to make our test sets publicly accessible, allowing users and developers to perform comparative assessments on functionality and performance against other mediators in the market. We aim to continually refine the Mediator's features based on real-world scenarios and demands, ensuring it remains a reliable and robust component in the Identus ecosystem.

We are working on enhancing the scope of our testing suite to address limitations and include additional protocols and scenarios. These future updates will make the Mediator even more robust and versatile.

The Mediator will continue to develop with the same rigorous testing protocols to ensure it remains at the forefront of secure, efficient, and interoperable decentralized identity management.

## Future Developments

Looking ahead, we are committed to the ongoing development of the Mediator, with plans to expand its feature set and integrate new protocols. Real-world use cases drive these updates to achieve feature completeness. Additionally, we aim to keep the Mediator at the forefront of industry developments, ensuring it remains aligned with emerging protocols and technological advancements.
</file>

<file path="documentation/learn/glossary.md">
# Glossary

## A
### Access control
Access control mechanisms define how tenants/entities can access and interact with their data and resources and control who can access them. 
It helps enforce security and privacy policies in a multi-tenant environment.

### Anchoring
The act of anchoring is tying to something that is trusted by assumption. Usually some sort of an entity with authority.

### Administrator
An administrator is a role who oversees the agent and releated resources, including tenant, Edge Agent management, or external services. Admistrator typically does not participate in day-to-day SSI interactions.


## C
### Claim {#claim}
An assertion made about a [subject](#did-subject).

### Claims {#claims}
Synonym of [claim](#claim) in the plural form. 

### Cloud Agent {#cloud-agent}
The Cloud Agent is a scaleable, easy-to-use, robust, and W3C standards-based agent that provides self-sovereign identity (SSI) services to build products and solutions based on it. The Cloud Agent exposes REST API for integration with any programming language.

### Controller
See [DID Controller](#did-controller).

### Connection Protocol
The protocol provides endpoints for creating and managing connections, as well as for accepting invitations.

### connection invitation
An invitation from one entity to another to establish a connection.

### Connection request
A request to establish a connection.

### Credential Definition {#credential-definition}
The term [refers](https://hyperledger.github.io/anoncreds-spec/#term:credential-definition) to the AnonCreds v1 implementation. 
Credential Definition contains public and private part. 
The public part is published and available for anyone to use to verify the credential. The private part is used to issue credentials.

### Credential schema
A data template for verifiable credentials (VCs). It contains claims of the VCs, credential schema author, type, name, version, and proof of authorship.

### Credential offer
An Issuer sends a request to the Holder to accept a verifiable credential.

### Credential request
When the Holder accepts or rejects a credential offer, a credential request is created from it.



## D
### Decentralized Identifier {#decentralized-identifier}
A globally unique persistent identifier that does not require a centralized registration authority and is often cryptographically generated. All DIDs use distributed ledger technology (DLT) or some other decentralized network.

### DID {#did}
See [decentralized identifier](#decentralized-identifier)

### DIDs {#dids}
See [decentralized identifiers](#decentralized-identifier)

### DIDComm {#didcomm}
A set of secure, standards-based communications protocols to establish and manage trusted, peer-to-peer connections and interactions between DIDs in a transport-agnostic and interoperable manner.

### DID controller
The entity that has control of the DID

### DID document {#did-document}
A set of data that describes the DID subject, including mechanisms such as cryptographic public keys. The entire W3C DID specification is [here](https://www.w3.org/TR/did-spec-registries/).

### DID method
The DID method defines how to implement a specific DID method schema. The specification defines the DID method, including precise operations to create DIDs and [DID documents](#did-document) and how to resolve, update, and deactivate them.

### DID resolution
The process for retrieving a [DID document](#did-document).

### DID subject {#did-subject}
The entity is identified by a [DID](#decentralized-identifier) and described by a [DID documents](#did-document). Anything can be a DID subject: person, group, organization, physical thing, digital thing, etc.

### DID Url
A DID itself is a type of a URL, while `did` is a registered schema type, like `http` and `https`. With Identus, we refer DID URLs to a DID that includes path and query parameters and can resolve a resource via one of the service endpoints in the DID document. For example: 
```
did:prism:9f847f8bbb66c112f71d08ab39930d468ccbfe1e0e1d002be53d46c431212c26?resourceService=agent-base-url&resourcePath=schema-registry/schemas/did-url&resourceHash=4074bb1a8e0ea45437ad86763cd7e12de3fe8349ef19113df773b0d65c8a9c46
```

### Distributed Ledger Technology (DLT) {#dlt}
A distributed database or ledger establishes confidence for the participants to rely on the data recorded. Typically these databases use nodes and a consensus protocol to confirm the order of cryptographically signed transactions. Linking the transactions over time creates a historical ledger that is effectively immutable.



## E
### Endpoints
A network address at which services operate on behalf of a [DID subject](#did-subject).

### Entity
An `entity,` in the context of the Identus platform, is an identity representing a user or system. 
Each entity possesses an Edge Agent and is associated with an authentication method. 
Entities are crucial for secure and verifiable transactions within the SSI ecosystem.

### Edge Agent SDK {#edge-agent-sdk}
For use with web and mobile (iOS, Android, TypeScript) enable identity holders to store credentials and respond to proof requests.

### Edge Agent
A Edge Agent can perform DID operations, like create, update, and deactivate. It also enables management of verifiable credentials, and communications.

## G
### Governance framework
See [Trust Framework](#trust-framework)



## H
### Holder {#holder}
An entity will take on this role by possessing one or more [verifiable credentials](#verifiable-credential) and generating [verifiable presentations](#verifiable-presentation). Also takes the role of a prover when presenting verifiable credentials for verification.



## I
### IAM
IAM (Identity and Access Management), is a framework that controls and manages user access to computing resources. It ensures secure authentication, appropriate authorization, and effective auditing to protect against unauthorized access in a computing environment.

### Identus
A suite of products that provides infrastructure for decentralized identity.

### IDP {#idp}
An Identity Provider (IDP) is a centralized service that manages and authenticates user identities, allowing individuals to access multiple applications and services with a single set of credentials. IDPs play a crucial role in Single Sign-On (SSO) systems, simplifying user access management across various platforms and services.

### Invitation
Sent by the [inviter](#inviter) to the [invitee](#invitee) to request and establish a connection.

### Invitee
A subject that receives a connection invitation and accepts it by sending a connection request.

### Inviter
A subject that initiates a connection request by sending a connection invitation.

### Issuer {#issuer}
An entity that asserts claim(s) about one or more [subjects](#did-subject) then creates a [verifiable credential](#verifiable-credential) from these claims and transmits the VC to a holder.

### Issue Credential Protocol
Allows you to create, retrieve, and manage issued [verifiable credentials (VCs)](#verifiable-credential) between a VC issuer and a VC holder.

## K
### Keycloak Service
Keycloak is an open-source [IAM](#iam) solution that provides authentication, authorization, and single sign-on capabilities for applications and services. It allows organizations to secure their applications by managing user identities, enforcing security policies, and facilitating seamless and secure user authentication.


## M
### Mediator {#mediator}
A mediator participates in agent-to-agent message delivery that the sender must model. It has its keys and will deliver messages only after decrypting an outer envelope to reveal a forward request. Many types of mediators may exist, but two important ones should be widely understood, as they commonly manifest in DID Docs:
- A service that hosts many cloud agents at a single endpoint to provide herd privacy (an "agency") is a mediator.
- A cloud-based agent that routes between/among the edges of a sovereign domain is a mediator.
For a detailed overview of mediators refer to the [RFC0046: Mediators and Relays](https://github.com/hyperledger/aries-rfcs/tree/main/concepts/0046-mediators-and-relays).

### Mutli-tenancy
Multi-tenancy is a core capability of the Identus platform, allowing it to serve numerous users/identities while logically isolating their Edge Agent assets. 
This segregation maintains data privacy and security, enhancing scalability and resource sharing within the SSI ecosystem.


## O
### OIDC
OIDC (OpenID Connect), is an authentication protocol built on top of OAuth 2.0. It enables secure user authentication and allows applications to obtain information about users, facilitating single sign-on (SSO) and identity verification in web and mobile applications.

### OID4VCI
[OID4VCI](https://openid.net/specs/openid-4-verifiable-credential-issuance-1_0.html) (OpenID for Verifiable Credential Issuance)
defines an authorization mechanism for issuing credentials using the OAuth2 protocol. It grants the holder access to credentials protected by the issuer's authorization server.

## P
### Peer DID
They are specialized DIDs for peer-to-peer relationships.

### Peer DID Method
A rich DID method that has no blockchain dependencies. The verifiable data registry is a synchronization protocol between peers. See the [Peer DID Method specification](https://github.com/decentralized-identity/peer-did-method-spec).

### Present Proof Protocol
The protocol provides endpoints for a Verifier to request new proof presentations from Holder/Provers and for a Holder/Prover to respond to the presentation request using a specific verifiable credential they own.

### Proof
A cryptographic mechanism that proves the information in a [verifiable credential](#verifiable-credential) or [verifiable presentation](#verifiable-presentation) has not been tampered with. Many types of cryptographic proofs include but are not limited to digital signatures, zero-knowledge proofs, Proofs of Work, and Proofs of Stake.

### Proof presentation
See [verifiable presentation](#verifiable-presentation). Also see [Present Proof Protocol](#present-proof-protocol).

### Protection API
The Protection API in User-Managed Access ([UMA](#uma)) is a set of endpoints that enables resource servers to enforce access policies and protect resources. It provides a mechanism for resource servers to interact with the authorization server to obtain necessary information and permissions, ensuring that access to user-managed resources aligns with the user's specified policies.

### Protection API
The Protection API in User-Managed Access ([UMA](#uma)) is a set of endpoints that enables resource servers to enforce access policies and protect resources. It provides a mechanism for resource servers to interact with the authorization server to obtain necessary information and permissions, ensuring that access to user-managed resources aligns with the user's specified policies.

### Protocol buffer
Also known as protobuf.

### Prism envelope
A response type for endpoints that implement prism anoncred method
```json
{"resource": <encoded resource as string>, url: <did url at which this resource can be resolved>}
```



## R
### Relay
A relay is an entity that passes along agent-to-agent messages depending on the sender's encryption choices. It does not decrypt anything. Relays can change the transport for a message (e.g., accept an HTTP POST, then turn around and emit an email, or accept a Bluetooth transmission, then turn around and transmit something in a message queue). Mix networks like TOR are a type of relay.
For a detailed overview of relays refer to the [RFC0046: Mediators and Relays](https://github.com/hyperledger/aries-rfcs/tree/main/concepts/0046-mediators-and-relays).

### Relying party
A party that depends on the authenticity of digital signatures.

### RPT
Requesting Party Token (RPT) is a concept within the [UMA](#uma) framework. It represents a token obtained by a client application from an authorization server, allowing the client to access protected resources on behalf of the requesting party (user), based on the user's policies and consent.


## S
### Secrets storage {#secrets-storage}
This component securely stores sensitive information, such as private keys associated with an individual's digital identity, Edge Agent seed, etc.
Secrets storage plays a crucial role in SSI implementations because it ensures that sensitive information is securely stored and protected against unauthorized access or disclosure

### Subject {#subject}
See [DID Subject](#did-subject)

### SSI {#ssi}
See [Self-Sovereign Identity](#self-sovereign-identity)

### Self-Sovereign Identity (SSI) {#self-sovereign-identity}
An identity model that shifts control to the edges, focused on security, privacy using public/private key encryption.



## T
### Tenant
A tenant is an individual user, organization, or entity that utilizes the SSI platform. 
Each tenant has its isolated area within the system, maintaining the separation of assets.

### Tenant Isolation
Tenant isolation is a core capability of the Identus platform, allowing it to serve numerous users/identities while logically isolating their Edge Agent assets.

### Tenant Management
Tenant management encompasses the processes and tools used to onboard, provision, and manage tenants within the SSI platform, including user registration, role assignment, authentication method configuring, and access permissions.

### Trust Framework
A governing body that establishes rules, requirements, establishes operating procedures, and a [trust registry](#trust-registry) for specific ecosystems.

### Trust Registry
A document that lists authorized issuers and verifiers established by the [Trust framework](#trust-framework).


## U
### UMA
User-Managed Access (UMA) is an authorization framework that allows users to control and manage access to their online resources. UMA enables individuals to share their digital assets with others while maintaining control over who can access the information and for what purposes.


## V
### Vault Service {#vault-service}
HashiCorp Vault is a widely used open-source and enterprise-grade solution designed for securely storing, accessing, and managing secrets and sensitive data in modern computing environments.
It offers a centralized platform for managing cryptographic keys, passwords, API keys, tokens, and other secrets.

### Verifiable Credential (VC) {#verifiable-credential}
A verifiable credential is a tamper-evident credential that contains one or more claims made by an issuer whose authorship can be cryptographically verified. It is possible to use VCs to create a [verifiable presentation](#verifiable-presentation). Also, the claims in a VC can be about different subjects.

### Verifiable Credentials (VCs) {#verifiable-credentials}
Synonym of the [Verifiable Credential (VC)](#verifiable-credential).

### Verifiable Data Registry {#verifiable-data-registry}
A system that mediates the creation and verification of identifiers, keys, and other relevant data.

### Verifiable Presentation {#verifiable-presentation}
Data is derived from one or more [verifiable credentials](#verifiable-credential), issued by issuers, and shared (presented) to a specific verifier. The verifiable presentation is tamper-evident and encoded in a way to trust the authorship of the data after a cryptographic verification. 

### Verifier
An entity that receives one or more [verifiable credentials](#verifiable-credential) optionally, inside a [verifiable presentation](#verifiable-presentation). Also known as a relying party.



## W
### Wallet SDK {#wallet-sdk}
A software development kit (SDK) that enables developers to build applications that interact with the Identus platform. 
The Wallet SDK provides a set of tools, libraries, and APIs that simplify the integration of SSI features, such as DID operations, verifiable credentials, and secure communications, into web and mobile applications.
Wallet SDK is much wider term than [Edge Agent SDK](#edge-agent-sdk), as it includes all the features of Edge Agent SDK and more.
</file>

<file path="documentation/learn/identity.md">
---
sidebar_position: 2
---

# Why Identity
Identity is about access. It is the key that unlocks doors we wish to enter. To stream movies, we need access to get into the virtual theater. Today, we do that by having an account with a streaming service, which authenticates us into the lobby. 

We need authorization to enter the theater to watch the movie, which requires a service plan. Which selection will determine whether we can watch in standard, high definition, or 4k.

This example is repeatable across all interactions: banking, insurance, online services, shopping, investing, education, traveling, driving, etc. Identity may be the most essential thing we undervalue in our lives. We use it to physically and digitally access goods and services locally and globally. 

## Self-Sovereign Identity (SSI)
[SSI](https://github.com/input-output-hk/atala-prism-docs/blob/doc-fixes/docs/concepts/glossary/#self-sovereign-identity) introduces new concepts that flip the existing identity models. The control shifts from the central authorities to the edges, with individuals. SSI is a set of principles that leverage decentralized identity technology. Sovrin compiled a list of the principles in an easy-to-digest format, available [here](https://sovrin.org/principles-of-ssi/). 

The World Wide Web Consortium (W3C) organization has been setting the standards for the Internet as we know it. Similarly, they are also working on next-generation technologies such as decentralized identity. In July 2022, the W3C approved the DID specification to become a [recommendation](https://www.w3.org/press-releases/2022/did-rec/).

The W3C has compiled a list in addition to the specifications for all DID methods available [here](https://www.w3.org/TR/did-spec-registries/).

For a deep dive into the DID specification itself, the W3C standard is [here](https://www.w3.org/TR/did-core/).
</file>

<file path="documentation/learn/README.md">
---
id: README
title: Introduction
sidebar_position: 1
sidebar_class_name: hidden-sidebar-item
---

# Introduction

Hyperledger Identus is a comprehensive toolset that provides the necessary infrastructure for decentralized identity systems. It operates as a layer-2 blockchain solution, using a distributed ledger as a [verifiable data registry (VDR)](./concepts/glossary#verifiable-data-registry). This innovative toolset facilitates the creation and management of decentralized identifiers (DIDs) and supports the issuance, updating, and revocation of verifiable credentials. With this technology, users can securely and efficiently manage their digital identities while ensuring the integrity and authenticity of their credentials.

Hyperledger Identus is deeply rooted in the concept of [self-sovereign identity (SSI)](./concepts/glossary#self-sovereign-identity), a powerful feature that empowers users to control their identity and personal information. With SSI, users can selectively disclose their data using asymmetric cryptography, giving them full control over their digital identity.

The documentation herein will explain the following:

* Decentralized identity concepts
* How Hyperledger Identus works
* Hyperledger Identus Quick Start Guide

## Supported Standards
* W3C DID Specification
* DIDComm v2
* W3C VC-JWT
* Hyperledger Anoncreds
* OpenID for Verifiable Credential Issuance
</file>

<file path="documentation/learn/sidebar.ts">
import type { SidebarsConfig } from '@docusaurus/plugin-content-docs';




const sidebar: SidebarsConfig[keyof SidebarsConfig] = [
    {
        type: 'doc',
        id: 'documentation/learn/README',
        className: 'hidden-sidebar-item',
    },
    {
        type: 'doc',
        id: 'documentation/learn/identity',
    },
    {
        type: 'category',
        label: 'Components',
        link: {
            type: 'generated-index',
            title: 'Cloud Agent',
            description: 'Cloud Agent'
        },
        items: [
            {
                type: 'autogenerated',
                dirName: 'documentation/learn/components',
            },

        ]
    },
    {
        type: 'doc',
        id: 'documentation/learn/glossary',
    },
]

export default sidebar
</file>

<file path="documentation/reference/adrs/_category_.json">
{
    "label": "Architecture decisions",
    "position": 1,
    "collapsed": true,
    "link": {
      "type": "doc",
      "id": "README"
    }
  }
</file>

<file path="documentation/reference/adrs/README.md">
---
id: README
title: Reference
sidebar_position: 1
---
<!-- This file is the homepage of your Log4brains knowledge base. You are free to edit it as you want -->

# Architecture knowledge base

Welcome 👋 to the architecture knowledge base of the Identus platform.

You will find here all the Architecture Decision Records (ADR) of the project.

The introduction of ADRs was approved in [RFC-0016](https://input-output.atlassian.net/wiki/spaces/ATB/pages/3580559403/RFC+0016+-+Use+Architectural+Design+Records)

Engeering guidance on creating and managing ADRs can be found [here](https://input-output.atlassian.net/wiki/spaces/AV2/pages/3599237263/Architectural+Decision+Records+ADRs)

## Definition and purpose

> An Architectural Decision (AD) is a software design choice that addresses a functional or non-functional requirement that is architecturally significant.
> An Architectural Decision Record (ADR) captures a single AD, such as often done when writing personal notes or meeting minutes; the collection of ADRs created and maintained in a project constitutes its decision log.

An ADR is immutable: only its status can change (i.e. become deprecated or superseded). That way, you can become familiar with the whole project history just by reading its decision log in chronological order.
Moreover, maintaining this documentation aims at:

- 🚀 Improving and speeding up the onboarding of a new team member
- 🔭 Avoiding blind acceptance/reversal of a past decision (cf [Michael Nygard's famous article on ADRs](https://cognitect.com/blog/2011/11/15/documenting-architecture-decisions.html))
- 🤝 Formalizing the decision process of the team

## Usage

This website is automatically updated after a change on the `main` branch of the project's Git repository.
In fact, the developers manage this documentation directly with markdown files located next to their code, so it is more convenient for them to keep it up-to-date.
You can browse the ADRs by using the left menu or the search bar.

## More information

- [RFC-0016](https://input-output.atlassian.net/wiki/spaces/ATB/pages/3580559403/RFC+0016+-+Use+Architectural+Design+Records)
- [Engineering Guidance](https://input-output.atlassian.net/wiki/spaces/AV2/pages/3599237263/Architectural+Decision+Records+ADRs)
- [Log4brains documentation](https://github.com/thomvaill/log4brains/tree/master#readme)
- [What is an ADR and why should you use them](https://github.com/thomvaill/log4brains/tree/master#-what-is-an-adr-and-why-should-you-use-them)
- [ADR GitHub organization](https://adr.github.io/)
</file>

<file path="documentation/reference/_category_.json">
{
    "label": "Reference",
    "position": 2,
    "collapsed": false,
    "link": {
      "type": "generated-index",
      "title": "Reference"
    }
  }
</file>

<file path="documentation/reference/specifications.md">
# Specifications

## Legend

- ✅ - Supported
- ❌ - Not supported
- 🚫 - Not applicable
- 🔄 - In progress
- ❓ - Not sure
- 🚧 - Under construction

## Mediator

|    | Specification                                                                                | Purpose                                                                                                                                      |
|----|----------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------|
| ✅  | [DIDComm Messaging v2.x Editor's Draft](https://identity.foundation/didcomm-messaging/spec/) | The core DIDCommV2 specification                                                                                                             |
| ✅  | [Peer DID 1.0](https://identity.foundation/peer-did-method-spec/)                            | ❓Peer DID specification, did:peer:2 is currently used                                                                                        |
| ✅  | [BasicMessage 2.0](https://didcomm.org/basicmessage/2.0)                                     | The BasicMessage protocol describes a stateless, easy to support user message protocol                                                       |
| ✅  | [Coordinate Mediation 2.0](https://didcomm.org/coordinate-mediation/2.0/)                    | The protocol to coordinate mediation configuration between a mediating agent and the recipient                                               |
| 🚧 | [Coordinate Mediation 3.0](https://didcomm.org/coordinate-mediation/3.0/) *(TODO)*           | The protocol to coordinate mediation configuration between a mediating agent and the recipient                                               |
| ✅  | [Message Pickup 3.0](https://didcomm.org/messagepickup/3.0/)                                 | The protocol to facilitate an agent picking up messages held at a mediator                                                                   |
| ✅  | [Trust Ping 2.0](https://didcomm.org/trust-ping/2.0/)                                        | The trust-ping protocol defined in the DIDComm Messaging Spec. This enables the sender and recipient to engage in an exchange of trust pings |
| ✅  | [Report Problem 2.0](https://didcomm.org/report-problem/2.0/)                                | The Report Problem protocol is defined in the DIDComm Messaging Spec. This protocol describes sending a problem report to another party      |

## Cloud Agent (CA) and SDKs TypeScript(TS)/Swift(SW)/Kotlin Multiplatform(KM)

| CA | TS | SW | KM | Specification                                                                                                                                                                                                                              | Purpose                                                                                                                              |  
|----|----|----|----|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------|
| ✅  | ✅  | ✅  | ✅  | [Decentralized Identifiers (DIDs) v1.0](https://www.w3.org/TR/did-1.0/)                                                                                                                                                                    | Core specification for DIDs                                                                                                          |
| ✅  | ✅  | ✅  | ✅  | [PRISM DID Method Specification - did:prism](https://github.com/input-output-hk/prism-did-method-spec/blob/main/w3c-spec/PRISM-method.md)                                                                                                  | PRISM DID Method specification                                                                                                       |
| ✅  | ✅  | ✅  | ✅  | [Peer DID 1.0](https://identity.foundation/peer-did-method-spec/)                                                                                                                                                                          | Peer DID specification. Partually suported (did:peer:2 is fully supported)                                                           |
| ✅  | ✅  | ✅  | ✅  | [Verifiable Credentials JSON Schema Specification](https://www.w3.org/TR/vc-json-schema/)                                                                                                                                                  | JSON Schemas for Verifiable Credentials                                                                                              |                                                                            
| ✅  | ✅  | ✅  | ✅  | [Securing Verifiable Credentials using JOSE and COSE](https://www.w3.org/TR/vc-jose-cose/)                                                                                                                                                 | Core specification for VC-JWT                                                                                                        |
| ✅  | ✅  | ✅  | ✅  | [JSON Object Signing and Encryption (JOSE)](https://www.iana.org/assignments/jose/jose.xhtml)                                                                                                                                              | JOSE registry of headers, curves, keys, signature and encryption algorithms                                                          |
| ✅  | ❌  | ❌  | ❌  | [Selective Disclosure for JWTs (SD-JWT)](https://datatracker.ietf.org/doc/draft-ietf-oauth-selective-disclosure-jwt/07/)                                                                                                                   | Core specification for SD-JWT (not SD-JWT-VC). Outdated, the latest is v14                                                           |
| ❌  | ✅  | ✅  | ✅  | [SD-JWT-based Verifiable Credentials (SD-JWT VC)](https://datatracker.ietf.org/doc/draft-ietf-oauth-sd-jwt-vc/)                                                                                                                            | Core specification for SD-JWT (not SD-JWT-VC). Outdated, the latest is v14                                                           |
| ✅  | ✅  | ✅  | ✅  | [AnonCreds Specification v1.0](https://hyperledger.github.io/anoncreds-spec/)                                                                                                                                                              | Core specification for AnonCreds                                                                                                     |
| ✅  | ✅  | ✅  | ✅  | [DID:PRISM AnonCreds Method](https://hyperledger.github.io/anoncreds-methods-registry/#didprism-anoncreds-method)                                                                                                                          | DID:PRISM AnonCreds Method for Schema and Credential Definition                                                                      |
| ✅  | ✅  | ✅  | ✅  | [HTTP AnonCreds Method](https://hyperledger.github.io/anoncreds-methods-registry/#http-anoncreds-method)                                                                                                                                   | HTTP AnonCreds Method for Schema and Credential Definition                                                                           |
| ✅  | ✅  | ✅  | ✅  | [Bitstring Status List v1.0](https://www.w3.org/TR/vc-bitstring-status-list/)                                                                                                                                                              | Core specification for VC-JWT                                                                                                        |
| ❌  | ❌  | ❌  | ❌  | [DIF Presentation Exchange 2.x.x](https://identity.foundation/presentation-exchange)                                                                                                                                                       | DIF Presentation Exchange protocol                                                                                                   |
| ✅  | ✅️ | ✅️ | ✅️ | [Out of Band Protocol 2.0](https://identity.foundation/didcomm-messaging/spec/#out-of-band-messages)                                                                                                                                       | Out of Band messages for DIDComm (part of DIDCommV2 specification)                                                                   |                                                                                          |
| 🚫 | ✅  | ✅  | ✅  | [Coordinate Mediation Protocol 2.0](https://didcomm.org/coordinate-mediation/2.0/)                                                                                                                                                         | Coordinate Mediation Protocol for DIDCommV2                                                                                          |
| ✅  | ✅️ | ✅️ | ✅️ | [Connection Protocol 1.0](https://github.com/hyperledger-identus/cloud-agent/blob/main/mercury/protocol-connection/Connection-Protocol.md)                                                                                                 | The protocol is used when you wish to create a connection with another agent                                                         |
| ✅  | ❓️ | ❓️ | ❓️ | [Aries RFC 0023: DID Exchange v1](https://github.com/hyperledger/aries-rfcs/tree/main/features/0023-did-exchange)                                                                                                                          | The protocol to exchange DIDs between agents when establishing a DID based relationship                                              |
| ✅  | ❓️ | ❓️ | ❓️ | [Aries RFC 0434: Out-of-Band Protocol 1.1](https://github.com/hyperledger/aries-rfcs/blob/main/features/0434-outofband/README.md), same as<br/> [DIDCommV2 Invitation 1.0](https://identity.foundation/didcomm-messaging/spec/#invitation) | Out-of-Band Protocol for DIDComm (part of DIDCommV2 specification)                                                                   |
| ✅  | ❌  | ❌  | ❌  | [Trust Ping Protocol 2.0](https://didcomm.org/trust-ping/2.0/)                                                                                                                                                                             | The protocol is a standard way for agents to test connectivity, responsiveness, and security of a DIDComm channel                    |
| ✅  | ❌  | ❌  | ❌  | [Routing Protocol 2.0](https://identity.foundation/didcomm-messaging/spec/#routing-protocol-20)                                                                                                                                            | The routing protocol defines how a sender and a recipient cooperate, using a partly trusted mediator, to facilitate message delivery |
| ✅  | ✅  | ✅  | ✅  | [Issue Credential Protocol 3.0](https://github.com/decentralized-identity/waci-presentation-exchange/blob/main/issue_credential/README.md)                                                                                                 | DIDCommV2 Issue Credential Protocol 3.0 for VC-JWT, AnonCreds and SD-JWT                                                             |
| ✅  | ✅  | ✅  | ✅  | [Present Proof Protocol 3.0](https://github.com/decentralized-identity/waci-didcomm/blob/main/present_proof/present-proof-v3.md)                                                                                                           | DIDCommV2 Present Proof Protocol 3.0 for VC-JWT, AnonCreds and SD-JWT                                                                |
| ✅  | ✅  | ✅  | ✅  | [Identus Revocation Notification Protocol 1.0](https://github.com/hyperledger-identus/cloud-agent/blob/main/mercury/protocol-revocation-notification/Revocation-notification-protocol.md)                                                  | The protocol for an Issuer to notify the revocation of a credential to the holder.                                                   |
| ✅  | ✅  | ✅  | ✅  | [Aries RFC 0035: Report Problem Protocol 1.0](https://github.com/hyperledger/aries-rfcs/blob/main/features/0035-report-problem/README.md)                                                                                                  | Report Problem Protocol for DIDCommV2                                                                                                |
| 🔄 | 🔄 | 🔄 | 🔄 | [OpenID for Verifiable Credential Issuance - draft 15](https://openid.net/specs/openid-4-verifiable-credential-issuance-1_0.html)                                                                                                          | OpenID Connect for VC Issuance (VC-JWT is supported only). Client side.                                                              |
| 🔄 | 🔄 | 🔄 | 🚧 | [OpenID for Verifiable Credential Presentation - draft 15](https://openid.net/specs/openid-4-verifiable-credential-presentation-1_0.html)                                                                                                  | OpenID Connect for VC Presentation (VC-JWT is supported only). Client side.                                                          |
</file>

<file path="src/components/atala-graphic/index.module.css">
:root {
    --graphic-height: auto;
}

.hero__graphic__wrapper {
    position: relative;
    margin-left: auto;
    margin-right: auto;
    z-index: 100;
    transform: translateX(-5%) translateY(-60%);
}

.hero__graphic {
    position: relative;
    left: 0;
    right: auto;
    width: clamp(16.25rem, 40vw, 32rem);
    height: clamp(16.25rem, 40vw, 32rem);
    overflow: hidden;
    margin-right: auto;
    mask-image: url("/static/img/graphics/identus-hero.svg");
    mask-size: 100% 100%;
    color: var(--ifm-color-primary);
    font-size: 14px;
    font-family: 'Space Mono', monospace;
}

@media screen and (min-width: 600px) {
    .hero__graphic {
        margin-left: auto;
        right: 0;
        left: auto;

    }

    .hero__graphic__wrapper {
        transform: translateX(25%);
    }
}
</file>

<file path="src/components/features/styles.module.css">
.features {
  display: flex;
  align-items: center;
  width: 100%;
}

.features a {
  color: inherit;
  text-decoration: underline;
}

.features p {
  color: var(--ifm-resource-cards-text-color);
}

.featureSvg {
  height: 97.5px;
  width: 100px;
}
</file>

<file path="src/config/multiDocPreset.ts">
import { PluginOptions } from "@docusaurus/types";
import type { Options as ThemeOptions } from '@docusaurus/theme-classic';

type Opts = {
    docs: PluginOptions[],
    theme?: ThemeOptions
}

const CONTENT_DOCS_PLUGIN = '@docusaurus/plugin-content-docs';
const CONTENT_OPENAPI_PLUGIN = 'docusaurus-plugin-openapi-docs';


export default function preset(context, opts: Opts) {
    const { docs } = opts;
    return  {
        themes: [
            ['@docusaurus/theme-classic', opts.theme],
            '@docusaurus/theme-mermaid',
            'docusaurus-theme-openapi-docs',
        ],
        plugins: [
            [
                '@docusaurus/plugin-svgr',
                {
                  svgrConfig: {
                    /* SVGR config */
                  },
                },
              ],
            '@docusaurus/plugin-content-pages',
            ...docs.map(contentConfig => ['docsPluginId' in contentConfig ? CONTENT_OPENAPI_PLUGIN : CONTENT_DOCS_PLUGIN, contentConfig]),
        ],
    };
}
</file>

<file path="src/css/custom.css">
@import url('https://fonts.googleapis.com/css2?family=Poppins:wght@400;500;600&family=Space+Mono&display=swap');
@font-face {
    font-family: 'Poppins';
    src: url('/static/font/Poppins-Regular.woff2') format('woff2'),
    url('/static/font/Poppins-Regular.woff') format('woff'),
    url('/static/font/Poppins-Regular.ttf') format('truetype');
    font-weight: normal;
    font-style: normal;
    font-display: swap;
}

@font-face {
    font-family: 'Poppins';
    src: url('/static/font/Poppins-Bold.woff2') format('woff2'),
    url('/static/font/Poppins-Bold.woff') format('woff'),
    url('/static/font/Poppins-Bold.ttf') format('truetype');
    font-weight: bold;
    font-style: normal;
    font-display: swap;
}

@font-face {
    font-family: 'Poppins';
    src: url('/static/font/Poppins-SemiBold.woff2') format('woff2'),
    url('/static/font/Poppins-SemiBold.woff') format('woff'),
    url('/static/font/Poppins-SemiBold.ttf') format('truetype');
    font-weight: 600;
    font-style: normal;
    font-display: swap;
}

@font-face {
    font-family: 'Poppins';
    src: url('/static/font/Poppins-Medium.woff2') format('woff2'),
    url('/static/font/Poppins-Medium.woff') format('woff'),
    url('/static/font/Poppins-Medium.ttf') format('truetype');
    font-weight: 500;
    font-style: normal;
    font-display: swap;
}

@font-face {
    font-family: 'Poppins';
    src: url('/static/font/Poppins-SemiBoldItalic.woff2') format('woff2'),
    url('/static/font/Poppins-SemiBoldItalic.woff') format('woff'),
    url('/static/font/Poppins-SemiBoldItalic.ttf') format('truetype');
    font-weight: 600;
    font-style: italic;
    font-display: swap;
}

html[data-theme="dark"] {
    --ifm-redoc-arrow-color: #fff;
    --ifm-background-color: #101828;
    --redocusaurus-h3-color: #fff;
    --ifm-color-primary: var(--ifm-color-primary-light);
    --ifm-footer-background-color: #1D2939;
    --ifm-side-pannel-background-color: transparent;
    --ifm-resource-cards-background-color: var(--ifm-footer-background-color);
    --ifm-resource-cards-text-color: #fff;
    --ifm-resource-card-getstarted-background: linear-gradient(45deg, #101828, #475467);
    --ifm-navbar-background-color: rgba(16, 24, 40, .7);
    --ifm-dropdown-background-color: var(--ifm-color-gray-800);
    --ifm-card-background-color: var(--ifm-color-gray-800);
    --ifm-hero-background-color: transparent;
    --ifm-hero-text-color: #fff;
    --ifm-resource-border-color: var(--ifm-color-gray-600);
    --ifm-blob-background: linear-gradient(to right, var(--ifm-color-primary), var(--ifm-color-primary-dark));
    --ifm-footer-hr-color: var(--ifm-color-gray-500);
    --ifm-color-resources-intro: #fff;
    --ifm-navbar-search-input-background-color: var(--ifm-color-gray-800);
    --ifm-navbar-search-input-icon: url("/static/img/search-icon-light.svg");
    --ifm-navbar-search-input-border-color: var(--ifm-color-gray-600);
}

:root {
    --ifm-redoc-arrow-color: rgba(0,0,0,.5);
    --ifm-resource-border-color: transparent;
    --ifm-side-pannel-background-color: var(--ifm-color-gray-200);
    --redocusaurus-h3-color: var(--ifm-color-gray-900);
    --ifm-blob-background: linear-gradient(to right, var(--ifm-color-primary), var(--ifm-color-primary-dark));
    --ifm-hero-background-color: transparent;
    --ifm-color-resources-intro: var(--ifm-color-gray-900);
    --ifm-hero-text-color: #000;
    --ifm-color-primary: #5559F2;
    --ifm-color-primary-dark: #3038EC;
    --ifm-color-primary-darker: #2C2FE0;
    --ifm-color-primary-darkest: #2121D4;
    --ifm-color-primary-light: #767AF5;
    --ifm-color-primary-lighter: #A0A0f7;
    --ifm-color-primary-lightest: #C7C5FA;
    --ifm-footer-background-color: #F9FAFB;
    --ifm-code-font-size: 95%;
    --ifm-heading-font-weight: 600;
    --ifm-font-family-base: "Poppins", "sans-serif";
    --ifm-color-gray-50: #F9FAFB;
    --ifm-color-gray-600: #475467;
    --ifm-color-gray-200: #EAECF0;
    --ifm-color-gray-300: #D0D5DD;
    --ifm-color-gray-500: #667085;
    --ifm-color-gray-900: #101828;
    --ifm-color-gray-800: #1D2939;
    --ifm-color-gray-700: #344054;
    --ifm-color-teal-300: #5FE9D0;
    --ifm-color-teal-400: #2ED3B7;
    --ifm-container-width: 1216px;
    --ifm-container-width-xl: 1216px;
    --ifm-resource-cards-background-color: var(--ifm-color-gray-50);
    --ifm-resource-cards-text-color: var(--ifm-color-gray-600);
    --ifm-resource-card-getstarted-background: linear-gradient(45deg, var(--ifm-color-primary-lightest), var(--ifm-color-gray-50));
    --ifm-navbar-background-color: rgba(255, 255, 255, .9);
    --ifm-footer-hr-color: var(--ifm-color-gray-200);
    --ifm-navbar-search-input-background-color: #fff;
    --ifm-navbar-search-input-icon: url("/static/img/search-icon-dark.svg");
    --ifm-navbar-search-input-border-color: var(--ifm-color-gray-300);
}
.navbar__inner {
    max-width: var(--ifm-container-width);
    margin: 0 auto;
    backdrop-filter: blur(16px);
    padding: var(--ifm-navbar-padding-vertical) var(--ifm-navbar-padding-horizontal);
}

.navbar__search-input {
    border: 1px solid var(--ifm-navbar-search-input-border-color);
}

@media screen and (min-width: 1440px) {
    .navbar__inner {
        max-width: var(--ifm-container-width-xl);
    }
}
.navbar--fixed-top {
    position: sticky;
    top: 0;
    padding: 0;
    z-index: var(--ifm-z-index-fixed);
    background-color: var(--ifm-navbar-background-color);
}

.navbar__link--active {
    font-weight: 600;
    text-decoration: none;
}

.docusaurus-highlight-code-line {
    background-color: rgba(0, 0, 0, 0.1);
    display: block;
    margin: 0 calc(-1 * var(--ifm-pre-padding));
    padding: 0 var(--ifm-pre-padding);
}

html[data-theme='dark'] .docusaurus-highlight-code-line {
    background-color: rgba(0, 0, 0, 0.3);
}

.navbar__search {
    padding-left: var(--ifm-navbar-padding-horizontal);
    display: none;
}

.navbar__items {
    flex-direction: row;
    justify-content: space-between;
    width: 100%;
    flex: unset;
}

.navbar__brand {
    /*justify-self: flex-end;*/
    /*margin-left: auto;*/
    /*display: block;*/
    order: 1;
}

.menu__list-item {
    margin: 1rem 0;
}

.navbar__toggle {
    order: 2;
    margin-right: 0;
}
.navbar-sidebar__brand {
    display: flex;
    justify-content: space-between;
}
.navbar-sidebar__brand .navbar__brand {
    order: 1;
    margin: 0;
    display: none;
}
.navbar-sidebar__backdrop {
    background: transparent;
}
.navbar-sidebar__brand .toggle_node_modules-\@docusaurus-theme-classic-lib-theme-ColorModeToggle-styles-module {
    margin: 0 !important;
    order: 2;
}

.navbar-sidebar__brand .navbar-sidebar__close {
    order: 3;
    margin-left: 0;
}
.navbar-sidebar {
    left: auto;
    right: 0;
    transform: translate3d(100%, 0, 0);
    backdrop-filter: blur(16px);
}

@media screen and (min-width: 996px) {
    .navbar__brand {
        order: unset;
    }

    .navbar__toggle {
        order: unset;
    }
    .navbar__items {
        justify-content: flex-start;
        width: auto;
    }
    .navbar__search {
        display: inline-block;
    }

    .navbar__inner {
        padding: 0 var(--ifm-spacing-horizontal);
    }
}

.version-number {
    color: var(--ifm-link-color);
}


.redocusaurus {
    font-family: Poppins, sans-serif;
}

.redocusaurus .menu-content {
    background: transparent;
}

.menu-content {
    top: var(--ifm-navbar-height) !important;
    padding: 0 .5rem;
}

.menu-content ul li label[type="tag"][role="menuitem"]:hover {
    color: inherit;
}

.menu-content ul li label[type="tag"][role="menuitem"] polygon {
    fill: var(--ifm-redoc-arrow-color);
}

.menu-content ul li label[type="tag"][role="menuitem"] {
    padding: var(--ifm-menu-link-padding-vertical) var(--ifm-menu-link-padding-horizontal);
    font-weight: var(--ifm-font-weight-semibold);
    transition: background var(--ifm-transition-fast) var(--ifm-transition-timing-default);
    border-radius: 0.25rem;
}

.menu-content ul li label[type="tag"][role="menuitem"].active {
    background: var(--ifm-menu-color-background-active);
    color: var(--ifm-menu-color-active);
}
.menu-content ul li label[type="tag"][role="menuitem"]:not(.active) {
    color: var(--ifm-menu-color);
}

.menu-content ul[role="menu"] li {
    margin: 1rem 0;
}
/*.redocusaurus div:first-child > div:nth-child(4) {*/
/*    background: var(--ifm-side-pannel-background-color);*/
/*}*/


.http-verb,
.react-tabs__tab-list li,
.operation-type {
    border-radius: 16px !important;
}

.redocusaurus h3 {
    margin: 1.5rem 0;
    color: var(--redocusaurus-h3-color) !important;
}

.react-tabs__tab-panel  {
    margin: 1.5rem 0;
}

.navbar__search-input {
    font-family: 'POPPINS', sans-serif;
}

button[aria-expanded] {
    border-radius: 10px;
}

/* Hide specific sidebar items */
.hidden-sidebar-item {
    display: none !important;
}
</file>

<file path="src/pages/index.module.css">
/**
 * CSS files with the .module.css suffix will be treated as CSS modules
 * and scoped locally.
 */

@media screen and (min-width: 600px) {
  .navbar--fixed-top {
    position: fixed;
    backdrop-filter: blur(24px);
  }
}

.heroBanner {
  overflow: hidden;
  height: 40vh;
}

.container {
  position: relative;
  height: fit-content;
  width: 100%;
  display: flex;
  align-items: center;
}

.hero__tagline {
  display: inline-block;
  margin-bottom: .5rem;
  font-size: 18px;
  color: var(--ifm-color-primary);
  font-weight: var(--ifm-heading-font-weight);
}

.hero__title {
  color: var(--ifm-hero-text-color);
  font-weight: 500;
  font-size: clamp(2rem, 5vw, 4rem);
}

@media screen and (max-width: 996px) {
  .heroBanner {
    padding: 2rem;
  }
}

.hero__content {
  max-width: 635px;
  position: absolute;
  z-index: 10;
  margin-top: -3rem;
}


.hero__button {
  margin-top: 1.5rem;
  cursor: pointer;
  border-radius: 5rem;
  background-color: var(--ifm-color-primary);
  padding: .7rem 1.5rem;
  color: #fff;
  font-weight: 500;
}

@media screen and (min-width: 600px) {
  .heroBanner {
    height: auto;
  }

  .hero__content {
    height: auto;
  }
}


@media screen and (min-height: 667px) {
  .hero__content {
    top: clamp(5.125rem, 100vw, 9.125rem);
  }
}
</file>

<file path="src/utils/index.ts">
import type { SidebarItemConfig } from '@docusaurus/plugin-content-docs/src/sidebars/types.js';

/**
 * This function builds a sidebar from a typedoc documentation exported markdown documents
 */
export function buildTypeDocCategorySidebar(path: string, label: string): SidebarItemConfig[] {

    const elements = [
        'Classes',
        'Enumerations',
        'Functions',
        'Interfaces',
        'Namespaces',
        'Type Aliases',
        'Variables',
    ];
    const items = elements.map((element) => {
        return {
            type: 'category',
            label: element,
            link: {
                type: 'generated-index',
                title: element,
                description: element
            },
            items: [
                {
                    type: 'autogenerated',
                    dirName: `${path}/${element.toLowerCase()}`,
                },
            ]
        } as SidebarItemConfig
    });

    return [
        {
            type: 'category',
            label,
            link: {
                type: 'doc',
                id: path + '/README',

            },
            items
        }
    ]
}
</file>

<file path="docusaurus.config.ts">
import { themes as prismThemes } from 'prism-react-renderer';
import type { Config } from '@docusaurus/types';
import type * as Preset from '@docusaurus/preset-classic';


import { headerMenu } from './src/config/headerMenu';
import { presets } from './src/config/presets';

const config: Config = {
    title: 'Hyperledger Identus',
    tagline: 'Hyperledger Identus Docs',
    url: 'https://hyperledger-identus.github.io/',
    baseUrl: '/',
    onBrokenLinks: 'throw',
    onBrokenMarkdownLinks: 'warn',
    favicon: 'img/favicon.ico',
    organizationName: 'hyperledger-identus',
    projectName: 'docs',
    markdown: {
        mermaid: true,
        
    },
    i18n: {
        defaultLocale: 'en',
        locales: ['en'],
    },
    presets,
    plugins: [
        require.resolve('docusaurus-lunr-search')
    ],
    themeConfig: {
        navbar: {
            logo: {
                alt: ' Identus logo',
                src: 'img/identus-navbar-light.png',
                srcDark: "img/identus-navbar-light.png",
            },
            items: headerMenu,
        },
        footer: {
            copyright: `Hyperledger Identus CC BY 4.0`,
        },
        prism: {
            theme: prismThemes.github,
            darkTheme: prismThemes.dracula,
        },
    } satisfies Preset.ThemeConfig,

};

export default config;
</file>

<file path="sidebars.ts">
import type { SidebarsConfig } from '@docusaurus/plugin-content-docs';

import learnSidebar from './documentation/learn/sidebar';
import developersSidebar from './documentation/developers/sidebar';
import referenceSidebar from './documentation/reference/sidebar';

const sidebars: SidebarsConfig = {
  learnSidebar,
  developersSidebar,
  referenceSidebar,
};

export default sidebars;
</file>

<file path="documentation/developers/README.md">
---
id: README
title: Developers
sidebar_position: 1
sidebar_class_name: hidden-sidebar-item
---

# Developer Documentation

Welcome to the Hyperledger Identus developer documentation. This comprehensive guide provides everything you need to build decentralized identity applications using the Identus platform.

## Getting Started

New to Identus? Start here to understand the fundamentals and get your development environment up and running.

### 🚀 [Quick Start Guide](./quick-start.md)

Follow our comprehensive quick start guide to:

- Deploy Issuer and Verifier Cloud Agents
- Set up Wallet SDKs (TypeScript, Swift, or Kotlin)
- Deploy and connect to a Mediator
- Create DIDs and credential schemas
- Issue your first verifiable credential
- Perform credential verification

**Perfect for:** Developers new to Identus or SSI who want to see the full flow in action.

## Developer Resources

### 📱 SDKs

Build wallet applications for web and mobile platforms using our comprehensive SDKs:

#### [TypeScript SDK](../../sdk-ts/docs/sdk/README.md)

Build browser and Node.js applications with full support for:
- DIDComm messaging
- Credential issuance and presentation
- AnonCreds, SD-JWT, and JWT credentials
- OpenID for Verifiable Credentials
- Backup and recovery

**Tutorials available:**
- [Storage with Pluto](../../sdk-ts/docs/pluto/README.md)
- [PRISM DID Management](../../sdk-ts/docs/prism/what-is-did-prism.md)
- [Connectionless & Out-of-Band](../../sdk-ts/docs/examples/connectionless/ConnectionlessOffer.md)
- [SDK Verification](../../sdk-ts/docs/examples/SDKVerification.md)
- [Migration Guides](/category/migration-guides)

#### [Swift SDK](https://hyperledger-identus.github.io/sdk-swift/documentation/edgeagentsdk/)

Native iOS SDK for building wallet applications with Swift. Includes sample applications and complete API reference.

#### [Kotlin Multiplatform SDK](https://hyperledger-identus.github.io/sdk-kmp/)

Cross-platform SDK for Android, iOS, and JVM applications. Share code across platforms while maintaining native performance.

### ☁️ Cloud Agent

The Cloud Agent provides REST API endpoints for enterprise SSI operations. It can act as an Issuer, Holder, or Verifier.

#### [Tutorials](../../cloud-agent/docs/docusaurus/index.md)

Step-by-step guides covering:

- **Connections** - Establish DIDComm connections between agents
- **DIDs** - Create, publish, update, and deactivate DIDs
- **Schemas** - Define and manage credential schemas
- **Credential Definitions** - Set up AnonCreds credential definitions
- **Credential Issuance** - Issue credentials via DIDComm and OID4VCI
- **Credential Verification** - Request and verify presentations
- **Multi-tenancy** - Manage multiple tenants and access control
- **Webhooks** - Subscribe to agent events
- **Secrets Management** - Secure key and secret storage

#### Cloud Agent Configuration

Essential guides for deploying and configuring the Cloud Agent:

- **[Building Blocks](./cloud-agent/building-blocks.md)** - Understanding Apollo, Castor, Pollux, and Mercury modules
- **[Authentication](./cloud-agent/authentication.md)** - API key and JWT authentication setup
- **[DID Management](./cloud-agent/did-management.md)** - Managing DIDs in the Cloud Agent
- **[Environment Variables](./cloud-agent/environment-variables.md)** - Complete configuration reference
- **[Secrets Storage](./cloud-agent/secrets-storage.md)** - Vault and database secret management
- **[VDR Configuration](./cloud-agent/vdr.md)** - Verifiable Data Registry setup
- **[Troubleshooting](./cloud-agent/troubleshooting&considerations.md)** - Common issues and considerations

## Core Concepts

Before diving deep, familiarize yourself with these essential concepts:

- **[Self-Sovereign Identity (SSI)](../learn/glossary.md#self-sovereign-identity)** - User-controlled digital identity
- **[Decentralized Identifiers (DIDs)](../learn/glossary.md#decentralized-identifier)** - Globally unique identifiers you control
- **[Verifiable Credentials (VCs)](../learn/glossary.md#verifiable-credential)** - Digital credentials you can prove are authentic
- **[DIDComm](../learn/glossary.md#didcomm)** - Secure, private messaging protocol for DIDs
- **[Trust Triangle](../learn/)** - The relationship between Issuers, Holders, and Verifiers

[Learn more about Identus concepts →](../learn/README.md)

## Architecture & Building Blocks

Identus provides modular building blocks that can be combined for various use cases:

| Building Block | Code Name | Purpose |
|----------------|-----------|---------|
| **Cryptography** | Apollo | Cryptographic primitives for data integrity and security |
| **DIDs** | Castor | Create, manage, and resolve decentralized identifiers |
| **Verifiable Credentials** | Pollux | Issue, manage, and verify credentials |
| **DIDComm** | Mercury | Secure peer-to-peer messaging protocols |

This modular architecture provides flexibility to customize solutions for specific requirements.

## Development Workflows

### Credential Issuance Flow

1. **Issuer** creates a credential schema
2. **Issuer** publishes a DID (for verification)
3. **Issuer** sends credential offer to **Holder**
4. **Holder** accepts and creates credential request
5. **Issuer** issues credential to **Holder**
6. **Holder** stores credential securely

### Credential Verification Flow

1. **Verifier** sends presentation request to **Holder**
2. **Holder** selects matching credentials
3. **Holder** creates and sends presentation to **Verifier**
4. **Verifier** validates credential authenticity and issuer
5. **Verifier** checks credential status (not revoked)

### Connection Establishment

Agents establish secure DIDComm connections using:
- **Direct invitations** - For known parties with existing connections
- **Out-of-Band (OOB) invitations** - QR codes or deep links for new connections
- **Mediator routing** - Message delivery for offline/mobile wallets

## Standards Support

Identus implements current W3C and DIF standards:

- ✅ W3C DID Core 1.0
- ✅ W3C Verifiable Credentials with JWT/SD-JWT
- ✅ DIDComm Messaging v2
- ✅ Hyperledger AnonCreds v1.0
- ✅ OpenID for Verifiable Credential Issuance (OID4VCI)
- ✅ OpenID for Verifiable Presentations (OID4VP)
- ✅ W3C Bitstring Status List v1.0
- ✅ DIF Presentation Exchange

[View complete specifications →](../reference/specifications.md)

## API Reference

- **[Cloud Agent API](../reference/Cloud%20Agent%20API/identus-cloud-agent-api-reference.info.mdx)** - Complete OpenAPI specification
- **[TypeScript SDK API](../../sdk-ts/docs/sdk/)** - Auto-generated API documentation
- **[Swift SDK API](https://hyperledger-identus.github.io/sdk-swift/documentation/edgeagentsdk/)**
- **[Kotlin SDK API](https://hyperledger-identus.github.io/sdk-kmp/)**

## Example Applications

All SDKs include sample applications demonstrating end-to-end flows:

- **TypeScript**: React demo at `sdk-ts/demos/next`
- **Swift**: Wallet Demo in `identus-edge-agent-sdk-swift/Sample`
- **Kotlin**: SampleApp in `identus-edge-agent-sdk-kmm`

## Community & Support

- **[GitHub Discussions](https://github.com/hyperledger/identus)** - Ask questions and share ideas
- **[Issue Tracker](https://github.com/hyperledger/identus-cloud-agent/issues)** - Report bugs and request features
- **[Discord](https://discord.gg/hyperledger)** - Real-time community chat
- **[Contributing Guidelines](../../CONTRIBUTING.md)** - How to contribute to Identus

## Additional Resources

- **[Architecture Decision Records](../reference/adrs/README.md)** - Understand key architectural decisions
- **[PRISM DID Specification](https://github.com/input-output-hk/prism-did-method-spec/)** - Deep dive into did:prism
- **[Security Policy](../../SECURITY.md)** - Security best practices and reporting

---

Ready to start building? Head to the [Quick Start Guide](./quick-start.md) to deploy your first Identus application!
</file>

<file path="documentation/developers/sidebar.ts">
import type { SidebarsConfig } from '@docusaurus/plugin-content-docs';


import Sidebar from '../../cloud-agent/docs/docusaurus/sidebars.js'
import { SidebarItemConfig } from '@docusaurus/plugin-content-docs/src/sidebars/types.js';

const tutorialsSidebar = Sidebar.tutorialsSidebar as SidebarItemConfig[];

const converted = tutorialsSidebar
    .filter((item) => item !== 'index')
    .map((item) => {
        if (typeof item === 'string') {
            return `cloud-agent/docs/docusaurus/${item}`
        }
        if (item.type === 'category') {
            return {
                ...item,
                items: Array.isArray(item.items) ?
                    item.items.map((item) => `cloud-agent/docs/docusaurus/${item}`) :
                    []
            }
        }
        return item;
    });

const sidebar: SidebarsConfig[keyof SidebarsConfig] = [
    {
        type: 'doc',
        id: 'documentation/developers/README',
        className: 'hidden-sidebar-item',
    },
    {
        type: 'category',
        label: 'SDKs',
        collapsed: true,
        link: {
            type: 'generated-index',
            title: 'SDKs',
            description: 'SDKs'
        },
        items: [
            {
                type: 'category',
                label: 'Typescript',
                collapsed: true,
                link: {
                    type: 'doc',
                    id: 'sdk-ts/docs/sdk/README',
                },
                items: [
                    {
                        type: 'category',
                        label: 'Tutorials',
                        link: {
                            type: 'generated-index',
                            title: 'Tutorials',
                            description: 'Tutorials'
                        },
                        items: [
                            {
                                type: 'doc',
                                id: 'sdk-ts/docs/examples/Backup',
                                label: 'Backup',
                            },
                            {
                                type: 'doc',
                                id: 'sdk-ts/docs/pluto/README',
                                label: "Storage - Pluto"
                            },
                            {
                                type: 'category',
                                label: 'Prism DID',
                                link: {
                                    type: 'generated-index',
                                    title: 'Prism DID',
                                    description: 'Prism DID Related tutorials and documentation'
                                },
                                items: [
                                    {
                                        type: 'autogenerated',
                                        dirName: 'sdk-ts/docs/prism',
                                    },
                                ]
                            },
                            {
                                type: 'category',
                                label: 'Connectionless & OOB',
                                link: {
                                    type: 'generated-index',
                                    title: 'Connectionless & OOB',
                                    description: 'Connectionless & OOB'
                                },
                                items: [
                                    {
                                        type: 'autogenerated',
                                        dirName: 'sdk-ts/docs/examples/connectionless',
                                    },
                                ]
                            },
                            {
                                type: 'doc',
                                id: 'sdk-ts/docs/examples/SDKVerification',
                                label: "SDK Verification"
                            },
                            {
                                type: 'doc',
                                id: 'sdk-ts/docs/examples/OIDC',
                                label: "OIDC Agent (WIP)"
                            },
                        ]
                    },

                    {
                        type: 'category',
                        label: 'Migration guides',
                        link: {
                            type: 'generated-index',
                            title: 'Migration guides',
                            description: 'Migration guides'
                        },
                        items: [
                            {
                                type: 'autogenerated',
                                dirName: 'sdk-ts/docs/migration',
                            },
                        ]
                    }
                ]
            },
            {
                type: 'link',
                label: 'Swift',
                href: 'https://hyperledger-identus.github.io/sdk-swift/documentation/edgeagentsdk/',
            },
            {
                type: 'link',
                label: 'Kotlin',
                href: 'https://hyperledger-identus.github.io/sdk-kmp/',
            },
        ]
    },
    {
        type: 'category',
        label: 'Cloud Agent',
        collapsed: true,
        link: {
            type: 'generated-index',
            title: 'Cloud Agent',
            description: 'Cloud Agent'
        },
        items: [
            {
                type: 'category',
                label: 'Tutorials',
                link: {
                    type: 'doc',
                    id: 'cloud-agent/docs/docusaurus/index'
                },
                items: [
                    'documentation/developers/quick-start',
                    ...converted
                ]
            },
            {
                type: 'autogenerated',
                dirName: 'documentation/developers/cloud-agent',
            },
        ]
    },

]

export default sidebar
</file>

<file path="documentation/reference/README.md">
---
id: README
title: Reference
sidebar_position: 1
sidebar_class_name: hidden-sidebar-item
---

# Reference Documentation

Welcome to the Hyperledger Identus reference documentation. This section provides comprehensive technical references, API specifications, and architectural decision records to support your development with Identus.

## What You'll Find Here

### 📚 SDKs

Detailed API documentation for Identus SDKs across multiple platforms:

- **[TypeScript SDK](../../sdk-ts/docs/sdk)** - Full TypeScript SDK reference with plugins for AnonCreds, DIDComm, DIF, OEA, and OIDC
- **[Swift SDK](https://hyperledger-identus.github.io/sdk-swift/documentation/edgeagentsdk/)** - Native iOS SDK documentation
- **[Kotlin Multiplatform SDK](https://hyperledger-identus.github.io/sdk-kmp/)** - Cross-platform Kotlin SDK reference

### 🔌 Cloud Agent API

OpenAPI specification and interactive documentation for the Identus Cloud Agent API. The Cloud Agent facilitates:

- DID (Decentralized Identifiers) management
- Verifiable credential issuance and verification
- Secure DIDComm messaging
- Connection management
- Multi-tenancy support

[Explore the Cloud Agent API →](./Cloud%20Agent%20API/identus-cloud-agent-api-reference)

### 📋 Specifications

A comprehensive overview of supported standards and protocols across the Identus platform, including:

- W3C DID and Verifiable Credentials specifications
- DIDComm v2 protocols
- AnonCreds support
- OpenID for Verifiable Credentials
- Status and support tracking for all implemented specifications

[View Specifications →](./specifications)

### 🔗 PRISM DID Method

The official PRISM DID Method specification, detailing the `did:prism` method implementation for Cardano-based decentralized identifiers.

[Read PRISM DID Spec →](https://github.com/input-output-hk/prism-did-method-spec/blob/main/w3c-spec/PRISM-method.md)

### 🏗️ Architectural Decision Records (ADRs)

Documentation of key architectural decisions made during the development of Hyperledger Identus. ADRs provide:

- **Context** - The circumstances and forces at play when decisions were made
- **Decision** - The architectural choice and its rationale
- **Consequences** - Expected outcomes and trade-offs

ADRs help developers understand the "why" behind technical choices and maintain consistency across the platform.

[Browse Decision Records →](/category/decision-records)

## Quick Links

- [Getting Started Guide](../developers/quick-start.md)
- [Learn Identus Concepts](../learn/README.md)
- [Developer Tutorials](../developers/README.md)

## Contributing

Found an issue or want to improve the documentation? Contributions are welcome! Visit our [GitHub repository](https://github.com/hyperledger-identus) to get involved.
</file>

<file path="documentation/reference/sidebar.ts">
import type { SidebarsConfig } from '@docusaurus/plugin-content-docs';
import { buildTypeDocCategorySidebar } from '../../src/utils';

const sidebar: SidebarsConfig[keyof SidebarsConfig] = [
  { type: 'doc', id: 'documentation/reference/README', className: 'hidden-sidebar-item' },
  {
    type: 'category',
    label: 'SDKs',
    collapsed: true,
    link: {
        type: 'generated-index',
        title: 'SDKs',
        description: 'SDKs'
    },
    items: [
      {
        type: 'category',
        label: 'Typescript',
        link: {
          type: 'generated-index',
          title: 'Typescript',
          description: 'Typescript'
        },
        items: [
          ...buildTypeDocCategorySidebar('sdk-ts/docs/sdk/overview', 'Overview').map((item) => (item as any).items ?? []),
          {
            type: 'category',
            label: 'Plugins',
            items: [
              ...buildTypeDocCategorySidebar('sdk-ts/docs/sdk/plugins/internal/anoncreds', 'AnonCreds'),
              ...buildTypeDocCategorySidebar('sdk-ts/docs/sdk/plugins/internal/didcomm', 'DIDComm'),
              ...buildTypeDocCategorySidebar('sdk-ts/docs/sdk/plugins/internal/dif', 'DIF'),
              ...buildTypeDocCategorySidebar('sdk-ts/docs/sdk/plugins/internal/oea', 'OEA'),
              ...buildTypeDocCategorySidebar('sdk-ts/docs/sdk/plugins/internal/oidc', 'OIDC'),
            ]
          }
        ]
      },
      {
        type: 'link',
        label: 'Swift',
        href: 'https://hyperledger-identus.github.io/sdk-swift/documentation/edgeagentsdk/',
    },
    {
        type: 'link',
        label: 'Kotlin',
        href: 'https://hyperledger-identus.github.io/sdk-kmp/',
    }
    ]
  },
  {
    type: 'category',
    label: 'Cloud Agent API',
    link: {
      type: 'generated-index',
      title: 'Cloud Agent API',
      description: 'OpenAPI specification for the Cloud Agent API'
    },
    items: [
      {
        type: 'autogenerated',
        dirName: 'documentation/reference/Cloud Agent API',
      }
    ]
  },
  'documentation/reference/specifications',
  {
    type: 'link',
    label: 'PRISM DID Spec',
    href: 'https://github.com/input-output-hk/prism-did-method-spec/blob/main/w3c-spec/PRISM-method.md',
  },
  {
    type: 'category',
    label: 'Decision Records',
    link: {
      type: 'generated-index',
      title: 'Decision Records',
    },
    items: [
      {
        type: 'autogenerated',
        dirName: 'documentation/adrs/decisions',
      },
      {
        type: 'autogenerated',
        dirName: 'sdk-ts/docs/decisions',
      },
    ]
  }
]

export default sidebar
</file>

<file path="src/components/features/index.js">
import React from 'react';
import clsx from 'clsx';
import styles from './styles.module.css';
import HomeResources from '../resources';
import { FutureOfIdentity } from '../resources';


const FeatureList = [
    {
        title: 'Decentralization',
        Svg: require('@site/static/img/modularity-and-flexibility.svg').default,
        description: (
            <>
                Identus’ suite of infrastructure products provides easy-to-use and easy-to-integrate verifiable
                data and identity components that power diverse and scalable solutions.
            </>
        ),
    },
    {
        title: 'Interoperability',
        Svg: require('@site/static/img/interoperability.svg').default,
        description: (
            <>
                Build secure, private, and verifiable interactions between individuals, organizations, and devices using
                open standards and protocols such as <a href="https://www.w3.org/TR/did-core/" target="_blank"
                    rel="noreferrer">W3C DID spec</a>, <a
                        href="https://www.w3.org/TR/vc-data-model/" target="_blank" rel="noreferrer">W3C VC-JWT</a>, <a
                            href="https://hyperledger.github.io/anoncreds-spec/"
                            target="_blank"
                            rel="noreferrer">HL
                    Anoncreds</a> , and <a href="https://identity.foundation/didcomm-messaging/spec/" target="_blank"
                        rel="noreferrer">DIDComm V2</a> .
            </>
        ),
    },
    {
        title: 'Security and Privacy',
        Svg: require('@site/static/img/security-privacy.svg').default,
        description: (
            <>
                Security features, such as end-to-end encryption, verifiable claims, and secure storage of data, ensure
                software-defined trust and protection for your users
            </>
        ),
    },
];

function Feature({ Svg, title, description }) {
    return (
        <div className={clsx('col col--4')}>
            <div className="text--center">
                <Svg className={styles.featureSvg} role="img" />
            </div>
            <div className="text--center padding-horiz--md">
                <h3>{title}</h3>
                <p>{description}</p>
            </div>
        </div>
    );
}

export default function HomepageFeatures() {
    return (
        <section className={styles.features}>
            <div className="container">
            <HomeResources />
                <FutureOfIdentity />
                <div className="row">
                    {FeatureList.map((props, idx) => (
                        <Feature key={idx} {...props} />
                    ))}
                </div>
               
            </div>
        </section>
    );
}
</file>

<file path="src/components/resources/index.module.css">
.home__resources {
    padding: 0;
    display: grid;
    margin-top: 35px;
    margin-bottom: 6.25rem;
    grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
    grid-gap: 2rem;
    perspective: 10000px;
}

.resource__wrapper {
    position: relative;
    overflow: hidden;
    border-radius: 10px;
}

.resource__link {
    height: 20px;
    margin-left: auto;
    align-self: flex-end;
}

.resource:hover .resource__link__svg__0 {
    transform: rotate(180deg);
}

.resource__link__svg__0 {
    transform-origin: 12px center;
    transition: transform 250ms;
}

.resource:hover .resource__link__svg__1 {
    transform: translateX(15px);
}

.resource__link__svg__1 {
    transition: transform 250ms;
}

.resource p {
    font-size: 14px;
    margin-bottom: 0;
    max-width: 307px;
    color: inherit;
}

.resource h3 {
    align-self: center;
    margin-bottom: 0;
    font-size: 17px;
    color: inherit;
}

.resource {
    display: flex;
    flex-wrap: wrap;
    position: relative;
    z-index: 10;
    text-align: left;
    border-radius: 15px;
    cursor: pointer;
    padding: 1.5rem 1.5rem 1rem 1.5rem;
    text-decoration: none !important;
    background: var(--ifm-resource-cards-background-color);
    height: 100%;
    gap: 1rem;
    overflow: hidden;
    transition: all ease-in-out 250ms;
}

.resource:hover {
    background: var(--ifm-color-primary-darker);
    color: white;
    box-shadow: 0px 20px 24px -4px rgba(16, 24, 40, 0.08), 0px 8px 8px -4px rgba(16, 24, 40, 0.03);
}

.resource:hover svg path {
    stroke: white;
}

.resource:hover .resource__light {
    opacity: 1;
}

.resource__light {
    top: 0;
    left: 0;
    opacity: 0;
    position: absolute;
    width: 25rem;
    height: 25rem;
    transform: translateX(-50%) translateY(-50%);
    pointer-events: none;
    border-radius: 50%;
    background: radial-gradient(circle, rgba(255, 255, 255, .1) 0%, transparent 70%);
    z-index: 10;
}

.home__resources .resource h3,
p {
    /*color: var(--ifm-color-gray-600);*/
    color: inherit;
}

.resource__intro {
    max-width: 693px;
    margin: clamp(10rem, 20vw, 20.0625rem) auto 0;
    height: auto;
    text-align: center;
}

.resource__intro h5 {
    font-size: 17px;
    line-height: 28px;
    color: var(--ifm-color-primary);
}

.resource__intro h2 {
    font-size: clamp(1.5rem, 2vw, 1.875rem);
    font-weight: 400;
    line-height: 38px;
    margin-bottom: 0;
    color: var(--ifm-color-resources-intro);
}

.resource__intro a {
    color: #ffffff;
    border-radius: 25px;
    font-size: 1rem;
    display: inline-block;
    margin-top: 3.125rem;
    background: var(--ifm-color-primary);
    padding: .625rem 1.75rem;
    text-decoration: none;
}

.project__description {
    max-width: 693px;
    margin: clamp(1rem, 20vw, 3rem) auto 5rem;
    height: auto;
    text-align: center;
}

.project__description h5 {
    font-size: 17px;
    line-height: 28px;
    color: var(--ifm-color-primary);
}

.project__description h2 {
    font-size: clamp(1.5rem, 2vw, 1.875rem);
    font-weight: 400;
    line-height: 38px;
    margin-bottom: 0;
    color: var(--ifm-color-resources-intro);
}

.project__description a {
    color: #ffffff;
    border-radius: 25px;
    font-size: 1rem;
    display: inline-block;
    margin-top: 3.125rem;
    background: var(--ifm-color-primary);
    padding: .625rem 1.75rem;
    text-decoration: none;
}
</file>

<file path="src/config/headerMenu.ts">
import type { NavbarItem } from '@docusaurus/theme-common';

export const headerMenu: NavbarItem[] = [
    // Example navbar items:
    {
        to: '/documentation/learn/',
        position: 'left',
        activeBaseRegex: `/documentation/learn/`,
        label: 'Learn',
    },
    {
        to: '/documentation/developers/',
        position: 'left',
        activeBaseRegex: `/documentation/developers/`,
        label: 'Developers',
    },
    {
        to: '/documentation/reference/',
        position: 'left',
        activeBaseRegex: `/documentation/reference/`,
        label: 'Reference',
    },
    // {
    //     to: '/tutorials/',
    //     label: 'Tutorials',
    //     position: 'left',
    //     activeBaseRegex: `/tutorials/`
    // },
    // {
    //     to: '/adrs/',
    //     label: 'ADRs',
    //     position: 'left',
    //     activeBaseRegex: `/adrs/`
    // },
    // {
    //     to: '/agent-api/',
    //     label: 'Agent API',
    //     activeBaseRegex: `/agent-api/`
    // },
    // {
    //     type: 'dropdown',
    //     label: 'SDKs',
    //     position: 'left',
    //     items: [
    //         {
    //             label: 'SDK Swift',
    //             href: 'https://hyperledger-identus.github.io/sdk-swift/documentation/edgeagentsdk/',
    //         },
    //         {
    //             to: '/sdk-ts/sdk',
    //             label: 'SDK Typescript',
    //             activeBaseRegex: `/sdk-ts/sdk`
    //         },
    //         {
    //             label: 'SDK Kotlin Multiplatform',
    //             href: 'https://hyperledger-identus.github.io/sdk-kmp/',
    //         },
    //     ],
    // },
    // {
    //     label: 'PRISM DID Spec',
    //     href: 'https://github.com/input-output-hk/prism-did-method-spec/blob/main/w3c-spec/PRISM-method.md',
    // },
];
</file>

<file path="src/config/presets.ts">
import type { PresetConfig } from '@docusaurus/types';
import type * as OpenApiPlugin from 'docusaurus-plugin-openapi-docs';

// Central mapping of legacy -> new link patterns used by the remarkLinkFixer plugin.
const remarkLinkFixerRules = [
  // Old "home" docs to new locations
  {
    type: 'prefix',
    from: '/home/concepts/glossary',
    to: '/documentation/learn/glossary',
  },
  {
    type: 'prefix',
    from: '/home/quick-start',
    to: '/documentation/developers/quick-start',
  },
  {
    type: 'prefix',
    from: '/home/identus/cloud-agent/overview',
    to: '/documentation/learn/components/cloud-agent',
  },
  {
    type: 'prefix',
    from: '/home/identus/cloud-agent/environment-variables',
    to: '/documentation/developers/cloud-agent/environment-variables',
  },
  {
    type: 'prefix',
    from: '/home/identus/mediator',
    to: '/documentation/learn/components/mediator',
  },

  // Old tutorials namespace -> unified Cloud Agent tutorials
  {
    type: 'prefix',
    from: '/tutorials/',
    to: '/cloud-agent/docs/docusaurus/',
  },

  // Old Agent API locations -> new Cloud Agent API reference
  {
    type: 'prefix',
    from: '/docs/agent-api',
    to: '/documentation/reference/Cloud Agent API/identus-cloud-agent-api-reference',
  },
  {
    type: 'prefix',
    from: '/agent-api',
    to: '/documentation/reference/Cloud Agent API/identus-cloud-agent-api-reference',
  },

  // Fix learn README relative path that used an extra "concepts" segment
  {
    type: 'prefix',
    from: './concepts/glossary',
    to: './glossary',
  },

  // Fix links that include the generated ".info.mdx" suffix instead of the doc id
  {
    type: 'regex',
    from: '\\.info\\.mdx(?=$|#)',
    to: '',
  },

  // Fix common anchor typos / legacy ids
  {
    type: 'regex',
    from: '#decentralized-identifer',
    to: '#decentralized-identifier',
  },
  {
    type: 'regex',
    from: '#decentralized-identifiers',
    to: '#decentralized-identifier',
  },
] as const;

// eslint-disable-next-line @typescript-eslint/no-var-requires,@typescript-eslint/no-require-imports
const remarkLinkFixer = require('../plugins/remarkLinkFixer');

export const presets: PresetConfig[] = [
  [
    './src/config/multiDocPreset.ts',
    {
      theme: {
        customCss: require.resolve('../css/custom.css'),
      },
      docs: [
        {
          id: 'docs',
          sidebarPath: require.resolve('../../sidebars.ts'),
          path: './',
          routeBasePath: '/',
          include: [
            'cloud-agent/docs/docusaurus/**/*.md',
            'cloud-agent/docs/docusaurus/**/*.mdx',
            'sdk-ts/docs/**/*.md',
            'sdk-ts/docs/**/*.mdx',
            'CONTRIBUTING.md',
            'SECURITY.md',
            'documentation/adrs/*.md',
            'documentation/adrs/*.mdx',
            'documentation/developers/*.md',
            'documentation/developers/*.mdx',
            'documentation/learn/*.md',
            'documentation/learn/*.mdx',
            'documentation/reference/adrs/**/*.md',
            'documentation/reference/adrs/**/*.mdx',
            'documentation/reference/*.md',
            'documentation/reference/*.mdx',
          ],
          remarkPlugins: [
            [
              remarkLinkFixer,
              {
                rules: remarkLinkFixerRules,
              },
            ],
          ],
        },
        {
          id: 'api', // plugin id
          docsPluginId: 'docs', // integrate into our docs plugin
          config: {
            cloudAgent: {
              label: 'Cloud Agent API',
              specPath:
                'cloud-agent/cloud-agent/service/api/http/cloud-agent-openapi-spec.yaml',
              outputDir: 'documentation/reference/Cloud Agent API',
              sidebarOptions: {
                groupPathsBy: 'tag',
              },
            } satisfies OpenApiPlugin.Options,
          },
        },
      ],
    },
  ],
];
</file>

<file path="src/pages/index.js">
import React from 'react';
import useDocusaurusContext from '@docusaurus/useDocusaurusContext';
import Layout from '@theme/Layout';
import HomepageFeatures from '@site/src/components/features';

import Blob from '../components/blob';

export default function Home() {
  const { siteConfig } = useDocusaurusContext();
  return (
    <Layout
      title={`Hyperledger Identus - ${siteConfig.title}`}
      description="Description will go into a meta tag in <head />">
      <Blob />
      <main>
        <HomepageFeatures />
      </main>
    </Layout>
  );
}
</file>

<file path="src/components/resources/index.js">
import React, { useCallback, useRef, useState } from 'react'
import styles from './index.module.css';
import Link from '@docusaurus/Link';
import { useColorMode } from '@docusaurus/theme-common';
import Button from '../button';

const Graphics = {
    apis({ color = "#5559F2" }) {
        return (
            <svg xmlns="http://www.w3.org/2000/svg" width={51} height={51} fill="none">
                <path
                    stroke={color}
                    strokeLinecap="round"
                    strokeLinejoin="round"
                    strokeWidth={2}
                    d="m30.813 31.875 6.375-6.375-6.376-6.375m-10.625 0L13.813 25.5l6.376 6.375m-3.613 12.75h17.85c3.57 0 5.356 0 6.72-.695a6.376 6.376 0 0 0 2.785-2.786c.695-1.363.695-3.149.695-6.719v-17.85c0-3.57 0-5.355-.695-6.72a6.375 6.375 0 0 0-2.786-2.785c-1.363-.695-3.149-.695-6.719-.695h-17.85c-3.57 0-5.355 0-6.72.695A6.375 6.375 0 0 0 7.07 9.856c-.695 1.364-.695 3.149-.695 6.719v17.85c0 3.57 0 5.356.695 6.72a6.375 6.375 0 0 0 2.786 2.785c1.364.695 3.149.695 6.719.695Z"
                />
            </svg>
        )
    },
    docs({ color = "#5559F2" }) {
        return (
            <svg xmlns="http://www.w3.org/2000/svg" width={51} height={51} fill="none">
                <path
                    stroke={color}
                    strokeLinecap="round"
                    strokeLinejoin="round"
                    strokeWidth={2}
                    d="M29.75 4.823V13.6c0 1.19 0 1.785.232 2.24.203.4.528.725.928.928.455.232 1.05.232 2.24.232h8.777M29.75 37.188l5.313-5.313-5.313-5.313m-8.5 0-5.313 5.313 5.313 5.313M42.5 21.224V36.55c0 3.57 0 5.356-.695 6.72a6.376 6.376 0 0 1-2.786 2.785c-1.363.695-3.149.695-6.719.695H18.7c-3.57 0-5.355 0-6.72-.695a6.375 6.375 0 0 1-2.785-2.786C8.5 41.906 8.5 40.12 8.5 36.55v-22.1c0-3.57 0-5.355.695-6.72a6.375 6.375 0 0 1 2.786-2.785c1.364-.695 3.149-.695 6.719-.695h6.825c1.56 0 2.339 0 3.073.176.65.156 1.272.414 1.842.763.644.395 1.195.946 2.297 2.049l6.776 6.774c1.102 1.103 1.653 1.654 2.048 2.298.35.57.607 1.192.763 1.842.176.734.176 1.514.176 3.073Z"
                />
            </svg>
        )
    },
    getStarted({ color = "#767AF5" }) {
        return (
            <svg xmlns="http://www.w3.org/2000/svg" width={201} height={201} fill="none">
                <path
                    stroke={color}
                    strokeLinecap="round"
                    strokeLinejoin="round"
                    strokeWidth={4}
                    d="M37.688 184.25v-41.875m0-83.75V16.75M16.75 37.688h41.875M16.75 163.311h41.875m50.25-138.187L94.351 62.887c-2.362 6.14-3.543 9.21-5.379 11.794a25.125 25.125 0 0 1-5.916 5.916c-2.583 1.837-5.653 3.017-11.794 5.38L33.5 100.5l37.762 14.524c6.14 2.362 9.21 3.543 11.794 5.379a25.13 25.13 0 0 1 5.916 5.916c1.837 2.583 3.017 5.653 5.38 11.794l14.523 37.762 14.524-37.762c2.362-6.141 3.543-9.211 5.379-11.794a25.136 25.136 0 0 1 5.916-5.916c2.583-1.836 5.653-3.017 11.794-5.379L184.25 100.5l-37.762-14.524c-6.141-2.362-9.211-3.543-11.794-5.379a25.13 25.13 0 0 1-5.916-5.916c-1.836-2.583-3.017-5.653-5.379-11.794l-14.524-37.762Z"
                />
            </svg>
        )
    },
    resources({ color = "#5559F2" }) {
        return (
            <svg xmlns="http://www.w3.org/2000/svg" width={51} height={51} fill="none">
                <path
                    stroke={color}
                    strokeLinecap="round"
                    strokeLinejoin="round"
                    strokeWidth={2}
                    d="m27.625 14.875-2.37-4.741c-.683-1.365-1.024-2.047-1.533-2.545a4.25 4.25 0 0 0-1.588-.982c-.673-.232-1.436-.232-2.962-.232H11.05c-2.38 0-3.57 0-4.48.463a4.25 4.25 0 0 0-1.857 1.858c-.463.909-.463 2.099-.463 4.479v1.7m0 0h32.3c3.57 0 5.356 0 6.72.695a6.376 6.376 0 0 1 2.785 2.786c.695 1.363.695 3.149.695 6.719v9.35c0 3.57 0 5.356-.695 6.72a6.376 6.376 0 0 1-2.786 2.785c-1.363.695-3.149.695-6.719.695h-22.1c-3.57 0-5.355 0-6.72-.695a6.375 6.375 0 0 1-2.785-2.786c-.695-1.363-.695-3.149-.695-6.719v-19.55Zm25.5 20.188 5.313-5.313-5.313-5.313m-8.5 0-5.313 5.313 5.313 5.313"
                />
            </svg>
        )
    },
    tutorials({ color = "#5559F2" }) {
        return (
            <svg xmlns="http://www.w3.org/2000/svg" width={51} height={51} fill="none">
                <path
                    stroke={color}
                    strokeLinecap="round"
                    strokeLinejoin="round"
                    strokeWidth={2}
                    d="M20.188 19.051c0-1.014 0-1.521.212-1.804.184-.247.467-.401.774-.423.353-.025.78.249 1.633.797l10.031 6.449c.74.476 1.11.714 1.238 1.017.112.264.112.562 0 .826-.127.303-.498.541-1.238 1.017l-10.031 6.449c-.853.548-1.28.822-1.633.797a1.063 1.063 0 0 1-.774-.423c-.212-.283-.212-.79-.212-1.804V19.05Z"
                />
                <path
                    stroke={color}
                    strokeLinecap="round"
                    strokeLinejoin="round"
                    strokeWidth={2}
                    d="M6.375 16.575c0-3.57 0-5.355.695-6.72A6.375 6.375 0 0 1 9.856 7.07c1.364-.695 3.149-.695 6.719-.695h17.85c3.57 0 5.356 0 6.72.695a6.375 6.375 0 0 1 2.785 2.786c.695 1.364.695 3.149.695 6.719v17.85c0 3.57 0 5.356-.695 6.72a6.376 6.376 0 0 1-2.786 2.785c-1.363.695-3.149.695-6.719.695h-17.85c-3.57 0-5.355 0-6.72-.695a6.375 6.375 0 0 1-2.785-2.786c-.695-1.363-.695-3.149-.695-6.719v-17.85Z"
                />
            </svg>
        )
    },
    walletSdkSwift({ color = "#5559F2" }) {
        return (
            <svg xmlns="http://www.w3.org/2000/svg" width={51} height={51} fill="none">
                <path
                    stroke={color}
                    strokeLinecap="round"
                    strokeLinejoin="round"
                    strokeWidth={2}
                    d="M30.5 8h-9.975m-3.1 38.75h16.15c2.38 0 3.57 0 4.48-.463a4.25 4.25 0 0 0 1.857-1.858c.463-.909.463-2.099.463-4.479v-28.9c0-2.38 0-3.57-.463-4.48a4.25 4.25 0 0 0-1.858-1.857c-.909-.463-2.099-.463-4.479-.463h-16.15c-2.38 0-3.57 0-4.48.463a4.25 4.25 0 0 0-1.857 1.858c-.463.909-.463 2.099-.463 4.479v28.9c0 2.38 0 3.57.463 4.48a4.25 4.25 0 0 0 1.857 1.857c.91.463 2.1.463 4.48.463Z"
                />
            </svg>
        )
    },
    walletSdkKmm({ color = "#5559F2" }) {
        return (
            <svg xmlns="http://www.w3.org/2000/svg" width={51} height={51} fill="none">
                <path
                    stroke={color}
                    strokeLinecap="round"
                    strokeLinejoin="round"
                    strokeWidth={2}
                    d="M30.5 8h-9.975m-3.1 38.75h16.15c2.38 0 3.57 0 4.48-.463a4.25 4.25 0 0 0 1.857-1.858c.463-.909.463-2.099.463-4.479v-28.9c0-2.38 0-3.57-.463-4.48a4.25 4.25 0 0 0-1.858-1.857c-.909-.463-2.099-.463-4.479-.463h-16.15c-2.38 0-3.57 0-4.48.463a4.25 4.25 0 0 0-1.857 1.858c-.463.909-.463 2.099-.463 4.479v28.9c0 2.38 0 3.57.463 4.48a4.25 4.25 0 0 0 1.857 1.857c.91.463 2.1.463 4.48.463Z"
                />
            </svg>
        )
    },
    walletSdkTS({ color = "#5559F2" }) {
        return (
            <svg xmlns="http://www.w3.org/2000/svg" width={51} height={51} fill="none">
                <path
                    stroke={color}
                    strokeLinecap="round"
                    strokeLinejoin="round"
                    strokeWidth={2}
                    d="M6.375 34V15.3c0-2.38 0-3.57.463-4.48a4.25 4.25 0 0 1 1.858-1.857c.909-.463 2.099-.463 4.479-.463h24.65c2.38 0 3.57 0 4.48.463a4.25 4.25 0 0 1 1.857 1.857c.463.91.463 2.1.463 4.48V34H33.283c-.52 0-.78 0-1.024.059a2.126 2.126 0 0 0-.614.254c-.215.132-.398.315-.766.683l-.133.133c-.368.368-.551.551-.766.683-.19.116-.397.202-.614.254-.245.059-.505.059-1.024.059h-5.684c-.52 0-.78 0-1.024-.059a2.126 2.126 0 0 1-.614-.254c-.215-.132-.398-.315-.766-.683l-.133-.133c-.368-.368-.551-.551-.766-.683a2.126 2.126 0 0 0-.614-.254C18.496 34 18.236 34 17.717 34H6.375Zm0 0a2.125 2.125 0 0 0-2.125 2.125v.708c0 1.318 0 1.977.145 2.517A4.25 4.25 0 0 0 7.4 42.355c.54.145 1.2.145 2.517.145h31.166c1.318 0 1.977 0 2.517-.145a4.25 4.25 0 0 0 3.005-3.005c.145-.54.145-1.2.145-2.517 0-.658 0-.988-.072-1.258a2.125 2.125 0 0 0-1.503-1.503c-.27-.072-.6-.072-1.258-.072H42.5"
                />
            </svg>
        )
    }
}

const RESOURCES = [
    {
        title: "Tutorials",
        Svg: Graphics.tutorials,
        content: "Learn how to build with Identus through our tutorials",
        linkTo: "/cloud-agent/docs/docusaurus/"
    },
    {
        title: "Wallet SDK Swift",
        Svg: Graphics.walletSdkSwift,
        content: "Build Swift-based applications using our SDKs",
        linkTo: "https://hyperledger-identus.github.io/sdk-swift/documentation/edgeagentsdk/"
    },
    {
        title: "Wallet SDK TypeScript",
        Svg: Graphics.walletSdkTS,
        content: "Build web-based applications using our SDKs",
        linkTo: "/sdk-ts/docs/sdk/"
    },
    {
        title: "Wallet SDK KMM",
        Svg: Graphics.walletSdkKmm,
        content: "Build KMM-based applications using our SDKs",
        linkTo: "https://hyperledger-identus.github.io/sdk-kmp/"
    },
    {
        title: "APIs",
        Svg: Graphics.apis,
        content: "Our APIs simplify agent management and reduce time and cost-to-solution for developers",
    linkTo: "/documentation/reference/Cloud Agent API/identus-cloud-agent-api-reference"
    },
    {
        title: "Resources",
        Svg: Graphics.resources,
        content: "Everything you need to learn more about Identus",
        linkTo: "/documentation/learn/"
    },
]

function ResourceLink() {
    return (
        <div className={styles.resource__link}>
            <svg width={32} height={22} fill="none" xmlns="http://www.w3.org/2000/svg">
                <path className={styles.resource__link__svg__0} d="M3.33789 5.99976C5.06694 3.01075 8.29866 0.999756 12.0001 0.999756C17.5229 0.999756 22.0001 5.47691 22.0001 10.9998C22.0001 16.5226 17.5229 20.9998 12.0001 20.9998C8.29866 20.9998 5.06694 18.9888 3.33789 15.9998" stroke="#667085" strokeWidth="2" strokeLinecap="round" strokeLinejoin="round" />
                <path className={styles.resource__link__svg__1} d="M11 14.9998L15 10.9998M15 10.9998L11 6.99976M15 10.9998H1" stroke="#667085" strokeWidth="2" strokeLinecap="round" strokeLinejoin="round" />
            </svg>
        </div>
    )
}

function Resource(props) {
    const ref = useRef(null);
    const lightRef = useRef(null)
    const backgroundRef = useRef(null);
    const { colorMode } = useColorMode();

    const handleMouseMove = useCallback((event) => {
        const { clientX, clientY, currentTarget } = event;
        const {
            height: targetHeight,
            width: targetWidth,
            x: offsetX,
            y: offsetY
        } = currentTarget.getBoundingClientRect();
        const centerX = targetWidth / 2;
        const centerY = targetHeight / 2;
        const mouseX = (clientX - offsetX);
        const mouseY = (clientY - offsetY);
        const x = ((mouseX - centerX) * .06);
        const y = ((mouseY - centerY) * .06);
        ref.current.animate({ transform: `rotateX(${x}deg) rotateY(${y}deg)` }, { fill: "forwards" })
        lightRef.current.animate({
            top: `${mouseY}px`,
            left: `${mouseX}px`,
        }, { fill: "forwards", duration: 500 })
    }, [ref, lightRef])

    const handleMouseLeave = useCallback((event) => {
        event.preventDefault()
        ref.current.animate({ transform: `rotateX(${0}deg) rotateY(${0}deg)` }, { fill: "forwards", duration: 1000 })

    }, [backgroundRef])

    return (
        <div
            ref={ref}
            onMouseLeave={handleMouseLeave}
            className={styles.resource__wrapper}
            onMouseMove={handleMouseMove}
        >
            <Link href={props.linkTo} className={`${styles.resource}`}>
                <props.Svg color={colorMode === 'dark' ? "#767AF5" : "#5559F2"} />
                <h3>{props.title}</h3>
                <p>{props.content}</p>
                <ResourceLink linkTo={props.linkTo} />
                <div className={styles.resource__light} ref={lightRef} />
            </Link>
        </div>
    );
}

export function FutureOfIdentity() {
    return (
        <>
            <div className={styles.project__description}>
                <h5>Build the Future of Digital Identity</h5>
                <p>Hyperledger Identus is an open-source platform for building secure, standards-based digital identity solutions. With complete DID and verifiable credential functionality, it gives developers everything they need to create and integrate self-sovereign identity into apps and workflows - bridging trust between users, organizations, and systems.</p>
            </div>
        </>
    );
}

export default function HomeResources() {
    return (
        <>

            <div className={styles.home__resources}>
                {
                    RESOURCES.map((resource, index) => (
                        <Resource {...resource} key={index} />
                    ))
                }
            </div>
        </>
    );
}
</file>

</files>
